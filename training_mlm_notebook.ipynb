{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf287e73",
   "metadata": {},
   "source": [
    "# Pre-training End To End Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eade81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_mlm import TransformerForMaskedLM\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import yaml\n",
    "from tokenizer import WordPieceTokenizerWrapper\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453be13",
   "metadata": {},
   "source": [
    "## Train tokenizer and encode training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3610f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_train = 'text1.txt' \n",
    "input_encode = 'text2.txt' #each line is treated as one training example\n",
    "\n",
    "tokenizer = WordPieceTokenizerWrapper(tokenizer_type=\"berat\")\n",
    "tokenizer.train(tokenizer_dir='my_tokenizer', input=input_train)\n",
    "\n",
    "ds = tokenizer.encode(\n",
    "    tokenizer_dir='my_tokenizer',\n",
    "    input=input_encode, \n",
    "    max_length=24\n",
    ")\n",
    "train_loader = DataLoader(ds, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(ds, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5a713",
   "metadata": {},
   "source": [
    "## Load hyperparameters from `config.yaml` and initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe187653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerForMaskedLM(\n",
       "  (embeddings): TransformerTextEmbeddings(\n",
       "    (word_embeddings): Embedding(30000, 384, padding_idx=0)\n",
       "    (position): LearnedPositionalEmbedding(\n",
       "      (position_embeddings): Embedding(256, 384)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerEncoderBlock(\n",
       "      (msa_block): MultiheadSelfAttentionBlock(\n",
       "        (multihead_attn): MultiheadSelfAttention(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (out_drop): Dropout(p=0.1, inplace=False)\n",
       "          (Uqkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (Uout): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (mlp_block): MLPBlock(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1024, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlm): MaskedLanguageModelingHead(\n",
       "    (transform): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=384, out_features=30000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "architecture = config['architecture']\n",
    "training_params = config['training']\n",
    "LEARNING_RATE = training_params['learning_rate']\n",
    "WEIGHT_DECAY = training_params['weight_decay']\n",
    "WARMUP_RATIO = training_params['warmup_ratio']\n",
    "MAX_GRAD_NORM = training_params['max_grad_norm']\n",
    "GRAD_ACCUM_STEPS = training_params['grad_accum_steps']\n",
    "USE_AMP = training_params['use_amp']\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "model = TransformerForMaskedLM(**architecture)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196bc844",
   "metadata": {},
   "source": [
    "## Initialize optimizer loss_fn, scheduler and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "param_groups = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters()\n",
    "                   if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters()\n",
    "                   if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "num_training_steps = NUM_EPOCHS * num_update_steps_per_epoch\n",
    "num_warmup_steps = int(WARMUP_RATIO * num_training_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler(device=device, enabled=USE_AMP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae7751",
   "metadata": {},
   "source": [
    "## Training loop and wand logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ad4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "wandb.init(\n",
    "    project=\"bert-training\",\n",
    "    config={\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"warmup_ratio\": WARMUP_RATIO,\n",
    "        \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "        \"grad_accum_steps\": GRAD_ACCUM_STEPS,\n",
    "        \"amp\": USE_AMP,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"linear_with_warmup\",\n",
    "        \"model\": type(model).__name__\n",
    "    }\n",
    ")\n",
    "wandb.watch(model, log=\"all\", log_freq=50)\n",
    "\n",
    "def train_step():\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader, start=1), total=len(\n",
    "        train_loader), desc=f\"Epoch {epoch}/{NUM_EPOCHS} [train]\")\n",
    "\n",
    "    for step, (input_ids, attention_mask) in pbar:\n",
    "        input_ids = input_ids.to(device, non_blocking=True)\n",
    "        attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "\n",
    "        input_ids_masked, labels = tokenizer.mask_input_for_mlm(input_ids=input_ids)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device, enabled=USE_AMP):\n",
    "            out = model(input_ids=input_ids_masked, attention_mask=attention_mask)\n",
    "            logits = out['logits']\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward() # multiplies a given loss by scaler's current scale factor <-> scale gradient\n",
    "\n",
    "        grad_norm = None\n",
    "        if step % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer) # uncscale gradient\n",
    "            grad_norm = clip_grad_norm_( #clip gradient\n",
    "                model.parameters(), MAX_GRAD_NORM).item()\n",
    "            scaler.step(optimizer) #calls optimizer.step()\n",
    "            scaler.update()  # updates scaler's scale factor\n",
    "\n",
    "            scheduler.step() #update lr\n",
    "\n",
    "        if grad_norm is not None:\n",
    "            wandb.log({\n",
    "                \"train/step_loss\": loss.item(),\n",
    "                \"train/lr\": scheduler.get_last_lr()[0],\n",
    "                \"train/grad_norm\": grad_norm,\n",
    "                \"train/epoch\": epoch\n",
    "            })\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{train_loss/step:.4f}\",\n",
    "            \"lr\":   f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "    epoch_train_loss = train_loss / len(train_loader)\n",
    "    return epoch_train_loss\n",
    "\n",
    "def val_step():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.inference_mode():\n",
    "        for input_ids, attention_mask in tqdm(val_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [val]\"):\n",
    "            input_ids = input_ids.to(device, non_blocking=True)\n",
    "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "\n",
    "            input_ids_masked, labels = tokenizer.mask_input_for_mlm(input_ids=input_ids)\n",
    "\n",
    "            out = model(input_ids=input_ids_masked, attention_mask=attention_mask)\n",
    "            logits = out['logits']\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    return epoch_val_loss\n",
    "\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    \n",
    "    epoch_train_loss = train_step()\n",
    "    epoch_val_loss = val_step()\n",
    "     \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train/loss\": epoch_train_loss,\n",
    "        \"val/loss\": epoch_val_loss,\n",
    "    })\n",
    "\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": epoch_val_loss\n",
    "        }, Path(\"best_model.pt\"))\n",
    "        wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "        wandb.alert(\n",
    "            title=\"New best model\",\n",
    "            text=f\"Epoch {epoch} | val_loss={epoch_val_loss:.4f}\"\n",
    "        )\n",
    "    # torch.save({\n",
    "    #         \"model_state\": model.state_dict(),\n",
    "    #         \"epoch\": epoch,\n",
    "    #         \"val_loss\": epoch_val_loss\n",
    "    #     }, Path(f\"epoch-{epoch}-model.pt\"))\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
