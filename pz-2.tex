% !TEX TS-program = pdflatex
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{xcolor}
\geometry{margin=2.5cm}
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue}

\title{Propozycja rozwiązania: modelowanie, narzędzia, techniki oraz raporty danych}
\author{Zespół NLP}
\date{\today}

\begin{document}
\maketitle

\section*{Zakres dokumentu}
Dokument obejmuje: (1) ocenę narzędzi i technik oraz założenia techniki modelowania, (2) plan pozyskania i przygotowania danych wraz z miejscem na raporty EDA (wstępna eksploracja i ocena jakości), (3) opcjonalnie: opis modułów, zarys GUI oraz plan wstępnego przetwarzania danych. Celem jest spójny plan realizacji: pretrening (MLM) na korpusie Wikipedii, TAPT na zbiorach docelowych (IMDB, AG~News, i jeden dłuższy korpus), a następnie \emph{fine-tuning} do klasyfikacji.

\section{Ocena narzędzi i technik}
\subsection{Biblioteki i środowisko}
\begin{itemize}[leftmargin=2em]
  \item Framework trenowania: \textbf{PyTorch}. Implementacja transformera jest w repozytorium, modularna i przejrzysta.
  \item Zarządzanie danymi: \texttt{datasets} (Hugging Face) do pobierania i cache'owania IMDB, AG~News i Wikipedii.
  \item Przetwarzanie tekstu: \textbf{WordPiece} z oryginalnego słownika BERT (\emph{bert-base-uncased}).
  \item Walidacja i metryki: \texttt{scikit-learn} (dokładność, F1, macierze pomyłek); opcjonalnie logowanie do W\&B.
\end{itemize}

\subsection{Tokenizer: WordPiece (BERT)}
Wykorzystamy gotowy tokenizer oparty o oryginalny słownik BERT (WordPiece). Pozwala to na:
\begin{itemize}[leftmargin=2em]
  \item Spójny podział na sub-słowa z tokenami specjalnymi: \texttt{[CLS]}, \texttt{[SEP]}, \texttt{[MASK]}, \texttt{[PAD]}, \texttt{[UNK]}.
  \item Zgodność z praktykami pretreningu BERT (MLM, długość sekwencji 512).
  \item Integrację z istniejącą strukturą repo, np. moduł \texttt{src/textclf\_transformer/tokenizer/wordpiece\_tokenizer\_wrapper.py}.
\end{itemize}

\section{Założenia techniki modelowania}
\subsection{Architektura}
\begin{itemize}[leftmargin=2em]
  \item \textbf{Encoder Transformer} trenowany od zera (lub z inicjalizacją zgodną z BERT-em tam, gdzie ma to sens).
  \item Długość sekwencji: \textbf{512} tokenów dla pretreningu i TAPT (umożliwia kontekst artykułów).
  \item Warstwa klasyfikacyjna dla \emph{fine-tuningu}: odczyt z wektora \texttt{[CLS]} (lub pooling) i \emph{linear head} do klasyfikacji.
\end{itemize}

\subsection{Cele treningowe}
\paragraph{Pretrening (MLM).} Maskowany model językowy na sekwencjach długości 512. Standardowa polityka maskowania (np. 15\% tokenów: 80\% \texttt{[MASK]}, 10\% losowy token, 10\% oryginał) z \emph{cross-entropy} tylko dla pozycji maskowanych.

\paragraph{TAPT (Task-Adaptive PreTraining).} Dodatkowy MLM na domenach docelowych (IMDB, AG~News, +~korpus długich sekwencji). Modele TAPT trenujemy \textbf{oddzielnie na każdy zbiór}, aby lepiej dopasować reprezentacje do danej domeny.

\paragraph{Fine-tuning (klasyfikacja).} Po pretreningu/TAPT zamieniamy cel na klasyfikację nad etykietami danych nadzorowanych. Metryki: \textbf{accuracy}, \textbf{macro-F1}; dodatkowo macierze pomyłek.

\section{Dane i przygotowanie danych}
\subsection{Korpus Wikipedii do pretreningu MLM}
\paragraph{Źródło.} \texttt{wikimedia/wikipedia}, konfiguracja \texttt{20231101.en}, split \texttt{train}. Wybieramy podzbiór ok. \textbf{200\,000} artykułów.

\paragraph{Filtracja.} Wybór dokumentów zawierających słowa kluczowe:
\begin{center}
\texttt{["film", "sport", "business", "science", "technology", "news"]}
\end{center}
Filtracja po polach tekstowych (np. tytuł, lead, treść). Dodatkowo: deduplikacja, usunięcie bardzo krótkich/technicznych stron.

\paragraph{Tokenizacja i cięcie.} Tokenizacja WordPiece do maks. \textbf{512} tokenów (\emph{truncate\/pad}). Przykładowy szkic:
\begin{verbatim}
from datasets import load_dataset
ds = load_dataset("wikimedia/wikipedia", "20231101.en", split="train")
# Filtrowanie po slowach kluczowych, sampling do ~200k, tokenizacja do 512
\end{verbatim}

\paragraph{Podział na zbiory.} \emph{Train\/validation} (np. 95\%\/5\%). Walidacja używana do monitoringu \emph{loss} i wczesnego stopu.

\subsection{TAPT: IMDB i AG~News (oraz długi korpus)}
\begin{itemize}[leftmargin=2em]
  \item \textbf{IMDB}: teksty recenzji (bez etykiet do TAPT); tokenizacja do 512; trening MLM.
  \item \textbf{AG News}: tytuł+streszczenie artykułów; tokenizacja do 512; trening MLM.
  \item \textbf{Dodatkowy długi korpus}: (do wybrania) — wymagane długie sekwencje, np. artykuły naukowe, raporty. \textit{Miejsce na decyzję i opis.}
\end{itemize}

Każdy TAPT będzie skutkował oddzielnym modelem wstępnie dostosowanym do zadania.

\subsection{Fine-tuning: klasyfikacja}
\begin{itemize}[leftmargin=2em]
  \item \textbf{IMDB}: binarna klasyfikacja sentymentu.
  \item \textbf{AG~News}: 4 klasy (world, sports, business, sci\/tech).
  \item Dla długiego korpusu: zadanie zależne od wyboru (np. wieloklasowa klasyfikacja tematów).
\end{itemize}

\section{Raporty danych (EDA)}
Poniższe sekcje zostaną uzupełnione wynikami z narzędzia EDA (\texttt{dataset\_analysis.py}), zapisanymi do \texttt{analysis\_report.txt}.

\subsection*{IMDB — EDA (do uzupełnienia)}
\begin{itemize}[leftmargin=2em]
  \item Statystyki długości, rozkład etykiet, przykłady.
  \item Raport klasyfikacji baseline TF-IDF+LR, macierz pomyłek.
\end{itemize}

\subsection*{AG News — EDA (do uzupełnienia)}
\begin{itemize}[leftmargin=2em]
  \item Statystyki długości, rozkład etykiet, przykłady.
  \item Raport klasyfikacji baseline TF-IDF+LR, macierz pomyłek.
\end{itemize}

\subsection*{Wikipedia (korpus pretreningowy) — EDA (do uzupełnienia)}
\begin{itemize}[leftmargin=2em]
  \item Liczba dokumentów po filtracji, rozkład po słowach kluczowych.
  \item Rozkład długości (tokeny\/znaki), przykłady.
\end{itemize}

\section{Opis modułów (opcjonalnie)}
Zarys głównych komponentów repozytorium:
\begin{itemize}[leftmargin=2em]
  \item \textbf{Tokenizer}: \texttt{src/textclf\_transformer/tokenizer/wordpiece\_tokenizer\_wrapper.py} — integracja WordPiece, obsługa tokenów specjalnych, batche.
  \item \textbf{Model}: \texttt{src/textclf\_transformer/models/} — enkoder Transformer, warstwa klasyfikacyjna.
  \item \textbf{Trening}: \texttt{src/textclf\_transformer/training/} i \texttt{train.py} — pętle treningowe, logowanie, checkpointy.
  \item \textbf{Utils}: pomocnicze funkcje do I\/O, metryk, konfiguracji.
\end{itemize}

\section{Projekt GUI (opcjonalnie)}
Lekki interfejs (CLI lub prosty notatnik\/aplikacja) do:
\begin{itemize}[leftmargin=2em]
  \item Interaktywnego podglądu tokenizacji (WordPiece) i segmentacji do 512.
  \item Wprowadzania własnego tekstu i podglądu predykcji modeli.
  \item Eksportu raportów (confusion matrix, metryki) do PDF\/PNG.
\end{itemize}

\section{Plan eksperymentów i hiperparametry}
\subsection*{Pretrening (MLM) na Wikipedii}
\begin{itemize}[leftmargin=2em]
  \item Długość sekwencji: 512, \% maskowania: 15\%.
  \item Batch: (np.) 32--128 w zależności od zasobów; optymalizator AdamW.
  \item LR: \(1\times10^{-4}\) z \emph{warmup} i \emph{linear decay}; \# kroków: zależny od budżetu.
\end{itemize}

\subsection*{TAPT}
\begin{itemize}[leftmargin=2em]
  \item Zbiory: IMDB, AG~News, (długi korpus — do wyboru).
  \item Długość sekwencji: 512, polityka maskowania jak wyżej.
\end{itemize}

\subsection*{Fine-tuning}
\begin{itemize}[leftmargin=2em]
  \item Głowa klasyfikacyjna na wektorze \texttt{[CLS]}.
  \item Metryki monitorowane: accuracy, macro-F1; zapisy macierzy pomyłek.
\end{itemize}

\section{Walidacja i ewaluacja}
\begin{itemize}[leftmargin=2em]
  \item Podział train\/val; \emph{early stopping} na stracie walidacyjnej.
  \item Raporty: \texttt{analysis\_report.txt} z baseline TF-IDF+LR; analogiczne raporty dla modeli Transformer.
  \item Macierze pomyłek per zadanie; analiza błędów jakościowo (przykłady).
\end{itemize}

\section{Ryzyka i mitigacje}
\begin{itemize}[leftmargin=2em]
  \item \textbf{Przeciążenie pamięci przy 512 tokenach:} gradient \emph{accumulation}, mniejsze batch'e, skrócenie sekwencji w TAPT jeśli konieczne.
  \item \textbf{Dryf domenowy:} TAPT per zbiór, ew. \emph{mix} domenowy z \emph{curriculum}.
  \item \textbf{Niejednoznaczne etykiety:} balanse klas, wagi strat, macro-F1.
\end{itemize}

\section{Harmonogram (wysoki poziom)}
\begin{enumerate}[leftmargin=2em]
  \item Przygotowanie korpusu Wikipedii (filtracja, tokenizacja) i krótki dry-run MLM.
  \item TAPT: IMDB i AG~News (oddzielne modele).
  \item Wybór długiego korpusu i TAPT.
  \item Fine-tuning na klasyfikację + raporty.
  \item Iteracje usprawniające (hiperparametry, analiza błędów).
\end{enumerate}

\section*{Miejsca do uzupełnienia}
\begin{itemize}[leftmargin=2em]
  \item \textbf{EDA}: wkleić sekcje z \texttt{analysis\_report.txt} (IMDB, AG~News, Wikipedia).
  \item \textbf{Długi korpus}: decyzja o zbiorze i opis zadania.
\end{itemize}

\end{document}

