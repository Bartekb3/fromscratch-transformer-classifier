experiment:
  name: test_finetuning_222
  kind: finetuning
  output_dir: experiments\finetuning\test_finetuning_222
  seed: 420
logging:
  use_wandb: true
  wandb:
    entity: praca-inzynierska
    project: text_classification
    run_name: test_finetuning_222
  log_train_loss: true
  log_train_lr: true
  log_train_grad_norm: true
  log_eval_metrics: true
  csv_train_metrics_path: metrics/train/metrics.csv
  csv_eval_metrics_path: metrics/eval/metrics.csv
pretrained_experiment:
  name: test_pretraining_222
  path: experiments\pretraining\test_pretraining_222
  checkpoint: model.ckpt
  inherit:
    - architecture
    - tokenizer
tokenizer:
  wrapper_path: src/textclf_transformer/tokenizer/wordpiece_tokenizer_wrapper.py
  vocab_dir: src/textclf_transformer/tokenizer/BERT_original
  vocab_path: src/textclf_transformer/tokenizer/BERT_original/vocab.txt
  max_length: 1000
architecture:
  max_sequence_length: 1000
  embedding_dim: 1
  num_layers: 1
  mlp_size: 128
  mlp_dropout: 0.1
  pos_encoding: learned
  embedding_dropout: 0.1
  pad_token_id: 0
  attention:
    kind: mha
    mha:
      num_heads: 1
      projection_bias: true
      mha_out_dropout: 0.1
      attn_dropout: 0.0
    reformer: {}
    performer: {}
classification_head:
  num_labels: 2
  classifier_dropout: 0.1
  pooling: cls
  pooler_type: bert
training:
  batch_size: 16
  epochs: 1
  learning_rate: 2.0e-05
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  grad_accum_steps: 1
  use_amp: true
  loss: cross_entropy
  device: auto
data:
  train:
    dataset_path: data/tokenized/IMDB/train_dataset.pt
  val:
    dataset_path: data/tokenized/IMDB/val_dataset.pt
  test:
    dataset_path: data/tokenized/IMDB/test_dataset.pt
