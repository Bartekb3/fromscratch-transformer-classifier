# === FINETUNING CONFIG ===
experiment:
  name: __WILL_BE_FILLED__
  kind: finetuning
  output_dir: experiments/finetuning/__WILL_BE_FILLED__
  seed: 420

logging:
  use_wandb: false
  wandb:
    entity: praca-inzynierska
    project: demo
    run_name: __WILL_BE_FILLED__

  log_eval_metrics: true
  log_metrics_csv: true
  log_gpu_memory: true
  csv_train_metrics_path: metrics/train/metrics.csv
  csv_eval_metrics_path: metrics/eval/metrics.csv

pretrained_experiment:
  name: __WILL_BE_FILLED__
  path: experiments/pretraining/__WILL_BE_FILLED__
  checkpoint: checkpoints/model.ckpt

tokenizer:
  wrapper_path: src/textclf_transformer/tokenizer/wordpiece_tokenizer_wrapper.py
  vocab_dir: src/textclf_transformer/tokenizer/BERT_original
  max_length: 512

architecture:
  embedding_dim: 128
  num_layers: 2
  mlp_size: 256
  mlp_dropout: 0.1
  embedding_dropout: 0.1
  pos_encoding: rope # 'learned' | 'sinusoidal' | 'rope'
  rope:
    rope_base: 10000.0
    rope_scale: 1.0

  attention:
    attention_embedding_dim:
    num_heads: 4
    projection_bias: true # (Q/K/V/out) bias
    attn_out_dropout: 0.1 # dropout na projekcji wyj≈õciowej
    attn_dropout: 0.0 # dropout na wagach atencji (po softmax)

    kind: mha # 'mha' | 'lsh' | 'favor'
    ###attention_params###
    mha:
      use_native_sdpa: true
    lsh:
      num_hashes: 4
      chunk_size: 64
      mask_within_chunks: true
    favor:
      nb_features: 256
      ortho_features: true
      redraw_interval: 0
      phi: exp
      stabilize: true
      eps: 1.0e-6

classification_head:
  num_labels: 2
  classifier_dropout: 0.1
  pooling: cls # 'cls' | 'mean' | 'max' | 'min'
  pooler_type: bert # 'bert' | 'roberta' | null

training:
  batch_size: 16
  epochs: 2
  learning_rate: 2.0e-4
  warmup_ratio: 0.1
  min_lr_ratio: 0.2
  weight_decay: 0.01
  max_grad_norm: 1.0
  grad_accum_steps: 1
  use_amp: true
  loss: cross_entropy
  device: auto

  head_lr_mult: 1.0
  backbone_lr_mult: 0.5
  freeze: false
  freeze_n_layers: 0
  freeze_epochs: 0
  freeze_embeddings: false

data:
  train:
    shuffle: true
    dataset_path: data/tokenized/imdb_train.pt
  val:
    shuffle: false
    dataset_path: data/tokenized/imdb_val.pt
  test:
    shuffle: false
    dataset_path: data/tokenized/imdb_test.pt
