{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2054f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textclf_transformer.tokenizer.wordpiece_tokenizer_wrapper import WordPieceTokenizerWrapper\n",
    "tok = WordPieceTokenizerWrapper()\n",
    "tok.load(tokenizer_dir=\"/Users/michaliwaniuk/Desktop/fromscratch-transformer-classifier/src/textclf_transformer/tokenizer/BERT_original\")\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1389d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def token_stats(dataset: str, max_len: int):\n",
    "    ds = load_dataset(dataset)\n",
    "    result = {}\n",
    "    for split in list(ds.keys()):\n",
    "        if dataset == 'ccdv/arxiv-classification' and split == 'train':\n",
    "            continue\n",
    "        \n",
    "        data = list(ds[split]['text'])\n",
    "        tokenized = tok.encode(data, max_length=max_len)\n",
    "        real_tokens = tokenized[:][0] != 0\n",
    "        token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "        max_len_achieved = torch.sum(torch.sum(real_tokens, dim=1) == max_len)\n",
    "        print(f\"Split: {split}\\nNumber of examples that are longer that max_len:\\n{max_len_achieved}\\n{max_len_achieved/len(data)*100}% of total data\")\n",
    "\n",
    "        stats = {\n",
    "            \"avg_tokens\": float(token_lengths.mean()),\n",
    "            \"std_tokens\": float(token_lengths.std()),\n",
    "            \"median_tokens\": float(torch.median(token_lengths)),\n",
    "            \"min_tokens\": int(token_lengths.min()),\n",
    "            \"max_tokens\": int(token_lengths.max()),\n",
    "        }\n",
    "        result[split] = stats\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5091f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input is treated as a list of input texts\n",
      "Split: train\n",
      "Number of examples that are longer that max_len:\n",
      "0\n",
      "0.0% of total data\n",
      "[INFO] input is treated as a list of input texts\n",
      "Split: test\n",
      "Number of examples that are longer that max_len:\n",
      "0\n",
      "0.0% of total data\n",
      "[INFO] input is treated as a list of input texts\n",
      "Split: unsupervised\n",
      "Number of examples that are longer that max_len:\n",
      "0\n",
      "0.0% of total data\n"
     ]
    }
   ],
   "source": [
    "imdb = token_stats('imdb', 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce1a24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'avg_tokens': 313.8713073730469,\n",
       "  'std_tokens': 234.29586791992188,\n",
       "  'median_tokens': 233.0,\n",
       "  'min_tokens': 13,\n",
       "  'max_tokens': 3127},\n",
       " 'test': {'avg_tokens': 306.77099609375,\n",
       "  'std_tokens': 227.89404296875,\n",
       "  'median_tokens': 230.0,\n",
       "  'min_tokens': 10,\n",
       "  'max_tokens': 3157},\n",
       " 'unsupervised': {'avg_tokens': 314.8410339355469,\n",
       "  'std_tokens': 234.513671875,\n",
       "  'median_tokens': 234.0,\n",
       "  'min_tokens': 13,\n",
       "  'max_tokens': 3446}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51544f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input is treated as a list of paths to text files\n",
      "[INFO] input is treated as a list of input texts\n",
      "Split: train\n",
      "Number of examples that are longer that max_len:\n",
      "0\n",
      "0.0% of total data\n",
      "[INFO] input is treated as a list of paths to text files\n",
      "[INFO] input is treated as a list of input texts\n",
      "Split: test\n",
      "Number of examples that are longer that max_len:\n",
      "0\n",
      "0.0% of total data\n"
     ]
    }
   ],
   "source": [
    "agnews = token_stats('ag_news', 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d91c04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'avg_tokens': 53.16641616821289,\n",
       "  'std_tokens': 19.055023193359375,\n",
       "  'median_tokens': 51.0,\n",
       "  'min_tokens': 15,\n",
       "  'max_tokens': 379},\n",
       " 'test': {'avg_tokens': 52.746185302734375,\n",
       "  'std_tokens': 18.22791290283203,\n",
       "  'median_tokens': 50.0,\n",
       "  'min_tokens': 18,\n",
       "  'max_tokens': 277}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202c5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input is treated as a list of input texts\n",
      "Split: validation\n",
      "Number of examples that are longer that max_len:\n",
      "182\n",
      "7.28000020980835% of total data\n",
      "[INFO] input is treated as a list of input texts\n",
      "Split: test\n",
      "Number of examples that are longer that max_len:\n",
      "164\n",
      "6.559999942779541% of total data\n"
     ]
    }
   ],
   "source": [
    "arxiv = token_stats('ccdv/arxiv-classification', 32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fe529e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation': {'avg_tokens': 15012.7451171875,\n",
       "  'std_tokens': 8385.609375,\n",
       "  'median_tokens': 12885.0,\n",
       "  'min_tokens': 1373,\n",
       "  'max_tokens': 32768},\n",
       " 'test': {'avg_tokens': 14745.8623046875,\n",
       "  'std_tokens': 8257.8232421875,\n",
       "  'median_tokens': 12631.0,\n",
       "  'min_tokens': 1268,\n",
       "  'max_tokens': 32768}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
