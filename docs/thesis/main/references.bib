@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}
@inproceedings{choromanski2021rethinking,
  title     = {Rethinking Attention with Performers},
  author    = {Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}
@inproceedings{Kitaev2020Reformer:,
  title     = {Reformer: The Efficient Transformer},
  author    = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}
@inproceedings{devlin2019bert,
  title     = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages     = {4171--4186},
  year      = {2019}
}
@article{liu2019roberta,
  title   = {Roberta: A robustly optimized bert pretraining approach},
  author  = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal = {arXiv preprint arXiv:1907.11692},
  year    = {2019}
}
@inproceedings{gururangan-etal-2020-dont,
  title     = {Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author    = {Gururangan, Suchin  and
               Marasovi{\'c}, Ana  and
               Swayamdipta, Swabha  and
               Lo, Kyle  and
               Beltagy, Iz  and
               Downey, Doug  and
               Smith, Noah A.},
  editor    = {Jurafsky, Dan  and
               Chai, Joyce  and
               Schluter, Natalie  and
               Tetreault, Joel},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2020.acl-main.740},
  pages     = {8342--8360},
  abstract  = {Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.}
}
@inproceedings{Yin2021AutoTinyBERTAH,
  title     = {AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models},
  author    = {Yichun Yin and Cheng Chen and Lifeng Shang and Xin Jiang and Xiao Chen and Qun Liu},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2021}
}
@article{dao2022flashattention,
  title   = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author  = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal = {Advances in neural information processing systems},
  volume  = {35},
  pages   = {16344--16359},
  year    = {2022}
}

@article{SU2024127063,
  title    = {RoFormer: Enhanced transformer with Rotary Position Embedding},
  journal  = {Neurocomputing},
  volume   = {568},
  pages    = {127063},
  year     = {2024},
  issn     = {0925-2312},
  doi      = {https://doi.org/10.1016/j.neucom.2023.127063},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
  author   = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
  keywords = {Pre-trained language models, Position information encoding, Pre-training, Natural language processing},
  abstract = {Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer.}
}


@article{beltagy2020longformer,
  title   = {Longformer: The long-document transformer},
  author  = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal = {arXiv preprint arXiv:2004.05150},
  year    = {2020}
}

@article{wang2020linformer,
  title   = {Linformer: Self-attention with linear complexity},
  author  = {Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal = {arXiv preprint arXiv:2006.04768},
  year    = {2020}
}

@article{qin2022cosformer,
  title   = {cosformer: Rethinking softmax in attention},
  author  = {Qin, Zhen and Sun, Weixuan and Deng, Hui and Li, Dongxu and Wei, Yunshen and Lv, Baohong and Yan, Junjie and Kong, Lingpeng and Zhong, Yiran},
  journal = {arXiv preprint arXiv:2202.08791},
  year    = {2022}
}

@article{zaheer2020big,
  title   = {Big bird: Transformers for longer sequences},
  author  = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {17283--17297},
  year    = {2020}
}

@article{child2019generating,
  title   = {Generating long sequences with sparse transformers},
  author  = {Child, Rewon},
  journal = {arXiv preprint arXiv:1904.10509},
  year    = {2019}
}

@article{liu2024deepseek,
  title   = {Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author  = {Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal = {arXiv preprint arXiv:2405.04434},
  year    = {2024}
}

@article{liu2025deepseek,
  title   = {Deepseek-v3. 2: Pushing the frontier of open large language models},
  author  = {Liu, Aixin and Mei, Aoxue and Lin, Bangcai and Xue, Bing and Wang, Bingxuan and Xu, Bingzheng and Wu, Bochao and Zhang, Bowei and Lin, Chaofan and Dong, Chen and others},
  journal = {arXiv preprint arXiv:2512.02556},
  year    = {2025}
}

@article{turc2019well,
  title   = {Well-read students learn better: On the importance of pre-training compact models},
  author  = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1908.08962},
  year    = {2019}
}