# From-Scratch Transformer Classifier ‚Äî One-Page Quick Use (README)

Minimal, practical guide to run **MLM pretraining** and **classification finetuning**. Up-to-date with this repo‚Äôs CLI and layout.

---

## üó∫ Repo Map (where things live)

- **CLI (run this)**
  - `train.py` ‚Äî run training with a mode flag:
    - `-m pretraining` for MLM pretraining
    - `-m finetuning` for classification finetuning
- **Templates & Experiment generators**
  - `config_templates/pretraining.yaml` ‚Äî template for MLM
  - `config_templates/finetuning.yaml` ‚Äî template for CLS
  - `experiments/generate_pretraining_experiment.py` ‚Äî creates `experiments/pretraining/<name>/` (use `-rp` to clone+resume an older run)
  - `experiments/generate_finetuning_experiment.py` ‚Äî creates `experiments/finetuning/<name>/` (links to a pretraining run and copies its tokenizer/architecture)
- **Generated runs**
  - `experiments/pretraining/<name>/` ‚Üí `config.yaml`, `metrics/train/*.csv` (if CSV logging enabled), `checkpoints/`, `model.ckpt`
  - `experiments/finetuning/<name>/` ‚Üí `config.yaml`, `metrics/train|eval/*.csv` (if CSV logging enabled), `checkpoints/`, `model.ckpt`
- **Library (`src/textclf_transformer/`)**
  - `models/` ‚Äî transformer blocks, heads, and variants (CLS, MLM)
  - `training/training_loop.py` ‚Äî TrainingLoop (AMP, grad accumulation, cosine LR, eval, checkpointing)
  - `training/utils/dataloader_utils.py` ‚Äî collate and DataLoader helpers (**expects `True = PAD`**)
  - `tokenizer/wordpiece_tokenizer_wrapper.py` ‚Äî wrapper with `load(...)`, `mask_input_for_mlm(...)`
  - `training/utils/config.py` ‚Äî seeding, dynamic import, arch kwargs
  - `logger/wandb_logger.py` ‚Äî CSV + optional W&B logging (see `logging.log_metrics_csv` flag)

---

## ‚öôÔ∏è Install (quick)

```bash
python -m venv .venv
source .venv/bin/activate            # Windows: .\.venv\Scripts\Activate.ps1

pip install --upgrade pip
pip install -r requirements.txt       # contains PyTorch, transformers, tokenizers, wandb, etc.
```

Notes:  
W&B is optional; local CSV logs are written only when `logging.log_metrics_csv=true` (templates default to false).  
Pydantic warnings are harmless.

---

## üì¶ Data Format (expected)

Pretraining (MLM) sample: (input_ids, attention_mask)
Finetuning (CLS) sample: (input_ids, attention_mask, labels)

Dtypes:
  input_ids: LongTensor
  attention_mask: BoolTensor (True = PAD)
  labels: LongTensor

Store as PyTorch objects (e.g., TensorDataset) via torch.save(...).

Batching: provided collate trims to the longest real seq per batch (ignoring PAD)
and caps to tokenizer.max_length.

---

## üß∞ Config ‚Äî what to edit (per experiment config.yaml)

- experiment ‚Äî metadata defining the run name, kind, output directory and deterministic seed.
- logging ‚Äî controls WandB, CSV dumps and eval logging toggles; set `log_metrics_csv: true` to emit local CSV metrics (templates default to false).
- tokenizer ‚Äî wrapper path, vocabulary source, and tokenization length limits.
- architecture ‚Äî backbone dimensions, attention style and positional encoding options.
- mlm_head (pretraining) / classification_head (finetuning) ‚Äî task-specific output layer settings.
- training ‚Äî optimizer/hyperparameter schedule, device selection, AMP, accumulation, and resume controls:
    - resume: `is_resume`, `resume_pretrainig_name`, `checkpoint_path` (relative to the *new* experiment dir unless absolute), `strict`, `load_only_model_state` (set to `false` to also restore optimizer/scheduler/scaler/best_val_loss/epoch/step).
- pretrained_experiment (finetuning) ‚Äî links back to the checkpoint that seeds the downstream run (filled automatically by the finetuning generator).
- data ‚Äî paths to serialized `.pt` datasets for train/val/test splits.

---

## üß™ Pretraining (MLM) ‚Äî create & run

``` bash
# Generate a fresh run
python experiments/generate_pretraining_experiment.py -p <pretrain_name>

# Clone an existing run to resume it (copies config + fills resume block)
python experiments/generate_pretraining_experiment.py -p <new_name> -rp <old_name>

# Edit experiments/pretraining/<pretrain_name>/config.yaml:
#   - Set data.train.dataset_path (your MLM .pt)
#   - Set tokenizer.wrapper_path & tokenizer.vocab_dir
#   - Adjust architecture, training, and optional mlm_head
#   - To resume manually: set training.resume.is_resume=true and point
#     training.resume.checkpoint_path to the checkpoint you want to continue from
#     (defaults to ../<old_name>/checkpoints/model.ckpt when using -rp)

# Run
python train.py -n <pretrain_name> -m pretraining

# Outputs:
#   Final checkpoint ‚Üí checkpoints/model.ckpt (model weights only)
#   Best checkpoint (if val set) ‚Üí checkpoints/best-model.ckpt (full state incl. optimizer/scheduler/scaler)
#   CSV metrics ‚Üí metrics/train/*.csv when logging.log_metrics_csv=true; otherwise W&B only
#   Optional W&B run
```

---

## üéØ Finetuning (CLS) ‚Äî create & run

``` bash
# Generate from pretraining (copies architecture/tokenizer from the pretraining config)
python experiments/generate_finetuning_experiment.py -f <finetune_name> -p <pretrain_name>

# Edit experiments/finetuning/<finetune_name>/config.yaml:
#   - Set data.train/val/test.dataset_path (your CLS .pt)
#   - Confirm pretrained_experiment.path + checkpoint exist (model.ckpt from pretraining)
#   - Set classification_head.num_labels (+ pooling/dropout if needed)

# Run
python train.py -n <finetune_name> -m finetuning

# Outputs:
#   Final checkpoint ‚Üí checkpoints/model.ckpt (model weights only)
#   Best checkpoint (if val set) ‚Üí checkpoints/best-model.ckpt
#   CSV metrics ‚Üí metrics/train/*.csv, metrics/eval/*.csv when logging.log_metrics_csv=true; otherwise W&B only
#   Optional W&B run
```

---

## ‚úÖ Conventions & Tips

```
- Mask semantics: collate expects True = PAD (pad_is_true_mask=True)
- Datasets: encode() in WordPiece wrapper already flips attention_mask to bool with PAD=True
- Collate trims each batch to the longest real sequence and caps to tokenizer.max_length
- Device selection: training.device: auto uses CUDA if available; forcing cuda without CUDA raises an error
- Mixed precision: training.use_amp: true (CUDA only) for speed
- LR schedule: cosine with optional linear warmup via training.warmup_ratio
- Logging: toggle W&B with logging.use_wandb; enable logging.log_metrics_csv for local CSV fallback/offline use
- Test split: evaluated automatically only for finetuning when data.test.dataset_path is set
```

---

## ‚õ≥ Cheat Sheet (copy & run)

``` bash
# Pretraining
python experiments/generate_pretraining_experiment.py -p pre_v1
# (resume example) 
python experiments/generate_pretraining_experiment.py -p pre_v1b -rp pre_v1
# edit: experiments/pretraining/pre_v1/config.yaml
#        set logging.log_metrics_csv=true if you need local CSV logs
python train.py -n pre_v1 -m pretraining

# Finetuning
python experiments/generate_finetuning_experiment.py -f ft_v1 -p pre_v1
# edit: experiments/finetuning/ft_v1/config.yaml
python train.py -n ft_v1 -m finetuning
```
