{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8db5da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from result_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import re\n",
    "\n",
    "def parse_run_name(df):\n",
    "    def parse_row(run_name):\n",
    "        if 'pretraining' in run_name and 'wikipedia' in run_name:\n",
    "            stage = 'pretrain'\n",
    "        elif 'pretraining' in run_name and any(d in run_name for d in ['arxiv', 'hyperpartisan', 'imdb']):\n",
    "            stage = 'tapt'\n",
    "        elif 'finetuning' in run_name:\n",
    "            stage = 'finetune'\n",
    "        else:\n",
    "            stage = 'unknown'\n",
    "        \n",
    "        for dataset in ['wikipedia', 'arxiv', 'hyperpartisan', 'imdb']:\n",
    "            if dataset in run_name:\n",
    "                break\n",
    "        else:\n",
    "            dataset = 'unknown'\n",
    "        \n",
    "        if '_mha' in run_name:\n",
    "            arch = 'mha'\n",
    "        elif match := re.search(r'favor_nb([\\d.]+)', run_name):\n",
    "            arch = f'favor_nb{match.group(1)}'\n",
    "        elif match := re.search(r'lsh_h(\\d+)_c(\\d+)', run_name):\n",
    "            arch = f'lsh_h{match.group(1)}_c{match.group(2)}'\n",
    "        else:\n",
    "            arch = 'unknown'\n",
    "        \n",
    "        return pd.Series({'stage': stage, 'dataset': dataset, 'arch': arch})\n",
    "    \n",
    "    parsed = df['run_name'].apply(parse_row)\n",
    "    return pd.concat([df, parsed], axis=1)\n",
    "\n",
    "\n",
    "def merge_stages(df_pretrain: pd.DataFrame, \n",
    "                 df_tapt: pd.DataFrame, \n",
    "                 df_finetune: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Łączy 3 tabele etapów:\n",
    "    1. pretrain + tapt -> join po 'arch'\n",
    "    2. wynik + finetune -> join po ['dataset', 'arch']\n",
    "    \"\"\"\n",
    "    pretrain = df_pretrain.rename(columns={\n",
    "        col: f'pretrain/{col}' \n",
    "        for col in df_pretrain.columns \n",
    "        if col not in ['dataset', 'arch']\n",
    "    })\n",
    "    \n",
    "    tapt = df_tapt.rename(columns={\n",
    "        col: f'tapt/{col}' \n",
    "        for col in df_tapt.columns \n",
    "        if col not in ['dataset', 'arch']\n",
    "    })\n",
    "    \n",
    "    finetune = df_finetune.rename(columns={\n",
    "        col: f'finetune/{col}' \n",
    "        for col in df_finetune.columns \n",
    "        if col not in ['dataset', 'arch']\n",
    "    })\n",
    "    \n",
    "    pretrain_cols = [c for c in pretrain.columns if c != 'dataset']\n",
    "    result = tapt.merge(pretrain[pretrain_cols], on='arch', how='left')\n",
    "    \n",
    "    result = result.merge(finetune, on=['dataset', 'arch'], how='outer')\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_relative_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Oblicza:\n",
    "    1. Zmianę procentową względem MHA (duration, RAM) w obrębie datasetu\n",
    "    2. Różnicę w punktach procentowych F1 względem baseline w obrębie datasetu\n",
    "    3. Różnicę w punktach procentowych F1 względem MHA w obrębie datasetu\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    pct_cols = ['finetune_tapt/train/gpu_mem_peak_gb', 'finetune_tapt/duration_min']\n",
    "    \n",
    "    f1_col = 'finetune/test/f1_macro'\n",
    "    \n",
    "    for dataset in df['dataset'].unique():\n",
    "        mask = df['dataset'] == dataset\n",
    "        \n",
    "        mha_mask = mask & (df['arch'] == 'mha')\n",
    "        if mha_mask.sum() > 0:\n",
    "            for col in pct_cols:\n",
    "                mha_val = df.loc[mha_mask, col].values[0]\n",
    "                if mha_val != 0:\n",
    "                    result.loc[mask, f'{col}_pct_vs_mha'] = ((df.loc[mask, col] - mha_val) / mha_val) * 100\n",
    "            \n",
    "            mha_f1 = df.loc[mha_mask, f1_col].values[0]\n",
    "            result.loc[mask, f'{f1_col}_pp_vs_mha'] = (df.loc[mask, f1_col] - mha_f1)\n",
    "        \n",
    "        baseline_mask = mask & (df['arch'] == 'baseline')\n",
    "        if baseline_mask.sum() > 0:\n",
    "            baseline_f1 = df.loc[baseline_mask, f1_col].values[0]\n",
    "            result.loc[mask, f'{f1_col}_pp_vs_baseline'] = (df.loc[mask, f1_col] - baseline_f1) \n",
    "    \n",
    "    return result\n",
    "\n",
    "def generate_latex_table_simple(\n",
    "    df: pd.DataFrame,\n",
    "    metrics: list[dict],\n",
    "    filter_col: str = None,\n",
    "    filter_val: str = None,\n",
    ") -> str:\n",
    "    \"\"\"Prosta tabela - metryki jako kolumny (bez podziału na datasety).\"\"\"\n",
    "    \n",
    "    arch_config = {\n",
    "        'mha': {'label': r'\\textit{SDPA}', 'group': None},\n",
    "        'lsh_h2_c64': {'label': r'$N_h{=}2$, $C{=}64$', 'group': 'LSH'},\n",
    "        'lsh_h2_c128': {'label': r'$N_h{=}2$, $C{=}128$', 'group': 'LSH'},\n",
    "        'lsh_h4_c64': {'label': r'$N_h{=}4$, $C{=}64$', 'group': 'LSH'},\n",
    "        'lsh_h4_c128': {'label': r'$N_h{=}4$, $C{=}128$', 'group': 'LSH'},\n",
    "        'favor_nb0.125': {'label': r'$N_f{=}0.125$', 'group': 'FAVOR+'},\n",
    "        'favor_nb0.25': {'label': r'$N_f{=}0.25$', 'group': 'FAVOR+'},\n",
    "        'favor_nb0.5': {'label': r'$N_f{=}0.5$', 'group': 'FAVOR+'},\n",
    "        'favor_nb1': {'label': r'$N_f{=}1.0$', 'group': 'FAVOR+'},\n",
    "    }\n",
    "    \n",
    "    arch_order = ['mha', 'lsh_h2_c64', 'lsh_h2_c128', 'lsh_h4_c64', 'lsh_h4_c128',\n",
    "                  'favor_nb0.125', 'favor_nb0.25', 'favor_nb0.5', 'favor_nb1']\n",
    "    \n",
    "    data = df\n",
    "    if filter_col and filter_val:\n",
    "        data = df[df[filter_col] == filter_val]\n",
    "    \n",
    "    n_metrics = len(metrics)\n",
    "    n_cols = n_metrics + 1\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(r'\\begin{tabular}{@{}l' + 'c' * n_metrics + '@{}}')\n",
    "    lines.append(r'\\toprule')\n",
    "    \n",
    "    metric_header = ' & '.join([f'\\\\textbf{{{m[\"label\"]}}}' for m in metrics])\n",
    "    lines.append(f'\\\\textbf{{Konfiguracja}} & {metric_header} \\\\\\\\')\n",
    "    lines.append(r'\\midrule')\n",
    "    \n",
    "    def get_value(arch, metric_cfg):\n",
    "        row = data[data['arch'] == arch]\n",
    "        if row.empty:\n",
    "            return '-'\n",
    "        val = row[metric_cfg['col']].values[0]\n",
    "        if pd.isna(val):\n",
    "            return '-'\n",
    "        suffix = metric_cfg.get('suffix', '')\n",
    "        return metric_cfg['fmt'].format(val) + suffix\n",
    "    \n",
    "    current_group = None\n",
    "    for arch in arch_order:\n",
    "        if arch not in arch_config:\n",
    "            continue\n",
    "        cfg = arch_config[arch]\n",
    "        \n",
    "        if cfg['group'] != current_group:\n",
    "            if cfg['group'] is not None:\n",
    "                lines.append(r'\\midrule')\n",
    "                lines.append(f'\\\\multicolumn{{{n_cols}}}{{l}}{{\\\\textit{{{cfg[\"group\"]}}}}} \\\\\\\\')\n",
    "            current_group = cfg['group']\n",
    "        \n",
    "        values = [get_value(arch, m) for m in metrics]\n",
    "        lines.append(f'{cfg[\"label\"]} & {\" & \".join(values)} \\\\\\\\')\n",
    "    \n",
    "    lines.append(r'\\bottomrule')\n",
    "    lines.append(r'\\end{tabular}')\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def generate_latex_table(\n",
    "    df: pd.DataFrame,\n",
    "    metrics: list[dict], \n",
    "    datasets: list[str] = ['imdb', 'hyperpartisan', 'arxiv'],\n",
    "    dataset_labels: dict = None,\n",
    "    show_baseline: bool = False,\n",
    "    show_sdpa: bool = False,\n",
    "    combined_cell: dict = None,\n",
    "    extra_col: dict = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Tabela z metrykami jako kolumny nadrzędne, datasetami jako podkolumny.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_labels = dataset_labels or {\n",
    "        'imdb': 'IMDB', \n",
    "        'hyperpartisan': 'Hyper.', \n",
    "        'arxiv': 'Arxiv'\n",
    "    }\n",
    "    \n",
    "    arch_config = {\n",
    "        'mha': {'label': r'\\textit{SDPA}', 'group': None},\n",
    "        'lsh_h2_c64': {'label': r'$N_h{=}2$, $C{=}64$', 'group': 'LSH'},\n",
    "        'lsh_h2_c128': {'label': r'$N_h{=}2$, $C{=}128$', 'group': 'LSH'},\n",
    "        'lsh_h4_c64': {'label': r'$N_h{=}4$, $C{=}64$', 'group': 'LSH'},\n",
    "        'lsh_h4_c128': {'label': r'$N_h{=}4$, $C{=}128$', 'group': 'LSH'},\n",
    "        'favor_nb0.125': {'label': r'$N_f{=}0.125$', 'group': 'FAVOR+'},\n",
    "        'favor_nb0.25': {'label': r'$N_f{=}0.25$', 'group': 'FAVOR+'},\n",
    "        'favor_nb0.5': {'label': r'$N_f{=}0.5$', 'group': 'FAVOR+'},\n",
    "        'favor_nb1': {'label': r'$N_f{=}1.0$', 'group': 'FAVOR+'},\n",
    "    }\n",
    "    \n",
    "    arch_order = ['mha', 'lsh_h2_c64', 'lsh_h2_c128', 'lsh_h4_c64', 'lsh_h4_c128',\n",
    "                  'favor_nb0.125', 'favor_nb0.25', 'favor_nb0.5', 'favor_nb1']\n",
    "    \n",
    "    arch_order_no_mha = ['lsh_h2_c64', 'lsh_h2_c128', 'lsh_h4_c64', 'lsh_h4_c128',\n",
    "                         'favor_nb0.125', 'favor_nb0.25', 'favor_nb0.5', 'favor_nb1']\n",
    "    \n",
    "    n_metrics = len(metrics)\n",
    "    n_datasets = len(datasets)\n",
    "    \n",
    "    has_extra = extra_col is not None\n",
    "    extra_cols = 1 if has_extra else 0\n",
    "    \n",
    "    n_cols = n_metrics * n_datasets + 1 + extra_cols\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(r'\\begin{tabular}{@{}l' + 'c' * (n_cols - 1) + '@{}}')\n",
    "    lines.append(r'\\toprule')\n",
    "    \n",
    "    if has_extra:\n",
    "        metric_header = ' & '.join([\n",
    "            f'\\\\multicolumn{{{n_datasets}}}{{c}}{{\\\\textbf{{{m[\"label\"]}}}}}'\n",
    "            for m in metrics\n",
    "        ])\n",
    "        lines.append(f'& \\\\textbf{{{extra_col[\"label\"]}}} & {metric_header} \\\\\\\\')\n",
    "        \n",
    "        lines.append(r'\\cmidrule(lr){2-2}')\n",
    "        for i, _ in enumerate(metrics):\n",
    "            start = 3 + i * n_datasets\n",
    "            end = start + n_datasets - 1\n",
    "            lines.append(f'\\\\cmidrule(lr){{{start}-{end}}}')\n",
    "        \n",
    "        dataset_header = ' & '.join([f'\\\\textbf{{{dataset_labels[d]}}}' for d in datasets] * n_metrics)\n",
    "        lines.append(f'\\\\textbf{{Model}} & \\\\textbf{{Wikipedia}} & {dataset_header} \\\\\\\\')\n",
    "    else:\n",
    "        if n_metrics > 1:\n",
    "            metric_header = ' & '.join([\n",
    "                f'\\\\multicolumn{{{n_datasets}}}{{c}}{{\\\\textbf{{{m[\"label\"]}}}}}'\n",
    "                for m in metrics\n",
    "            ])\n",
    "            lines.append(f'& {metric_header} \\\\\\\\')\n",
    "            \n",
    "            for i, _ in enumerate(metrics):\n",
    "                start = 2 + i * n_datasets\n",
    "                end = start + n_datasets - 1\n",
    "                lines.append(f'\\\\cmidrule(lr){{{start}-{end}}}')\n",
    "            \n",
    "            dataset_header = ' & '.join([f'\\\\textbf{{{dataset_labels[d]}}}' for d in datasets] * n_metrics)\n",
    "            lines.append(f'\\\\textbf{{Model}} & {dataset_header} \\\\\\\\')\n",
    "        else:\n",
    "\n",
    "            dataset_header = ' & '.join([f'\\\\textbf{{{dataset_labels[d]}}}' for d in datasets])\n",
    "            lines.append(f'\\\\textbf{{Model}} & {dataset_header} \\\\\\\\')\n",
    "    \n",
    "    lines.append(r'\\midrule')\n",
    "    \n",
    "    def get_val(arch, dataset, col):\n",
    "        row = df[(df['arch'] == arch) & (df['dataset'] == dataset)]\n",
    "        if row.empty:\n",
    "            return None\n",
    "        return row[col].values[0]\n",
    "    \n",
    "    if show_baseline:\n",
    "        baseline_cells = []\n",
    "        if has_extra:\n",
    "            baseline_cells.append('-')\n",
    "        for m in metrics:\n",
    "            for d in datasets:\n",
    "                val = get_val('baseline', d, m['col'])\n",
    "                if val is not None and not pd.isna(val):\n",
    "                    suffix = m.get('suffix', '')\n",
    "                    baseline_cells.append(m['fmt'].format(val) + suffix)\n",
    "                else:\n",
    "                    baseline_cells.append('-')\n",
    "        lines.append(f'\\\\textit{{TF-IDF + LR}} & {\" & \".join(baseline_cells)} \\\\\\\\')\n",
    "        lines.append(r'\\midrule')\n",
    "    \n",
    "    if show_sdpa:\n",
    "        sdpa_cells = []\n",
    "        if has_extra:\n",
    "            sdpa_cells.append('-')\n",
    "        for m in metrics:\n",
    "            for d in datasets:\n",
    "                val = get_val('mha', d, m['col'])\n",
    "                diff = get_val('mha', d, m['col'] + '_pp_vs_baseline')\n",
    "                if val is not None and not pd.isna(val):\n",
    "                    suffix = m.get('suffix', '')\n",
    "                    cell = m['fmt'].format(val) + suffix\n",
    "                    if diff is not None and not pd.isna(diff):\n",
    "                        cell += f' ({diff:+.1f})'\n",
    "                    sdpa_cells.append(cell)\n",
    "                else:\n",
    "                    sdpa_cells.append('-')\n",
    "        lines.append(f'\\\\textit{{SDPA}} (vs TF-IDF+LR) & {\" & \".join(sdpa_cells)} \\\\\\\\')\n",
    "    \n",
    "    use_arch_order = arch_order_no_mha if show_sdpa else arch_order\n",
    "    \n",
    "    current_group = None\n",
    "    for arch in use_arch_order:\n",
    "        if arch not in arch_config:\n",
    "            continue\n",
    "        \n",
    "        cfg = arch_config[arch]\n",
    "        \n",
    "        if cfg['group'] != current_group:\n",
    "            if cfg['group'] is not None:\n",
    "                lines.append(r'\\midrule')\n",
    "                if combined_cell:\n",
    "                    group_label = f'{cfg[\"group\"]} (vs SDPA / vs TF-IDF+LR)'\n",
    "                else:\n",
    "                    group_label = cfg['group']\n",
    "                lines.append(f'\\\\multicolumn{{{n_cols}}}{{l}}{{\\\\textit{{{group_label}}}}} \\\\\\\\')\n",
    "            current_group = cfg['group']\n",
    "        \n",
    "        values = []\n",
    "        \n",
    "        if has_extra:\n",
    "            extra_m = extra_col['metrics'][0]\n",
    "            val = get_val(arch, datasets[0], extra_m['col'])\n",
    "            if val is not None and not pd.isna(val):\n",
    "                suffix = extra_m.get('suffix', '')\n",
    "                values.append(extra_m['fmt'].format(val) + suffix)\n",
    "            else:\n",
    "                values.append('-')\n",
    "        \n",
    "        if combined_cell:\n",
    "            for d in datasets:\n",
    "                parts = []\n",
    "                for col in combined_cell['cols']:\n",
    "                    val = get_val(arch, d, col)\n",
    "                    if val is not None and not pd.isna(val):\n",
    "                        parts.append(combined_cell['fmt'].format(val))\n",
    "                    else:\n",
    "                        parts.append('-')\n",
    "                values.append(combined_cell['sep'].join(parts))\n",
    "        else:\n",
    "            for m in metrics:\n",
    "                for d in datasets:\n",
    "                    val = get_val(arch, d, m['col'])\n",
    "                    if val is not None and not pd.isna(val):\n",
    "                        suffix = m.get('suffix', '')\n",
    "                        values.append(m['fmt'].format(val) + suffix)\n",
    "                    else:\n",
    "                        values.append('-')\n",
    "        \n",
    "        lines.append(f'{cfg[\"label\"]} & {\" & \".join(values)} \\\\\\\\')\n",
    "    \n",
    "    lines.append(r'\\bottomrule')\n",
    "    lines.append(r'\\end{tabular}')\n",
    "    \n",
    "    return '\\n'.join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92641986",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments = get_experiments_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dbd07946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hd228t3k': 'E1_pretraining_wikipedia_bertsmall_mha',\n",
       " 'ghydwtq8': 'E1_pretraining_imdb_bertsmall_mha',\n",
       " 'gnty8r83': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.1_cls',\n",
       " 'dz8ut8xo': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.1_mean',\n",
       " 'xzv6zdrh': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.2_cls',\n",
       " 'izheqn5h': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.2_mean',\n",
       " 'y6w5l42k': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.1_cls',\n",
       " 'dtucpxrs': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.1_mean',\n",
       " 'gkglmyzr': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.2_cls',\n",
       " 'haph0z0a': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.2_mean',\n",
       " 'dqnki4qe': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.1_cls',\n",
       " 'sj6wkwyl': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.1_mean',\n",
       " 'd8wyaq7o': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.2_cls',\n",
       " 'l2jq2x7g': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean',\n",
       " 'z28fmtgo': 'E2_pretraining_wikipedia_bertsmall_favor_nb0.125',\n",
       " '0itox8yh': 'E2_pretraining_wikipedia_bertsmall_favor_nb0.25',\n",
       " '0e2zxgor': 'E2_pretraining_wikipedia_bertsmall_lsh_h4_c128',\n",
       " 'yjcapkj3': 'E2_pretraining_wikipedia_bertsmall_favor_nb0.5',\n",
       " 'wi223o9o': 'E2_pretraining_wikipedia_bertsmall_lsh_h4_c64',\n",
       " 'noug0x9s': 'E2_pretraining_wikipedia_bertsmall_favor_nb1',\n",
       " 'mzlsnnzy': 'E2_pretraining_imdb_bertsmall_favor_nb0.125',\n",
       " '1qxi1tf7': 'E2_pretraining_imdb_bertsmall_favor_nb0.25',\n",
       " '1crsv1ae': 'E2_pretraining_imdb_bertsmall_favor_nb0.5',\n",
       " 'eudij3yf': 'E2_pretraining_wikipedia_bertsmall_lsh_h2_c64',\n",
       " 'hu06m597': 'E2_pretraining_wikipedia_bertsmall_lsh_h2_c128',\n",
       " 'd659uj2j': 'E2_finetuning_imdb_bertsmall_favor_nb0.125',\n",
       " 'if2ozrk2': 'E2_finetuning_imdb_bertsmall_favor_nb0.25',\n",
       " 'hhoyoxfp': 'E2_finetuning_imdb_bertsmall_favor_nb0.5',\n",
       " 'jucxpr34': 'E2_pretraining_imdb_bertsmall_favor_nb1',\n",
       " 'u44pgkly': 'E2_finetuning_imdb_bertsmall_favor_nb1',\n",
       " 'a16qafd4': 'E2_pretraining_imdb_bertsmall_lsh_h2_c64',\n",
       " 'o031jp0p': 'E2_pretraining_imdb_bertsmall_lsh_h2_c128',\n",
       " '9fnn58aq': 'E2_pretraining_imdb_bertsmall_lsh_h4_c128',\n",
       " '2fpocsb8': 'E2_pretraining_imdb_bertsmall_lsh_h4_c64',\n",
       " 'tq5q132s': 'E2_finetuning_imdb_bertsmall_lsh_h2_c128',\n",
       " 'j2x1q39c': 'E2_finetuning_imdb_bertsmall_lsh_h2_c64',\n",
       " 's1c2rnaf': 'E2_finetuning_imdb_bertsmall_lsh_h4_c128',\n",
       " 'u262vax9': 'E2_finetuning_imdb_bertsmall_lsh_h4_c64',\n",
       " 'jibomvsj': 'E1_pretraining_arxiv_bertsmall_mha',\n",
       " '5g4wrk2x': 'E1_finetuning_arxiv_bertsmall_mha_f0_d0.1_cls',\n",
       " 'tzihl99m': 'E2_pretraining_hyperpartisan_bertsmall_favor_nb0.125',\n",
       " 'vi5hkih4': 'E1_pretraining_hyperpartisan_bertsmall_mha',\n",
       " '71eagzdx': 'E2_pretraining_hyperpartisan_bertsmall_favor_nb0.25',\n",
       " 'arurxxaw': 'E1_finetuning_hyperpartisan_bertsmall_mha_f0_d0.1_cls',\n",
       " 'tpv93ewu': 'E1_finetuning_arxiv_bertsmall_mha_f0_d0.1_mean',\n",
       " 'yl0svm2m': 'E1_finetuning_hyperpartisan_bertsmall_mha_f0_d0.2_cls',\n",
       " 'pu4bht10': 'E2_pretraining_hyperpartisan_bertsmall_favor_nb0.5',\n",
       " 'leh043an': 'E1_finetuning_hyperpartisan_bertsmall_mha_f1_d0.1_cls',\n",
       " 'bgi1ilvg': 'E1_finetuning_hyperpartisan_bertsmall_mha_f1_d0.2_cls',\n",
       " 'vyeowm82': 'E2_pretraining_hyperpartisan_bertsmall_favor_nb1',\n",
       " 'jrtiv3gp': 'E1_finetuning_arxiv_bertsmall_mha_f0_d0.2_cls',\n",
       " 'd8dksdry': 'E1_finetuning_hyperpartisan_bertsmall_mha_f2_d0.1_cls',\n",
       " 'xv1tdw9m': 'E1_finetuning_hyperpartisan_bertsmall_mha_f2_d0.2_cls',\n",
       " '2g7hhgqh': 'E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64',\n",
       " 'fcag25ot': 'E1_finetuning_arxiv_bertsmall_mha_f0_d0.2_mean',\n",
       " 'i1bylyot': 'E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c128',\n",
       " 'm6psyu6o': 'E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64',\n",
       " 'fu6xtm94': 'E1_finetuning_arxiv_bertsmall_mha_f1_d0.1_cls',\n",
       " 'o9riawb7': 'E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c128',\n",
       " 'd666p1hh': 'E1_finetuning_arxiv_bertsmall_mha_f1_d0.1_mean',\n",
       " '9bevf2pd': 'E1_finetuning_arxiv_bertsmall_mha_f2_d0.1_cls',\n",
       " 'kb5ewjpp': 'E2_pretraining_arxiv_bertsmall_lsh_h2_c64',\n",
       " '4h910kzj': 'E2_pretraining_arxiv_bertsmall_lsh_h2_c128',\n",
       " 'prjwiyzb': 'E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_cls',\n",
       " 'z1bfydwh': 'E1_finetuning_arxiv_bertsmall_mha_f2_d0.1_mean',\n",
       " '9hb1xk49': 'E2_pretraining_arxiv_bertsmall_lsh_h4_c64',\n",
       " '7olzfob3': 'E2_pretraining_arxiv_bertsmall_lsh_h4_c128',\n",
       " 't67wmtc1': 'E2_pretraining_arxiv_bertsmall_favor_nb0.125',\n",
       " 'aplguq3f': 'E2_pretraining_arxiv_bertsmall_favor_nb0.25',\n",
       " 'k9km9nuj': 'E2_pretraining_arxiv_bertsmall_favor_nb0.5',\n",
       " 'p0sm9w3v': 'E2_pretraining_arxiv_bertsmall_favor_nb1',\n",
       " 'lho0p773': 'E1_finetuning_arxiv_bertsmall_mha_f2_d0.2_cls',\n",
       " '0srvc8xd': 'E1_finetuning_hyperpartisan_bertsmall_mha_f0_d0.1_mean',\n",
       " 'rsdv117o': 'E1_finetuning_hyperpartisan_bertsmall_mha_f0_d0.2_mean',\n",
       " '4hjx43mn': 'E1_finetuning_arxiv_bertsmall_mha_f2_d0.2_mean',\n",
       " 'ulsu61q4': 'E1_finetuning_hyperpartisan_bertsmall_mha_f1_d0.1_mean',\n",
       " 'nht4xqr4': 'E1_finetuning_hyperpartisan_bertsmall_mha_f1_d0.2_mean',\n",
       " 'kiez0hnp': 'E1_finetuning_hyperpartisan_bertsmall_mha_f2_d0.1_mean',\n",
       " 'cpzdzfbw': 'E1_finetuning_hyperpartisan_bertsmall_mha_f2_d0.2_mean',\n",
       " 'xhbujbys': 'E2_finetuning_hyperpartisan_bertsmall_favor_nb0.125',\n",
       " '3klrqv94': 'E2_finetuning_hyperpartisan_bertsmall_favor_nb0.25',\n",
       " '3jtkh8iw': 'E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5',\n",
       " 'k2j8xkd0': 'E2_finetuning_hyperpartisan_bertsmall_favor_nb1',\n",
       " 'ys8qnftm': 'E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64',\n",
       " 'yjqias1k': 'E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128',\n",
       " '16tul7l4': 'E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64',\n",
       " 'ied1ulsn': 'E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean',\n",
       " 'gyaw279z': 'E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128',\n",
       " 'fs5plm2w': 'E2_finetuning_arxiv_bertsmall_favor_nb0.125',\n",
       " 'xf2puymg': 'E2_finetuning_arxiv_bertsmall_lsh_h2_c128',\n",
       " 'tug4ff1q': 'E2_finetuning_arxiv_bertsmall_favor_nb0.25',\n",
       " 'ebmfez6l': 'E2_finetuning_arxiv_bertsmall_lsh_h2_c64',\n",
       " '3ksmzwr0': 'E2_finetuning_arxiv_bertsmall_favor_nb0.5',\n",
       " 'wqah0rrq': 'E2_finetuning_arxiv_bertsmall_lsh_h4_c128',\n",
       " '0zr9387l': 'E2_finetuning_arxiv_bertsmall_favor_nb1',\n",
       " 'v0prpm6r': 'E2_finetuning_arxiv_bertsmall_lsh_h4_c64',\n",
       " 'aejc1ghu': 'E2_pretraining_wikipedia_bertsmall_lsh_h2_c64_mask',\n",
       " '49vu3wc0': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.1_cls',\n",
       " 'n5tmp5w3': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.2_cls',\n",
       " 'wegquwq3': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.1_cls',\n",
       " 'pjza7dd3': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.2_cls',\n",
       " '804gqgud': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.1_cls',\n",
       " 'cgkuvivl': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.2_cls',\n",
       " '7d02hzjq': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.1_mean',\n",
       " 'yf2srys7': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.2_mean',\n",
       " '3zjsb303': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.1_mean',\n",
       " '2o0ztly6': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.2_mean',\n",
       " 'rdcy5kh9': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.1_mean',\n",
       " 'uh18yeka': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean',\n",
       " 'c22fnp0d': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.1_cls',\n",
       " 's4c75y6h': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.2_cls',\n",
       " '0zfk6lpb': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.1_cls',\n",
       " 'qutg21sp': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.2_cls',\n",
       " 'lwxdech2': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.1_cls',\n",
       " 'ws6uehtf': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.2_cls',\n",
       " '864fwbis': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.1_mean',\n",
       " '7jj3kdxt': 'E1_finetuning_imdb_bertsmall_mha_f0_d0.2_mean',\n",
       " 'z74wx0dt': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.1_mean',\n",
       " 'xn7xqhu4': 'E1_finetuning_imdb_bertsmall_mha_f1_d0.2_mean',\n",
       " 'eadyrdve': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.1_mean',\n",
       " '1v4i3r18': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean',\n",
       " 'aoabu8dl': 'pretraining_arxiv_favor_from_sdpa',\n",
       " 'qser8y2s': 'finetuning_arxiv_favor_from_sdpa'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4f222",
   "metadata": {},
   "source": [
    "# main_result_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00f35e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results_runs = {\n",
    " 'hd228t3k': 'E1_pretraining_wikipedia_bertsmall_mha',\n",
    " 'ghydwtq8': 'E1_pretraining_imdb_bertsmall_mha',\n",
    " 'l2jq2x7g': 'E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean',\n",
    " 'z28fmtgo': 'E2_pretraining_wikipedia_bertsmall_favor_nb0.125',\n",
    " '0itox8yh': 'E2_pretraining_wikipedia_bertsmall_favor_nb0.25',\n",
    " '0e2zxgor': 'E2_pretraining_wikipedia_bertsmall_lsh_h4_c128',\n",
    " 'yjcapkj3': 'E2_pretraining_wikipedia_bertsmall_favor_nb0.5',\n",
    " 'wi223o9o': 'E2_pretraining_wikipedia_bertsmall_lsh_h4_c64',\n",
    " 'noug0x9s': 'E2_pretraining_wikipedia_bertsmall_favor_nb1',\n",
    " 'mzlsnnzy': 'E2_pretraining_imdb_bertsmall_favor_nb0.125',\n",
    " '1qxi1tf7': 'E2_pretraining_imdb_bertsmall_favor_nb0.25',\n",
    " '1crsv1ae': 'E2_pretraining_imdb_bertsmall_favor_nb0.5',\n",
    " 'eudij3yf': 'E2_pretraining_wikipedia_bertsmall_lsh_h2_c64',\n",
    " 'hu06m597': 'E2_pretraining_wikipedia_bertsmall_lsh_h2_c128',\n",
    " 'd659uj2j': 'E2_finetuning_imdb_bertsmall_favor_nb0.125',\n",
    " 'if2ozrk2': 'E2_finetuning_imdb_bertsmall_favor_nb0.25',\n",
    " 'hhoyoxfp': 'E2_finetuning_imdb_bertsmall_favor_nb0.5',\n",
    " 'jucxpr34': 'E2_pretraining_imdb_bertsmall_favor_nb1',\n",
    " 'u44pgkly': 'E2_finetuning_imdb_bertsmall_favor_nb1',\n",
    " 'a16qafd4': 'E2_pretraining_imdb_bertsmall_lsh_h2_c64',\n",
    " 'o031jp0p': 'E2_pretraining_imdb_bertsmall_lsh_h2_c128',\n",
    " '9fnn58aq': 'E2_pretraining_imdb_bertsmall_lsh_h4_c128',\n",
    " '2fpocsb8': 'E2_pretraining_imdb_bertsmall_lsh_h4_c64',\n",
    " 'tq5q132s': 'E2_finetuning_imdb_bertsmall_lsh_h2_c128',\n",
    " 'j2x1q39c': 'E2_finetuning_imdb_bertsmall_lsh_h2_c64',\n",
    " 's1c2rnaf': 'E2_finetuning_imdb_bertsmall_lsh_h4_c128',\n",
    " 'u262vax9': 'E2_finetuning_imdb_bertsmall_lsh_h4_c64',\n",
    " 'jibomvsj': 'E1_pretraining_arxiv_bertsmall_mha',\n",
    " 'tzihl99m': 'E2_pretraining_hyperpartisan_bertsmall_favor_nb0.125',\n",
    " 'vi5hkih4': 'E1_pretraining_hyperpartisan_bertsmall_mha',\n",
    " '71eagzdx': 'E2_pretraining_hyperpartisan_bertsmall_favor_nb0.25',\n",
    " 'pu4bht10': 'E2_pretraining_hyperpartisan_bertsmall_favor_nb0.5',\n",
    " 'vyeowm82': 'E2_pretraining_hyperpartisan_bertsmall_favor_nb1',\n",
    " '2g7hhgqh': 'E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64',\n",
    " 'i1bylyot': 'E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c128',\n",
    " 'm6psyu6o': 'E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64',\n",
    " 'o9riawb7': 'E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c128',\n",
    " 'kb5ewjpp': 'E2_pretraining_arxiv_bertsmall_lsh_h2_c64',\n",
    " '4h910kzj': 'E2_pretraining_arxiv_bertsmall_lsh_h2_c128',\n",
    " '9hb1xk49': 'E2_pretraining_arxiv_bertsmall_lsh_h4_c64',\n",
    " '7olzfob3': 'E2_pretraining_arxiv_bertsmall_lsh_h4_c128',\n",
    " 't67wmtc1': 'E2_pretraining_arxiv_bertsmall_favor_nb0.125',\n",
    " 'aplguq3f': 'E2_pretraining_arxiv_bertsmall_favor_nb0.25',\n",
    " 'k9km9nuj': 'E2_pretraining_arxiv_bertsmall_favor_nb0.5',\n",
    " 'p0sm9w3v': 'E2_pretraining_arxiv_bertsmall_favor_nb1',\n",
    " 'ulsu61q4': 'E1_finetuning_hyperpartisan_bertsmall_mha_f1_d0.1_mean',\n",
    " 'xhbujbys': 'E2_finetuning_hyperpartisan_bertsmall_favor_nb0.125',\n",
    " '3klrqv94': 'E2_finetuning_hyperpartisan_bertsmall_favor_nb0.25',\n",
    " '3jtkh8iw': 'E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5',\n",
    " 'k2j8xkd0': 'E2_finetuning_hyperpartisan_bertsmall_favor_nb1',\n",
    " 'ys8qnftm': 'E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64',\n",
    " 'yjqias1k': 'E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128',\n",
    " '16tul7l4': 'E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64',\n",
    " 'ied1ulsn': 'E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean',\n",
    " 'gyaw279z': 'E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128',\n",
    " 'fs5plm2w': 'E2_finetuning_arxiv_bertsmall_favor_nb0.125',\n",
    " 'xf2puymg': 'E2_finetuning_arxiv_bertsmall_lsh_h2_c128',\n",
    " 'tug4ff1q': 'E2_finetuning_arxiv_bertsmall_favor_nb0.25',\n",
    " 'ebmfez6l': 'E2_finetuning_arxiv_bertsmall_lsh_h2_c64',\n",
    " '3ksmzwr0': 'E2_finetuning_arxiv_bertsmall_favor_nb0.5',\n",
    " 'wqah0rrq': 'E2_finetuning_arxiv_bertsmall_lsh_h4_c128',\n",
    " '0zr9387l': 'E2_finetuning_arxiv_bertsmall_favor_nb1',\n",
    " 'v0prpm6r': 'E2_finetuning_arxiv_bertsmall_lsh_h4_c64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "072cb6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1_pretraining_wikipedia_bertsmall_mha hd228t3k\n",
      "E1_pretraining_imdb_bertsmall_mha ghydwtq8\n",
      "E2_pretraining_wikipedia_bertsmall_favor_nb0.125 z28fmtgo\n",
      "E2_pretraining_wikipedia_bertsmall_favor_nb0.25 0itox8yh\n",
      "E2_pretraining_wikipedia_bertsmall_lsh_h4_c128 0e2zxgor\n",
      "E2_pretraining_wikipedia_bertsmall_favor_nb0.5 yjcapkj3\n",
      "E2_pretraining_wikipedia_bertsmall_lsh_h4_c64 wi223o9o\n",
      "E2_pretraining_wikipedia_bertsmall_favor_nb1 noug0x9s\n",
      "E2_pretraining_imdb_bertsmall_favor_nb0.125 mzlsnnzy\n",
      "E2_pretraining_imdb_bertsmall_favor_nb0.25 1qxi1tf7\n",
      "E2_pretraining_imdb_bertsmall_favor_nb0.5 1crsv1ae\n",
      "E2_pretraining_wikipedia_bertsmall_lsh_h2_c64 eudij3yf\n",
      "E2_pretraining_wikipedia_bertsmall_lsh_h2_c128 hu06m597\n",
      "E2_pretraining_imdb_bertsmall_favor_nb1 jucxpr34\n",
      "E2_pretraining_imdb_bertsmall_lsh_h2_c64 a16qafd4\n",
      "E2_pretraining_imdb_bertsmall_lsh_h2_c128 o031jp0p\n",
      "E2_pretraining_imdb_bertsmall_lsh_h4_c128 9fnn58aq\n",
      "E2_pretraining_imdb_bertsmall_lsh_h4_c64 2fpocsb8\n",
      "E1_pretraining_arxiv_bertsmall_mha jibomvsj\n",
      "E2_pretraining_hyperpartisan_bertsmall_favor_nb0.125 tzihl99m\n",
      "E1_pretraining_hyperpartisan_bertsmall_mha vi5hkih4\n",
      "E2_pretraining_hyperpartisan_bertsmall_favor_nb0.25 71eagzdx\n",
      "E2_pretraining_hyperpartisan_bertsmall_favor_nb0.5 pu4bht10\n",
      "E2_pretraining_hyperpartisan_bertsmall_favor_nb1 vyeowm82\n",
      "E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64 2g7hhgqh\n",
      "E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c128 i1bylyot\n",
      "E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64 m6psyu6o\n",
      "E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c128 o9riawb7\n",
      "E2_pretraining_arxiv_bertsmall_lsh_h2_c64 kb5ewjpp\n",
      "E2_pretraining_arxiv_bertsmall_lsh_h2_c128 4h910kzj\n",
      "E2_pretraining_arxiv_bertsmall_lsh_h4_c64 9hb1xk49\n",
      "E2_pretraining_arxiv_bertsmall_lsh_h4_c128 7olzfob3\n",
      "E2_pretraining_arxiv_bertsmall_favor_nb0.125 t67wmtc1\n",
      "E2_pretraining_arxiv_bertsmall_favor_nb0.25 aplguq3f\n",
      "E2_pretraining_arxiv_bertsmall_favor_nb0.5 k9km9nuj\n",
      "E2_pretraining_arxiv_bertsmall_favor_nb1 p0sm9w3v\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "names = []\n",
    "\n",
    "for run_id, name in main_results_runs.items():\n",
    "    if  'pretraining' in name:\n",
    "        ids.append(run_id)\n",
    "        names.append(name)\n",
    "        print(name, run_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97bc38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretrain = parse_run_name(build_metric_runtime_df(ids, names, [\n",
    "    {'metric_name': 'avg_epoch_loss', 'metric_scope': 'train', 'agg': 'min'},\n",
    "    {'metric_name': 'gpu_mem_peak_mb', 'metric_scope': 'train', 'agg': 'max'}\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e99674f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>run_name</th>\n",
       "      <th>duration_s</th>\n",
       "      <th>avg_epoch_time_s</th>\n",
       "      <th>train/avg_epoch_loss</th>\n",
       "      <th>train/gpu_mem_peak_mb</th>\n",
       "      <th>stage</th>\n",
       "      <th>dataset</th>\n",
       "      <th>arch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>E1_pretraining_wikipedia_bertsmall_mha</td>\n",
       "      <td>4728.551459</td>\n",
       "      <td>472.855146</td>\n",
       "      <td>2.156817</td>\n",
       "      <td>14785.845215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>mha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ghydwtq8</td>\n",
       "      <td>E1_pretraining_imdb_bertsmall_mha</td>\n",
       "      <td>1113.179355</td>\n",
       "      <td>74.211957</td>\n",
       "      <td>2.387619</td>\n",
       "      <td>14784.345215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>mha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>E2_pretraining_wikipedia_bertsmall_favor_nb0.125</td>\n",
       "      <td>6593.419117</td>\n",
       "      <td>659.341912</td>\n",
       "      <td>3.069484</td>\n",
       "      <td>17017.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>E2_pretraining_wikipedia_bertsmall_favor_nb0.25</td>\n",
       "      <td>7271.608264</td>\n",
       "      <td>727.160826</td>\n",
       "      <td>2.694388</td>\n",
       "      <td>18842.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>E2_pretraining_wikipedia_bertsmall_lsh_h4_c128</td>\n",
       "      <td>17036.511550</td>\n",
       "      <td>1703.651155</td>\n",
       "      <td>2.248216</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>E2_pretraining_wikipedia_bertsmall_favor_nb0.5</td>\n",
       "      <td>9173.221722</td>\n",
       "      <td>917.322172</td>\n",
       "      <td>2.666425</td>\n",
       "      <td>22493.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>E2_pretraining_wikipedia_bertsmall_lsh_h4_c64</td>\n",
       "      <td>10765.664537</td>\n",
       "      <td>1076.566454</td>\n",
       "      <td>2.285827</td>\n",
       "      <td>24964.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>E2_pretraining_wikipedia_bertsmall_favor_nb1</td>\n",
       "      <td>13037.930545</td>\n",
       "      <td>1303.793054</td>\n",
       "      <td>2.578758</td>\n",
       "      <td>29795.487793</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>favor_nb1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mzlsnnzy</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.125</td>\n",
       "      <td>1365.717010</td>\n",
       "      <td>91.047801</td>\n",
       "      <td>3.199574</td>\n",
       "      <td>17015.669434</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1qxi1tf7</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.25</td>\n",
       "      <td>1540.193664</td>\n",
       "      <td>102.679578</td>\n",
       "      <td>2.865847</td>\n",
       "      <td>18840.782715</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1crsv1ae</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.5</td>\n",
       "      <td>1944.468278</td>\n",
       "      <td>129.631219</td>\n",
       "      <td>2.834124</td>\n",
       "      <td>22489.657715</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>E2_pretraining_wikipedia_bertsmall_lsh_h2_c64</td>\n",
       "      <td>7647.912501</td>\n",
       "      <td>764.791250</td>\n",
       "      <td>2.344709</td>\n",
       "      <td>19543.765137</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hu06m597</td>\n",
       "      <td>E2_pretraining_wikipedia_bertsmall_lsh_h2_c128</td>\n",
       "      <td>10888.843224</td>\n",
       "      <td>1088.884322</td>\n",
       "      <td>2.277830</td>\n",
       "      <td>21848.345215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jucxpr34</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb1</td>\n",
       "      <td>3032.204451</td>\n",
       "      <td>202.146963</td>\n",
       "      <td>2.761649</td>\n",
       "      <td>29793.169434</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a16qafd4</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h2_c64</td>\n",
       "      <td>1774.054965</td>\n",
       "      <td>118.270331</td>\n",
       "      <td>2.602880</td>\n",
       "      <td>19543.776855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o031jp0p</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h2_c128</td>\n",
       "      <td>2053.627131</td>\n",
       "      <td>136.908475</td>\n",
       "      <td>2.502445</td>\n",
       "      <td>21848.151855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9fnn58aq</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h4_c128</td>\n",
       "      <td>3070.909744</td>\n",
       "      <td>204.727316</td>\n",
       "      <td>2.478065</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2fpocsb8</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h4_c64</td>\n",
       "      <td>2522.166988</td>\n",
       "      <td>168.144466</td>\n",
       "      <td>2.534777</td>\n",
       "      <td>24966.026855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jibomvsj</td>\n",
       "      <td>E1_pretraining_arxiv_bertsmall_mha</td>\n",
       "      <td>8411.599251</td>\n",
       "      <td>4205.799625</td>\n",
       "      <td>1.672165</td>\n",
       "      <td>14793.368652</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>mha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tzihl99m</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>5584.716981</td>\n",
       "      <td>930.786164</td>\n",
       "      <td>4.505261</td>\n",
       "      <td>16990.050293</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vi5hkih4</td>\n",
       "      <td>E1_pretraining_hyperpartisan_bertsmall_mha</td>\n",
       "      <td>5994.331384</td>\n",
       "      <td>999.055231</td>\n",
       "      <td>1.887050</td>\n",
       "      <td>14786.095215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>mha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>71eagzdx</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>6335.581790</td>\n",
       "      <td>1055.930298</td>\n",
       "      <td>3.913812</td>\n",
       "      <td>18786.679199</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pu4bht10</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>8143.261960</td>\n",
       "      <td>1357.210327</td>\n",
       "      <td>3.665452</td>\n",
       "      <td>22377.155762</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vyeowm82</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_nb1</td>\n",
       "      <td>11534.012031</td>\n",
       "      <td>1922.335338</td>\n",
       "      <td>3.267827</td>\n",
       "      <td>29563.599121</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2g7hhgqh</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64</td>\n",
       "      <td>6697.284041</td>\n",
       "      <td>1116.214007</td>\n",
       "      <td>2.557153</td>\n",
       "      <td>19545.526855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>i1bylyot</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h2_...</td>\n",
       "      <td>7766.640232</td>\n",
       "      <td>1294.440039</td>\n",
       "      <td>2.439533</td>\n",
       "      <td>21849.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>m6psyu6o</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64</td>\n",
       "      <td>9655.740008</td>\n",
       "      <td>1609.290001</td>\n",
       "      <td>2.351454</td>\n",
       "      <td>24966.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>o9riawb7</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h4_...</td>\n",
       "      <td>11807.380705</td>\n",
       "      <td>1967.896784</td>\n",
       "      <td>2.263041</td>\n",
       "      <td>29574.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>kb5ewjpp</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h2_c64</td>\n",
       "      <td>4578.586053</td>\n",
       "      <td>2289.293026</td>\n",
       "      <td>2.299913</td>\n",
       "      <td>19551.526855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4h910kzj</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h2_c128</td>\n",
       "      <td>5230.441202</td>\n",
       "      <td>2615.220601</td>\n",
       "      <td>2.223732</td>\n",
       "      <td>21855.970215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9hb1xk49</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h4_c64</td>\n",
       "      <td>6600.884504</td>\n",
       "      <td>3300.442252</td>\n",
       "      <td>2.072415</td>\n",
       "      <td>24974.400879</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7olzfob3</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h4_c128</td>\n",
       "      <td>7900.804413</td>\n",
       "      <td>3950.402206</td>\n",
       "      <td>2.023962</td>\n",
       "      <td>29580.395020</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>t67wmtc1</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.125</td>\n",
       "      <td>4140.682855</td>\n",
       "      <td>2070.341427</td>\n",
       "      <td>5.032768</td>\n",
       "      <td>16991.537598</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>aplguq3f</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.25</td>\n",
       "      <td>4649.542211</td>\n",
       "      <td>2324.771106</td>\n",
       "      <td>4.796945</td>\n",
       "      <td>18783.696777</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>k9km9nuj</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.5</td>\n",
       "      <td>5686.842252</td>\n",
       "      <td>2843.421126</td>\n",
       "      <td>4.631720</td>\n",
       "      <td>22371.815918</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>p0sm9w3v</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb1</td>\n",
       "      <td>7790.235805</td>\n",
       "      <td>3895.117902</td>\n",
       "      <td>4.316468</td>\n",
       "      <td>29547.308105</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      run_id                                           run_name    duration_s  \\\n",
       "0   hd228t3k             E1_pretraining_wikipedia_bertsmall_mha   4728.551459   \n",
       "1   ghydwtq8                  E1_pretraining_imdb_bertsmall_mha   1113.179355   \n",
       "2   z28fmtgo   E2_pretraining_wikipedia_bertsmall_favor_nb0.125   6593.419117   \n",
       "3   0itox8yh    E2_pretraining_wikipedia_bertsmall_favor_nb0.25   7271.608264   \n",
       "4   0e2zxgor     E2_pretraining_wikipedia_bertsmall_lsh_h4_c128  17036.511550   \n",
       "5   yjcapkj3     E2_pretraining_wikipedia_bertsmall_favor_nb0.5   9173.221722   \n",
       "6   wi223o9o      E2_pretraining_wikipedia_bertsmall_lsh_h4_c64  10765.664537   \n",
       "7   noug0x9s       E2_pretraining_wikipedia_bertsmall_favor_nb1  13037.930545   \n",
       "8   mzlsnnzy        E2_pretraining_imdb_bertsmall_favor_nb0.125   1365.717010   \n",
       "9   1qxi1tf7         E2_pretraining_imdb_bertsmall_favor_nb0.25   1540.193664   \n",
       "10  1crsv1ae          E2_pretraining_imdb_bertsmall_favor_nb0.5   1944.468278   \n",
       "11  eudij3yf      E2_pretraining_wikipedia_bertsmall_lsh_h2_c64   7647.912501   \n",
       "12  hu06m597     E2_pretraining_wikipedia_bertsmall_lsh_h2_c128  10888.843224   \n",
       "13  jucxpr34            E2_pretraining_imdb_bertsmall_favor_nb1   3032.204451   \n",
       "14  a16qafd4           E2_pretraining_imdb_bertsmall_lsh_h2_c64   1774.054965   \n",
       "15  o031jp0p          E2_pretraining_imdb_bertsmall_lsh_h2_c128   2053.627131   \n",
       "16  9fnn58aq          E2_pretraining_imdb_bertsmall_lsh_h4_c128   3070.909744   \n",
       "17  2fpocsb8           E2_pretraining_imdb_bertsmall_lsh_h4_c64   2522.166988   \n",
       "18  jibomvsj                 E1_pretraining_arxiv_bertsmall_mha   8411.599251   \n",
       "19  tzihl99m  E2_pretraining_hyperpartisan_bertsmall_favor_n...   5584.716981   \n",
       "20  vi5hkih4         E1_pretraining_hyperpartisan_bertsmall_mha   5994.331384   \n",
       "21  71eagzdx  E2_pretraining_hyperpartisan_bertsmall_favor_n...   6335.581790   \n",
       "22  pu4bht10  E2_pretraining_hyperpartisan_bertsmall_favor_n...   8143.261960   \n",
       "23  vyeowm82   E2_pretraining_hyperpartisan_bertsmall_favor_nb1  11534.012031   \n",
       "24  2g7hhgqh  E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64   6697.284041   \n",
       "25  i1bylyot  E2_pretraining_hyperpartisan_bertsmall_lsh_h2_...   7766.640232   \n",
       "26  m6psyu6o  E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64   9655.740008   \n",
       "27  o9riawb7  E2_pretraining_hyperpartisan_bertsmall_lsh_h4_...  11807.380705   \n",
       "28  kb5ewjpp          E2_pretraining_arxiv_bertsmall_lsh_h2_c64   4578.586053   \n",
       "29  4h910kzj         E2_pretraining_arxiv_bertsmall_lsh_h2_c128   5230.441202   \n",
       "30  9hb1xk49          E2_pretraining_arxiv_bertsmall_lsh_h4_c64   6600.884504   \n",
       "31  7olzfob3         E2_pretraining_arxiv_bertsmall_lsh_h4_c128   7900.804413   \n",
       "32  t67wmtc1       E2_pretraining_arxiv_bertsmall_favor_nb0.125   4140.682855   \n",
       "33  aplguq3f        E2_pretraining_arxiv_bertsmall_favor_nb0.25   4649.542211   \n",
       "34  k9km9nuj         E2_pretraining_arxiv_bertsmall_favor_nb0.5   5686.842252   \n",
       "35  p0sm9w3v           E2_pretraining_arxiv_bertsmall_favor_nb1   7790.235805   \n",
       "\n",
       "    avg_epoch_time_s  train/avg_epoch_loss  train/gpu_mem_peak_mb     stage  \\\n",
       "0         472.855146              2.156817           14785.845215  pretrain   \n",
       "1          74.211957              2.387619           14784.345215      tapt   \n",
       "2         659.341912              3.069484           17017.976074  pretrain   \n",
       "3         727.160826              2.694388           18842.976074  pretrain   \n",
       "4        1703.651155              2.248216           29574.151855  pretrain   \n",
       "5         917.322172              2.666425           22493.976074  pretrain   \n",
       "6        1076.566454              2.285827           24964.151855  pretrain   \n",
       "7        1303.793054              2.578758           29795.487793  pretrain   \n",
       "8          91.047801              3.199574           17015.669434      tapt   \n",
       "9         102.679578              2.865847           18840.782715      tapt   \n",
       "10        129.631219              2.834124           22489.657715      tapt   \n",
       "11        764.791250              2.344709           19543.765137  pretrain   \n",
       "12       1088.884322              2.277830           21848.345215  pretrain   \n",
       "13        202.146963              2.761649           29793.169434      tapt   \n",
       "14        118.270331              2.602880           19543.776855      tapt   \n",
       "15        136.908475              2.502445           21848.151855      tapt   \n",
       "16        204.727316              2.478065           29574.151855      tapt   \n",
       "17        168.144466              2.534777           24966.026855      tapt   \n",
       "18       4205.799625              1.672165           14793.368652      tapt   \n",
       "19        930.786164              4.505261           16990.050293      tapt   \n",
       "20        999.055231              1.887050           14786.095215      tapt   \n",
       "21       1055.930298              3.913812           18786.679199      tapt   \n",
       "22       1357.210327              3.665452           22377.155762      tapt   \n",
       "23       1922.335338              3.267827           29563.599121      tapt   \n",
       "24       1116.214007              2.557153           19545.526855      tapt   \n",
       "25       1294.440039              2.439533           21849.901855      tapt   \n",
       "26       1609.290001              2.351454           24966.901855      tapt   \n",
       "27       1967.896784              2.263041           29574.901855      tapt   \n",
       "28       2289.293026              2.299913           19551.526855      tapt   \n",
       "29       2615.220601              2.223732           21855.970215      tapt   \n",
       "30       3300.442252              2.072415           24974.400879      tapt   \n",
       "31       3950.402206              2.023962           29580.395020      tapt   \n",
       "32       2070.341427              5.032768           16991.537598      tapt   \n",
       "33       2324.771106              4.796945           18783.696777      tapt   \n",
       "34       2843.421126              4.631720           22371.815918      tapt   \n",
       "35       3895.117902              4.316468           29547.308105      tapt   \n",
       "\n",
       "          dataset           arch  \n",
       "0       wikipedia            mha  \n",
       "1            imdb            mha  \n",
       "2       wikipedia  favor_nb0.125  \n",
       "3       wikipedia   favor_nb0.25  \n",
       "4       wikipedia    lsh_h4_c128  \n",
       "5       wikipedia    favor_nb0.5  \n",
       "6       wikipedia     lsh_h4_c64  \n",
       "7       wikipedia      favor_nb1  \n",
       "8            imdb  favor_nb0.125  \n",
       "9            imdb   favor_nb0.25  \n",
       "10           imdb    favor_nb0.5  \n",
       "11      wikipedia     lsh_h2_c64  \n",
       "12      wikipedia    lsh_h2_c128  \n",
       "13           imdb      favor_nb1  \n",
       "14           imdb     lsh_h2_c64  \n",
       "15           imdb    lsh_h2_c128  \n",
       "16           imdb    lsh_h4_c128  \n",
       "17           imdb     lsh_h4_c64  \n",
       "18          arxiv            mha  \n",
       "19  hyperpartisan  favor_nb0.125  \n",
       "20  hyperpartisan            mha  \n",
       "21  hyperpartisan   favor_nb0.25  \n",
       "22  hyperpartisan    favor_nb0.5  \n",
       "23  hyperpartisan      favor_nb1  \n",
       "24  hyperpartisan     lsh_h2_c64  \n",
       "25  hyperpartisan    lsh_h2_c128  \n",
       "26  hyperpartisan     lsh_h4_c64  \n",
       "27  hyperpartisan    lsh_h4_c128  \n",
       "28          arxiv     lsh_h2_c64  \n",
       "29          arxiv    lsh_h2_c128  \n",
       "30          arxiv     lsh_h4_c64  \n",
       "31          arxiv    lsh_h4_c128  \n",
       "32          arxiv  favor_nb0.125  \n",
       "33          arxiv   favor_nb0.25  \n",
       "34          arxiv    favor_nb0.5  \n",
       "35          arxiv      favor_nb1  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "332ed930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean l2jq2x7g\n",
      "E2_finetuning_imdb_bertsmall_favor_nb0.125 d659uj2j\n",
      "E2_finetuning_imdb_bertsmall_favor_nb0.25 if2ozrk2\n",
      "E2_finetuning_imdb_bertsmall_favor_nb0.5 hhoyoxfp\n",
      "E2_finetuning_imdb_bertsmall_favor_nb1 u44pgkly\n",
      "E2_finetuning_imdb_bertsmall_lsh_h2_c128 tq5q132s\n",
      "E2_finetuning_imdb_bertsmall_lsh_h2_c64 j2x1q39c\n",
      "E2_finetuning_imdb_bertsmall_lsh_h4_c128 s1c2rnaf\n",
      "E2_finetuning_imdb_bertsmall_lsh_h4_c64 u262vax9\n",
      "E1_finetuning_hyperpartisan_bertsmall_mha_f1_d0.1_mean ulsu61q4\n",
      "E2_finetuning_hyperpartisan_bertsmall_favor_nb0.125 xhbujbys\n",
      "E2_finetuning_hyperpartisan_bertsmall_favor_nb0.25 3klrqv94\n",
      "E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5 3jtkh8iw\n",
      "E2_finetuning_hyperpartisan_bertsmall_favor_nb1 k2j8xkd0\n",
      "E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64 ys8qnftm\n",
      "E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128 yjqias1k\n",
      "E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64 16tul7l4\n",
      "E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean ied1ulsn\n",
      "E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128 gyaw279z\n",
      "E2_finetuning_arxiv_bertsmall_favor_nb0.125 fs5plm2w\n",
      "E2_finetuning_arxiv_bertsmall_lsh_h2_c128 xf2puymg\n",
      "E2_finetuning_arxiv_bertsmall_favor_nb0.25 tug4ff1q\n",
      "E2_finetuning_arxiv_bertsmall_lsh_h2_c64 ebmfez6l\n",
      "E2_finetuning_arxiv_bertsmall_favor_nb0.5 3ksmzwr0\n",
      "E2_finetuning_arxiv_bertsmall_lsh_h4_c128 wqah0rrq\n",
      "E2_finetuning_arxiv_bertsmall_favor_nb1 0zr9387l\n",
      "E2_finetuning_arxiv_bertsmall_lsh_h4_c64 v0prpm6r\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "names = []\n",
    "\n",
    "for run_id, name in main_results_runs.items():\n",
    "    if  'pretraining' not in name:\n",
    "        ids.append(run_id)\n",
    "        names.append(name)\n",
    "        print(name, run_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "adb0fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune = parse_run_name(build_metric_runtime_df(ids, names, [\n",
    "    {'metric_name': 'f1_macro', 'metric_scope': 'test', 'agg': 'at_index',\"select_at\": {\n",
    "            \"metric_name\": \"f1_macro\",\n",
    "            \"metric_scope\": \"eval\",\n",
    "            \"agg\": \"idxmax\",\n",
    "        } },\n",
    "    {'metric_name': 'gpu_mem_peak_mb', 'metric_scope': 'train', 'agg': 'max'}\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "90f58f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>run_name</th>\n",
       "      <th>duration_s</th>\n",
       "      <th>avg_epoch_time_s</th>\n",
       "      <th>test/f1_macro</th>\n",
       "      <th>train/gpu_mem_peak_mb</th>\n",
       "      <th>stage</th>\n",
       "      <th>dataset</th>\n",
       "      <th>arch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l2jq2x7g</td>\n",
       "      <td>E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean</td>\n",
       "      <td>309.168199</td>\n",
       "      <td>38.646025</td>\n",
       "      <td>0.928599</td>\n",
       "      <td>3399.745117</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>mha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d659uj2j</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.125</td>\n",
       "      <td>452.629664</td>\n",
       "      <td>56.578708</td>\n",
       "      <td>0.911595</td>\n",
       "      <td>5633.739258</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if2ozrk2</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.25</td>\n",
       "      <td>535.890604</td>\n",
       "      <td>66.986325</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>7456.000977</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hhoyoxfp</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.5</td>\n",
       "      <td>728.522018</td>\n",
       "      <td>91.065252</td>\n",
       "      <td>0.916397</td>\n",
       "      <td>11106.000977</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u44pgkly</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb1</td>\n",
       "      <td>1232.003859</td>\n",
       "      <td>154.000482</td>\n",
       "      <td>0.917398</td>\n",
       "      <td>18846.411133</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tq5q132s</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h2_c128</td>\n",
       "      <td>790.832176</td>\n",
       "      <td>98.854022</td>\n",
       "      <td>0.926597</td>\n",
       "      <td>12042.296387</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>j2x1q39c</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h2_c64</td>\n",
       "      <td>646.792933</td>\n",
       "      <td>80.849117</td>\n",
       "      <td>0.927800</td>\n",
       "      <td>9258.346680</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s1c2rnaf</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h4_c128</td>\n",
       "      <td>1298.198816</td>\n",
       "      <td>162.274852</td>\n",
       "      <td>0.929800</td>\n",
       "      <td>21841.253418</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>u262vax9</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h4_c64</td>\n",
       "      <td>1020.584711</td>\n",
       "      <td>127.573089</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>16270.889648</td>\n",
       "      <td>finetune</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ulsu61q4</td>\n",
       "      <td>E1_finetuning_hyperpartisan_bertsmall_mha_f1_d...</td>\n",
       "      <td>4338.320566</td>\n",
       "      <td>619.760081</td>\n",
       "      <td>0.645501</td>\n",
       "      <td>3518.334961</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>mha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xhbujbys</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb...</td>\n",
       "      <td>3936.268652</td>\n",
       "      <td>562.324093</td>\n",
       "      <td>0.534071</td>\n",
       "      <td>5727.854492</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3klrqv94</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb...</td>\n",
       "      <td>4820.846868</td>\n",
       "      <td>688.692410</td>\n",
       "      <td>0.569878</td>\n",
       "      <td>7519.026367</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3jtkh8iw</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5</td>\n",
       "      <td>6889.461540</td>\n",
       "      <td>984.208791</td>\n",
       "      <td>0.593013</td>\n",
       "      <td>11114.338867</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>k2j8xkd0</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb1</td>\n",
       "      <td>10759.819213</td>\n",
       "      <td>1537.117030</td>\n",
       "      <td>0.550229</td>\n",
       "      <td>18797.067871</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ys8qnftm</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64</td>\n",
       "      <td>5297.502927</td>\n",
       "      <td>756.786132</td>\n",
       "      <td>0.653656</td>\n",
       "      <td>9382.263184</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>yjqias1k</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128</td>\n",
       "      <td>6567.803232</td>\n",
       "      <td>938.257605</td>\n",
       "      <td>0.612541</td>\n",
       "      <td>12168.060547</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16tul7l4</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64</td>\n",
       "      <td>8770.437606</td>\n",
       "      <td>1252.919658</td>\n",
       "      <td>0.624688</td>\n",
       "      <td>16397.189453</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ied1ulsn</td>\n",
       "      <td>E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean</td>\n",
       "      <td>13235.081686</td>\n",
       "      <td>3308.770421</td>\n",
       "      <td>0.883798</td>\n",
       "      <td>3524.024414</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>mha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gyaw279z</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128</td>\n",
       "      <td>11314.827481</td>\n",
       "      <td>1616.403926</td>\n",
       "      <td>0.556799</td>\n",
       "      <td>21965.084473</td>\n",
       "      <td>finetune</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fs5plm2w</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.125</td>\n",
       "      <td>5314.207820</td>\n",
       "      <td>1328.551955</td>\n",
       "      <td>0.860208</td>\n",
       "      <td>5725.700195</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xf2puymg</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h2_c128</td>\n",
       "      <td>7525.100944</td>\n",
       "      <td>1881.275236</td>\n",
       "      <td>0.872998</td>\n",
       "      <td>12174.626465</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tug4ff1q</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.25</td>\n",
       "      <td>6291.965838</td>\n",
       "      <td>1572.991459</td>\n",
       "      <td>0.865219</td>\n",
       "      <td>7517.092773</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ebmfez6l</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h2_c64</td>\n",
       "      <td>6224.091129</td>\n",
       "      <td>1556.022782</td>\n",
       "      <td>0.868965</td>\n",
       "      <td>9389.609375</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3ksmzwr0</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.5</td>\n",
       "      <td>8319.524132</td>\n",
       "      <td>2079.881033</td>\n",
       "      <td>0.865297</td>\n",
       "      <td>11104.350586</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>wqah0rrq</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h4_c128</td>\n",
       "      <td>12944.441540</td>\n",
       "      <td>3236.110385</td>\n",
       "      <td>0.874043</td>\n",
       "      <td>21973.819824</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0zr9387l</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb1</td>\n",
       "      <td>12386.017915</td>\n",
       "      <td>3096.504479</td>\n",
       "      <td>0.855688</td>\n",
       "      <td>18783.645020</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>v0prpm6r</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h4_c64</td>\n",
       "      <td>10335.925940</td>\n",
       "      <td>2583.981485</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>16404.604980</td>\n",
       "      <td>finetune</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      run_id                                           run_name    duration_s  \\\n",
       "0   l2jq2x7g      E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean    309.168199   \n",
       "1   d659uj2j         E2_finetuning_imdb_bertsmall_favor_nb0.125    452.629664   \n",
       "2   if2ozrk2          E2_finetuning_imdb_bertsmall_favor_nb0.25    535.890604   \n",
       "3   hhoyoxfp           E2_finetuning_imdb_bertsmall_favor_nb0.5    728.522018   \n",
       "4   u44pgkly             E2_finetuning_imdb_bertsmall_favor_nb1   1232.003859   \n",
       "5   tq5q132s           E2_finetuning_imdb_bertsmall_lsh_h2_c128    790.832176   \n",
       "6   j2x1q39c            E2_finetuning_imdb_bertsmall_lsh_h2_c64    646.792933   \n",
       "7   s1c2rnaf           E2_finetuning_imdb_bertsmall_lsh_h4_c128   1298.198816   \n",
       "8   u262vax9            E2_finetuning_imdb_bertsmall_lsh_h4_c64   1020.584711   \n",
       "9   ulsu61q4  E1_finetuning_hyperpartisan_bertsmall_mha_f1_d...   4338.320566   \n",
       "10  xhbujbys  E2_finetuning_hyperpartisan_bertsmall_favor_nb...   3936.268652   \n",
       "11  3klrqv94  E2_finetuning_hyperpartisan_bertsmall_favor_nb...   4820.846868   \n",
       "12  3jtkh8iw  E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5   6889.461540   \n",
       "13  k2j8xkd0    E2_finetuning_hyperpartisan_bertsmall_favor_nb1  10759.819213   \n",
       "14  ys8qnftm   E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64   5297.502927   \n",
       "15  yjqias1k  E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128   6567.803232   \n",
       "16  16tul7l4   E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64   8770.437606   \n",
       "17  ied1ulsn     E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean  13235.081686   \n",
       "18  gyaw279z  E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128  11314.827481   \n",
       "19  fs5plm2w        E2_finetuning_arxiv_bertsmall_favor_nb0.125   5314.207820   \n",
       "20  xf2puymg          E2_finetuning_arxiv_bertsmall_lsh_h2_c128   7525.100944   \n",
       "21  tug4ff1q         E2_finetuning_arxiv_bertsmall_favor_nb0.25   6291.965838   \n",
       "22  ebmfez6l           E2_finetuning_arxiv_bertsmall_lsh_h2_c64   6224.091129   \n",
       "23  3ksmzwr0          E2_finetuning_arxiv_bertsmall_favor_nb0.5   8319.524132   \n",
       "24  wqah0rrq          E2_finetuning_arxiv_bertsmall_lsh_h4_c128  12944.441540   \n",
       "25  0zr9387l            E2_finetuning_arxiv_bertsmall_favor_nb1  12386.017915   \n",
       "26  v0prpm6r           E2_finetuning_arxiv_bertsmall_lsh_h4_c64  10335.925940   \n",
       "\n",
       "    avg_epoch_time_s  test/f1_macro  train/gpu_mem_peak_mb     stage  \\\n",
       "0          38.646025       0.928599            3399.745117  finetune   \n",
       "1          56.578708       0.911595            5633.739258  finetune   \n",
       "2          66.986325       0.909774            7456.000977  finetune   \n",
       "3          91.065252       0.916397           11106.000977  finetune   \n",
       "4         154.000482       0.917398           18846.411133  finetune   \n",
       "5          98.854022       0.926597           12042.296387  finetune   \n",
       "6          80.849117       0.927800            9258.346680  finetune   \n",
       "7         162.274852       0.929800           21841.253418  finetune   \n",
       "8         127.573089       0.925000           16270.889648  finetune   \n",
       "9         619.760081       0.645501            3518.334961  finetune   \n",
       "10        562.324093       0.534071            5727.854492  finetune   \n",
       "11        688.692410       0.569878            7519.026367  finetune   \n",
       "12        984.208791       0.593013           11114.338867  finetune   \n",
       "13       1537.117030       0.550229           18797.067871  finetune   \n",
       "14        756.786132       0.653656            9382.263184  finetune   \n",
       "15        938.257605       0.612541           12168.060547  finetune   \n",
       "16       1252.919658       0.624688           16397.189453  finetune   \n",
       "17       3308.770421       0.883798            3524.024414  finetune   \n",
       "18       1616.403926       0.556799           21965.084473  finetune   \n",
       "19       1328.551955       0.860208            5725.700195  finetune   \n",
       "20       1881.275236       0.872998           12174.626465  finetune   \n",
       "21       1572.991459       0.865219            7517.092773  finetune   \n",
       "22       1556.022782       0.868965            9389.609375  finetune   \n",
       "23       2079.881033       0.865297           11104.350586  finetune   \n",
       "24       3236.110385       0.874043           21973.819824  finetune   \n",
       "25       3096.504479       0.855688           18783.645020  finetune   \n",
       "26       2583.981485       0.866779           16404.604980  finetune   \n",
       "\n",
       "          dataset           arch  \n",
       "0            imdb            mha  \n",
       "1            imdb  favor_nb0.125  \n",
       "2            imdb   favor_nb0.25  \n",
       "3            imdb    favor_nb0.5  \n",
       "4            imdb      favor_nb1  \n",
       "5            imdb    lsh_h2_c128  \n",
       "6            imdb     lsh_h2_c64  \n",
       "7            imdb    lsh_h4_c128  \n",
       "8            imdb     lsh_h4_c64  \n",
       "9   hyperpartisan            mha  \n",
       "10  hyperpartisan  favor_nb0.125  \n",
       "11  hyperpartisan   favor_nb0.25  \n",
       "12  hyperpartisan    favor_nb0.5  \n",
       "13  hyperpartisan      favor_nb1  \n",
       "14  hyperpartisan     lsh_h2_c64  \n",
       "15  hyperpartisan    lsh_h2_c128  \n",
       "16  hyperpartisan     lsh_h4_c64  \n",
       "17          arxiv            mha  \n",
       "18  hyperpartisan    lsh_h4_c128  \n",
       "19          arxiv  favor_nb0.125  \n",
       "20          arxiv    lsh_h2_c128  \n",
       "21          arxiv   favor_nb0.25  \n",
       "22          arxiv     lsh_h2_c64  \n",
       "23          arxiv    favor_nb0.5  \n",
       "24          arxiv    lsh_h4_c128  \n",
       "25          arxiv      favor_nb1  \n",
       "26          arxiv     lsh_h4_c64  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "87e48ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tapt = df_pretrain.loc[df_pretrain['stage'] == 'tapt']\n",
    "df_pretrain = df_pretrain.loc[df_pretrain['stage'] == 'pretrain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2c6a1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merge_stages(\n",
    "    df_pretrain,\n",
    "    df_tapt,\n",
    "    df_finetune\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fe230aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tapt/run_id</th>\n",
       "      <th>tapt/run_name</th>\n",
       "      <th>tapt/duration_s</th>\n",
       "      <th>tapt/avg_epoch_time_s</th>\n",
       "      <th>tapt/train/avg_epoch_loss</th>\n",
       "      <th>tapt/train/gpu_mem_peak_mb</th>\n",
       "      <th>tapt/stage</th>\n",
       "      <th>dataset</th>\n",
       "      <th>arch</th>\n",
       "      <th>pretrain/run_id</th>\n",
       "      <th>...</th>\n",
       "      <th>pretrain/train/avg_epoch_loss</th>\n",
       "      <th>pretrain/train/gpu_mem_peak_mb</th>\n",
       "      <th>pretrain/stage</th>\n",
       "      <th>finetune/run_id</th>\n",
       "      <th>finetune/run_name</th>\n",
       "      <th>finetune/duration_s</th>\n",
       "      <th>finetune/avg_epoch_time_s</th>\n",
       "      <th>finetune/test/f1_macro</th>\n",
       "      <th>finetune/train/gpu_mem_peak_mb</th>\n",
       "      <th>finetune/stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t67wmtc1</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.125</td>\n",
       "      <td>4140.682855</td>\n",
       "      <td>2070.341427</td>\n",
       "      <td>5.032768</td>\n",
       "      <td>16991.537598</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>3.069484</td>\n",
       "      <td>17017.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>fs5plm2w</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.125</td>\n",
       "      <td>5314.207820</td>\n",
       "      <td>1328.551955</td>\n",
       "      <td>0.860208</td>\n",
       "      <td>5725.700195</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aplguq3f</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.25</td>\n",
       "      <td>4649.542211</td>\n",
       "      <td>2324.771106</td>\n",
       "      <td>4.796945</td>\n",
       "      <td>18783.696777</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694388</td>\n",
       "      <td>18842.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>tug4ff1q</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.25</td>\n",
       "      <td>6291.965838</td>\n",
       "      <td>1572.991459</td>\n",
       "      <td>0.865219</td>\n",
       "      <td>7517.092773</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k9km9nuj</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.5</td>\n",
       "      <td>5686.842252</td>\n",
       "      <td>2843.421126</td>\n",
       "      <td>4.631720</td>\n",
       "      <td>22371.815918</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.666425</td>\n",
       "      <td>22493.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>3ksmzwr0</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.5</td>\n",
       "      <td>8319.524132</td>\n",
       "      <td>2079.881033</td>\n",
       "      <td>0.865297</td>\n",
       "      <td>11104.350586</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p0sm9w3v</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb1</td>\n",
       "      <td>7790.235805</td>\n",
       "      <td>3895.117902</td>\n",
       "      <td>4.316468</td>\n",
       "      <td>29547.308105</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>2.578758</td>\n",
       "      <td>29795.487793</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>0zr9387l</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb1</td>\n",
       "      <td>12386.017915</td>\n",
       "      <td>3096.504479</td>\n",
       "      <td>0.855688</td>\n",
       "      <td>18783.645020</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4h910kzj</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h2_c128</td>\n",
       "      <td>5230.441202</td>\n",
       "      <td>2615.220601</td>\n",
       "      <td>2.223732</td>\n",
       "      <td>21855.970215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>2.277830</td>\n",
       "      <td>21848.345215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>xf2puymg</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h2_c128</td>\n",
       "      <td>7525.100944</td>\n",
       "      <td>1881.275236</td>\n",
       "      <td>0.872998</td>\n",
       "      <td>12174.626465</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kb5ewjpp</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h2_c64</td>\n",
       "      <td>4578.586053</td>\n",
       "      <td>2289.293026</td>\n",
       "      <td>2.299913</td>\n",
       "      <td>19551.526855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344709</td>\n",
       "      <td>19543.765137</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ebmfez6l</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h2_c64</td>\n",
       "      <td>6224.091129</td>\n",
       "      <td>1556.022782</td>\n",
       "      <td>0.868965</td>\n",
       "      <td>9389.609375</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7olzfob3</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h4_c128</td>\n",
       "      <td>7900.804413</td>\n",
       "      <td>3950.402206</td>\n",
       "      <td>2.023962</td>\n",
       "      <td>29580.395020</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>2.248216</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wqah0rrq</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h4_c128</td>\n",
       "      <td>12944.441540</td>\n",
       "      <td>3236.110385</td>\n",
       "      <td>0.874043</td>\n",
       "      <td>21973.819824</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9hb1xk49</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h4_c64</td>\n",
       "      <td>6600.884504</td>\n",
       "      <td>3300.442252</td>\n",
       "      <td>2.072415</td>\n",
       "      <td>24974.400879</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>2.285827</td>\n",
       "      <td>24964.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>v0prpm6r</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h4_c64</td>\n",
       "      <td>10335.925940</td>\n",
       "      <td>2583.981485</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>16404.604980</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jibomvsj</td>\n",
       "      <td>E1_pretraining_arxiv_bertsmall_mha</td>\n",
       "      <td>8411.599251</td>\n",
       "      <td>4205.799625</td>\n",
       "      <td>1.672165</td>\n",
       "      <td>14793.368652</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156817</td>\n",
       "      <td>14785.845215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ied1ulsn</td>\n",
       "      <td>E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean</td>\n",
       "      <td>13235.081686</td>\n",
       "      <td>3308.770421</td>\n",
       "      <td>0.883798</td>\n",
       "      <td>3524.024414</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tzihl99m</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>5584.716981</td>\n",
       "      <td>930.786164</td>\n",
       "      <td>4.505261</td>\n",
       "      <td>16990.050293</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>3.069484</td>\n",
       "      <td>17017.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>xhbujbys</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb...</td>\n",
       "      <td>3936.268652</td>\n",
       "      <td>562.324093</td>\n",
       "      <td>0.534071</td>\n",
       "      <td>5727.854492</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>71eagzdx</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>6335.581790</td>\n",
       "      <td>1055.930298</td>\n",
       "      <td>3.913812</td>\n",
       "      <td>18786.679199</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694388</td>\n",
       "      <td>18842.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>3klrqv94</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb...</td>\n",
       "      <td>4820.846868</td>\n",
       "      <td>688.692410</td>\n",
       "      <td>0.569878</td>\n",
       "      <td>7519.026367</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pu4bht10</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>8143.261960</td>\n",
       "      <td>1357.210327</td>\n",
       "      <td>3.665452</td>\n",
       "      <td>22377.155762</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.666425</td>\n",
       "      <td>22493.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>3jtkh8iw</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5</td>\n",
       "      <td>6889.461540</td>\n",
       "      <td>984.208791</td>\n",
       "      <td>0.593013</td>\n",
       "      <td>11114.338867</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vyeowm82</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_nb1</td>\n",
       "      <td>11534.012031</td>\n",
       "      <td>1922.335338</td>\n",
       "      <td>3.267827</td>\n",
       "      <td>29563.599121</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>2.578758</td>\n",
       "      <td>29795.487793</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>k2j8xkd0</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb1</td>\n",
       "      <td>10759.819213</td>\n",
       "      <td>1537.117030</td>\n",
       "      <td>0.550229</td>\n",
       "      <td>18797.067871</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i1bylyot</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h2_...</td>\n",
       "      <td>7766.640232</td>\n",
       "      <td>1294.440039</td>\n",
       "      <td>2.439533</td>\n",
       "      <td>21849.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>2.277830</td>\n",
       "      <td>21848.345215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>yjqias1k</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128</td>\n",
       "      <td>6567.803232</td>\n",
       "      <td>938.257605</td>\n",
       "      <td>0.612541</td>\n",
       "      <td>12168.060547</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2g7hhgqh</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64</td>\n",
       "      <td>6697.284041</td>\n",
       "      <td>1116.214007</td>\n",
       "      <td>2.557153</td>\n",
       "      <td>19545.526855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344709</td>\n",
       "      <td>19543.765137</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ys8qnftm</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64</td>\n",
       "      <td>5297.502927</td>\n",
       "      <td>756.786132</td>\n",
       "      <td>0.653656</td>\n",
       "      <td>9382.263184</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o9riawb7</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h4_...</td>\n",
       "      <td>11807.380705</td>\n",
       "      <td>1967.896784</td>\n",
       "      <td>2.263041</td>\n",
       "      <td>29574.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>2.248216</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>gyaw279z</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128</td>\n",
       "      <td>11314.827481</td>\n",
       "      <td>1616.403926</td>\n",
       "      <td>0.556799</td>\n",
       "      <td>21965.084473</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>m6psyu6o</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64</td>\n",
       "      <td>9655.740008</td>\n",
       "      <td>1609.290001</td>\n",
       "      <td>2.351454</td>\n",
       "      <td>24966.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>2.285827</td>\n",
       "      <td>24964.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>16tul7l4</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64</td>\n",
       "      <td>8770.437606</td>\n",
       "      <td>1252.919658</td>\n",
       "      <td>0.624688</td>\n",
       "      <td>16397.189453</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vi5hkih4</td>\n",
       "      <td>E1_pretraining_hyperpartisan_bertsmall_mha</td>\n",
       "      <td>5994.331384</td>\n",
       "      <td>999.055231</td>\n",
       "      <td>1.887050</td>\n",
       "      <td>14786.095215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156817</td>\n",
       "      <td>14785.845215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ulsu61q4</td>\n",
       "      <td>E1_finetuning_hyperpartisan_bertsmall_mha_f1_d...</td>\n",
       "      <td>4338.320566</td>\n",
       "      <td>619.760081</td>\n",
       "      <td>0.645501</td>\n",
       "      <td>3518.334961</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mzlsnnzy</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.125</td>\n",
       "      <td>1365.717010</td>\n",
       "      <td>91.047801</td>\n",
       "      <td>3.199574</td>\n",
       "      <td>17015.669434</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>3.069484</td>\n",
       "      <td>17017.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>d659uj2j</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.125</td>\n",
       "      <td>452.629664</td>\n",
       "      <td>56.578708</td>\n",
       "      <td>0.911595</td>\n",
       "      <td>5633.739258</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1qxi1tf7</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.25</td>\n",
       "      <td>1540.193664</td>\n",
       "      <td>102.679578</td>\n",
       "      <td>2.865847</td>\n",
       "      <td>18840.782715</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694388</td>\n",
       "      <td>18842.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>if2ozrk2</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.25</td>\n",
       "      <td>535.890604</td>\n",
       "      <td>66.986325</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>7456.000977</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1crsv1ae</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.5</td>\n",
       "      <td>1944.468278</td>\n",
       "      <td>129.631219</td>\n",
       "      <td>2.834124</td>\n",
       "      <td>22489.657715</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.666425</td>\n",
       "      <td>22493.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>hhoyoxfp</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.5</td>\n",
       "      <td>728.522018</td>\n",
       "      <td>91.065252</td>\n",
       "      <td>0.916397</td>\n",
       "      <td>11106.000977</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>jucxpr34</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb1</td>\n",
       "      <td>3032.204451</td>\n",
       "      <td>202.146963</td>\n",
       "      <td>2.761649</td>\n",
       "      <td>29793.169434</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>2.578758</td>\n",
       "      <td>29795.487793</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>u44pgkly</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb1</td>\n",
       "      <td>1232.003859</td>\n",
       "      <td>154.000482</td>\n",
       "      <td>0.917398</td>\n",
       "      <td>18846.411133</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>o031jp0p</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h2_c128</td>\n",
       "      <td>2053.627131</td>\n",
       "      <td>136.908475</td>\n",
       "      <td>2.502445</td>\n",
       "      <td>21848.151855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>2.277830</td>\n",
       "      <td>21848.345215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>tq5q132s</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h2_c128</td>\n",
       "      <td>790.832176</td>\n",
       "      <td>98.854022</td>\n",
       "      <td>0.926597</td>\n",
       "      <td>12042.296387</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a16qafd4</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h2_c64</td>\n",
       "      <td>1774.054965</td>\n",
       "      <td>118.270331</td>\n",
       "      <td>2.602880</td>\n",
       "      <td>19543.776855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344709</td>\n",
       "      <td>19543.765137</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>j2x1q39c</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h2_c64</td>\n",
       "      <td>646.792933</td>\n",
       "      <td>80.849117</td>\n",
       "      <td>0.927800</td>\n",
       "      <td>9258.346680</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9fnn58aq</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h4_c128</td>\n",
       "      <td>3070.909744</td>\n",
       "      <td>204.727316</td>\n",
       "      <td>2.478065</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>2.248216</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>s1c2rnaf</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h4_c128</td>\n",
       "      <td>1298.198816</td>\n",
       "      <td>162.274852</td>\n",
       "      <td>0.929800</td>\n",
       "      <td>21841.253418</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2fpocsb8</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h4_c64</td>\n",
       "      <td>2522.166988</td>\n",
       "      <td>168.144466</td>\n",
       "      <td>2.534777</td>\n",
       "      <td>24966.026855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>2.285827</td>\n",
       "      <td>24964.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>u262vax9</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h4_c64</td>\n",
       "      <td>1020.584711</td>\n",
       "      <td>127.573089</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>16270.889648</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ghydwtq8</td>\n",
       "      <td>E1_pretraining_imdb_bertsmall_mha</td>\n",
       "      <td>1113.179355</td>\n",
       "      <td>74.211957</td>\n",
       "      <td>2.387619</td>\n",
       "      <td>14784.345215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156817</td>\n",
       "      <td>14785.845215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>l2jq2x7g</td>\n",
       "      <td>E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean</td>\n",
       "      <td>309.168199</td>\n",
       "      <td>38.646025</td>\n",
       "      <td>0.928599</td>\n",
       "      <td>3399.745117</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tapt/run_id                                      tapt/run_name  \\\n",
       "0     t67wmtc1       E2_pretraining_arxiv_bertsmall_favor_nb0.125   \n",
       "1     aplguq3f        E2_pretraining_arxiv_bertsmall_favor_nb0.25   \n",
       "2     k9km9nuj         E2_pretraining_arxiv_bertsmall_favor_nb0.5   \n",
       "3     p0sm9w3v           E2_pretraining_arxiv_bertsmall_favor_nb1   \n",
       "4     4h910kzj         E2_pretraining_arxiv_bertsmall_lsh_h2_c128   \n",
       "5     kb5ewjpp          E2_pretraining_arxiv_bertsmall_lsh_h2_c64   \n",
       "6     7olzfob3         E2_pretraining_arxiv_bertsmall_lsh_h4_c128   \n",
       "7     9hb1xk49          E2_pretraining_arxiv_bertsmall_lsh_h4_c64   \n",
       "8     jibomvsj                 E1_pretraining_arxiv_bertsmall_mha   \n",
       "9     tzihl99m  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "10    71eagzdx  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "11    pu4bht10  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "12    vyeowm82   E2_pretraining_hyperpartisan_bertsmall_favor_nb1   \n",
       "13    i1bylyot  E2_pretraining_hyperpartisan_bertsmall_lsh_h2_...   \n",
       "14    2g7hhgqh  E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64   \n",
       "15    o9riawb7  E2_pretraining_hyperpartisan_bertsmall_lsh_h4_...   \n",
       "16    m6psyu6o  E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64   \n",
       "17    vi5hkih4         E1_pretraining_hyperpartisan_bertsmall_mha   \n",
       "18    mzlsnnzy        E2_pretraining_imdb_bertsmall_favor_nb0.125   \n",
       "19    1qxi1tf7         E2_pretraining_imdb_bertsmall_favor_nb0.25   \n",
       "20    1crsv1ae          E2_pretraining_imdb_bertsmall_favor_nb0.5   \n",
       "21    jucxpr34            E2_pretraining_imdb_bertsmall_favor_nb1   \n",
       "22    o031jp0p          E2_pretraining_imdb_bertsmall_lsh_h2_c128   \n",
       "23    a16qafd4           E2_pretraining_imdb_bertsmall_lsh_h2_c64   \n",
       "24    9fnn58aq          E2_pretraining_imdb_bertsmall_lsh_h4_c128   \n",
       "25    2fpocsb8           E2_pretraining_imdb_bertsmall_lsh_h4_c64   \n",
       "26    ghydwtq8                  E1_pretraining_imdb_bertsmall_mha   \n",
       "\n",
       "    tapt/duration_s  tapt/avg_epoch_time_s  tapt/train/avg_epoch_loss  \\\n",
       "0       4140.682855            2070.341427                   5.032768   \n",
       "1       4649.542211            2324.771106                   4.796945   \n",
       "2       5686.842252            2843.421126                   4.631720   \n",
       "3       7790.235805            3895.117902                   4.316468   \n",
       "4       5230.441202            2615.220601                   2.223732   \n",
       "5       4578.586053            2289.293026                   2.299913   \n",
       "6       7900.804413            3950.402206                   2.023962   \n",
       "7       6600.884504            3300.442252                   2.072415   \n",
       "8       8411.599251            4205.799625                   1.672165   \n",
       "9       5584.716981             930.786164                   4.505261   \n",
       "10      6335.581790            1055.930298                   3.913812   \n",
       "11      8143.261960            1357.210327                   3.665452   \n",
       "12     11534.012031            1922.335338                   3.267827   \n",
       "13      7766.640232            1294.440039                   2.439533   \n",
       "14      6697.284041            1116.214007                   2.557153   \n",
       "15     11807.380705            1967.896784                   2.263041   \n",
       "16      9655.740008            1609.290001                   2.351454   \n",
       "17      5994.331384             999.055231                   1.887050   \n",
       "18      1365.717010              91.047801                   3.199574   \n",
       "19      1540.193664             102.679578                   2.865847   \n",
       "20      1944.468278             129.631219                   2.834124   \n",
       "21      3032.204451             202.146963                   2.761649   \n",
       "22      2053.627131             136.908475                   2.502445   \n",
       "23      1774.054965             118.270331                   2.602880   \n",
       "24      3070.909744             204.727316                   2.478065   \n",
       "25      2522.166988             168.144466                   2.534777   \n",
       "26      1113.179355              74.211957                   2.387619   \n",
       "\n",
       "    tapt/train/gpu_mem_peak_mb tapt/stage        dataset           arch  \\\n",
       "0                 16991.537598       tapt          arxiv  favor_nb0.125   \n",
       "1                 18783.696777       tapt          arxiv   favor_nb0.25   \n",
       "2                 22371.815918       tapt          arxiv    favor_nb0.5   \n",
       "3                 29547.308105       tapt          arxiv      favor_nb1   \n",
       "4                 21855.970215       tapt          arxiv    lsh_h2_c128   \n",
       "5                 19551.526855       tapt          arxiv     lsh_h2_c64   \n",
       "6                 29580.395020       tapt          arxiv    lsh_h4_c128   \n",
       "7                 24974.400879       tapt          arxiv     lsh_h4_c64   \n",
       "8                 14793.368652       tapt          arxiv            mha   \n",
       "9                 16990.050293       tapt  hyperpartisan  favor_nb0.125   \n",
       "10                18786.679199       tapt  hyperpartisan   favor_nb0.25   \n",
       "11                22377.155762       tapt  hyperpartisan    favor_nb0.5   \n",
       "12                29563.599121       tapt  hyperpartisan      favor_nb1   \n",
       "13                21849.901855       tapt  hyperpartisan    lsh_h2_c128   \n",
       "14                19545.526855       tapt  hyperpartisan     lsh_h2_c64   \n",
       "15                29574.901855       tapt  hyperpartisan    lsh_h4_c128   \n",
       "16                24966.901855       tapt  hyperpartisan     lsh_h4_c64   \n",
       "17                14786.095215       tapt  hyperpartisan            mha   \n",
       "18                17015.669434       tapt           imdb  favor_nb0.125   \n",
       "19                18840.782715       tapt           imdb   favor_nb0.25   \n",
       "20                22489.657715       tapt           imdb    favor_nb0.5   \n",
       "21                29793.169434       tapt           imdb      favor_nb1   \n",
       "22                21848.151855       tapt           imdb    lsh_h2_c128   \n",
       "23                19543.776855       tapt           imdb     lsh_h2_c64   \n",
       "24                29574.151855       tapt           imdb    lsh_h4_c128   \n",
       "25                24966.026855       tapt           imdb     lsh_h4_c64   \n",
       "26                14784.345215       tapt           imdb            mha   \n",
       "\n",
       "   pretrain/run_id  ... pretrain/train/avg_epoch_loss  \\\n",
       "0         z28fmtgo  ...                      3.069484   \n",
       "1         0itox8yh  ...                      2.694388   \n",
       "2         yjcapkj3  ...                      2.666425   \n",
       "3         noug0x9s  ...                      2.578758   \n",
       "4         hu06m597  ...                      2.277830   \n",
       "5         eudij3yf  ...                      2.344709   \n",
       "6         0e2zxgor  ...                      2.248216   \n",
       "7         wi223o9o  ...                      2.285827   \n",
       "8         hd228t3k  ...                      2.156817   \n",
       "9         z28fmtgo  ...                      3.069484   \n",
       "10        0itox8yh  ...                      2.694388   \n",
       "11        yjcapkj3  ...                      2.666425   \n",
       "12        noug0x9s  ...                      2.578758   \n",
       "13        hu06m597  ...                      2.277830   \n",
       "14        eudij3yf  ...                      2.344709   \n",
       "15        0e2zxgor  ...                      2.248216   \n",
       "16        wi223o9o  ...                      2.285827   \n",
       "17        hd228t3k  ...                      2.156817   \n",
       "18        z28fmtgo  ...                      3.069484   \n",
       "19        0itox8yh  ...                      2.694388   \n",
       "20        yjcapkj3  ...                      2.666425   \n",
       "21        noug0x9s  ...                      2.578758   \n",
       "22        hu06m597  ...                      2.277830   \n",
       "23        eudij3yf  ...                      2.344709   \n",
       "24        0e2zxgor  ...                      2.248216   \n",
       "25        wi223o9o  ...                      2.285827   \n",
       "26        hd228t3k  ...                      2.156817   \n",
       "\n",
       "    pretrain/train/gpu_mem_peak_mb  pretrain/stage  finetune/run_id  \\\n",
       "0                     17017.976074        pretrain         fs5plm2w   \n",
       "1                     18842.976074        pretrain         tug4ff1q   \n",
       "2                     22493.976074        pretrain         3ksmzwr0   \n",
       "3                     29795.487793        pretrain         0zr9387l   \n",
       "4                     21848.345215        pretrain         xf2puymg   \n",
       "5                     19543.765137        pretrain         ebmfez6l   \n",
       "6                     29574.151855        pretrain         wqah0rrq   \n",
       "7                     24964.151855        pretrain         v0prpm6r   \n",
       "8                     14785.845215        pretrain         ied1ulsn   \n",
       "9                     17017.976074        pretrain         xhbujbys   \n",
       "10                    18842.976074        pretrain         3klrqv94   \n",
       "11                    22493.976074        pretrain         3jtkh8iw   \n",
       "12                    29795.487793        pretrain         k2j8xkd0   \n",
       "13                    21848.345215        pretrain         yjqias1k   \n",
       "14                    19543.765137        pretrain         ys8qnftm   \n",
       "15                    29574.151855        pretrain         gyaw279z   \n",
       "16                    24964.151855        pretrain         16tul7l4   \n",
       "17                    14785.845215        pretrain         ulsu61q4   \n",
       "18                    17017.976074        pretrain         d659uj2j   \n",
       "19                    18842.976074        pretrain         if2ozrk2   \n",
       "20                    22493.976074        pretrain         hhoyoxfp   \n",
       "21                    29795.487793        pretrain         u44pgkly   \n",
       "22                    21848.345215        pretrain         tq5q132s   \n",
       "23                    19543.765137        pretrain         j2x1q39c   \n",
       "24                    29574.151855        pretrain         s1c2rnaf   \n",
       "25                    24964.151855        pretrain         u262vax9   \n",
       "26                    14785.845215        pretrain         l2jq2x7g   \n",
       "\n",
       "                                    finetune/run_name finetune/duration_s  \\\n",
       "0         E2_finetuning_arxiv_bertsmall_favor_nb0.125         5314.207820   \n",
       "1          E2_finetuning_arxiv_bertsmall_favor_nb0.25         6291.965838   \n",
       "2           E2_finetuning_arxiv_bertsmall_favor_nb0.5         8319.524132   \n",
       "3             E2_finetuning_arxiv_bertsmall_favor_nb1        12386.017915   \n",
       "4           E2_finetuning_arxiv_bertsmall_lsh_h2_c128         7525.100944   \n",
       "5            E2_finetuning_arxiv_bertsmall_lsh_h2_c64         6224.091129   \n",
       "6           E2_finetuning_arxiv_bertsmall_lsh_h4_c128        12944.441540   \n",
       "7            E2_finetuning_arxiv_bertsmall_lsh_h4_c64        10335.925940   \n",
       "8      E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean        13235.081686   \n",
       "9   E2_finetuning_hyperpartisan_bertsmall_favor_nb...         3936.268652   \n",
       "10  E2_finetuning_hyperpartisan_bertsmall_favor_nb...         4820.846868   \n",
       "11  E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5         6889.461540   \n",
       "12    E2_finetuning_hyperpartisan_bertsmall_favor_nb1        10759.819213   \n",
       "13  E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128         6567.803232   \n",
       "14   E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64         5297.502927   \n",
       "15  E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128        11314.827481   \n",
       "16   E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64         8770.437606   \n",
       "17  E1_finetuning_hyperpartisan_bertsmall_mha_f1_d...         4338.320566   \n",
       "18         E2_finetuning_imdb_bertsmall_favor_nb0.125          452.629664   \n",
       "19          E2_finetuning_imdb_bertsmall_favor_nb0.25          535.890604   \n",
       "20           E2_finetuning_imdb_bertsmall_favor_nb0.5          728.522018   \n",
       "21             E2_finetuning_imdb_bertsmall_favor_nb1         1232.003859   \n",
       "22           E2_finetuning_imdb_bertsmall_lsh_h2_c128          790.832176   \n",
       "23            E2_finetuning_imdb_bertsmall_lsh_h2_c64          646.792933   \n",
       "24           E2_finetuning_imdb_bertsmall_lsh_h4_c128         1298.198816   \n",
       "25            E2_finetuning_imdb_bertsmall_lsh_h4_c64         1020.584711   \n",
       "26      E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean          309.168199   \n",
       "\n",
       "   finetune/avg_epoch_time_s finetune/test/f1_macro  \\\n",
       "0                1328.551955               0.860208   \n",
       "1                1572.991459               0.865219   \n",
       "2                2079.881033               0.865297   \n",
       "3                3096.504479               0.855688   \n",
       "4                1881.275236               0.872998   \n",
       "5                1556.022782               0.868965   \n",
       "6                3236.110385               0.874043   \n",
       "7                2583.981485               0.866779   \n",
       "8                3308.770421               0.883798   \n",
       "9                 562.324093               0.534071   \n",
       "10                688.692410               0.569878   \n",
       "11                984.208791               0.593013   \n",
       "12               1537.117030               0.550229   \n",
       "13                938.257605               0.612541   \n",
       "14                756.786132               0.653656   \n",
       "15               1616.403926               0.556799   \n",
       "16               1252.919658               0.624688   \n",
       "17                619.760081               0.645501   \n",
       "18                 56.578708               0.911595   \n",
       "19                 66.986325               0.909774   \n",
       "20                 91.065252               0.916397   \n",
       "21                154.000482               0.917398   \n",
       "22                 98.854022               0.926597   \n",
       "23                 80.849117               0.927800   \n",
       "24                162.274852               0.929800   \n",
       "25                127.573089               0.925000   \n",
       "26                 38.646025               0.928599   \n",
       "\n",
       "    finetune/train/gpu_mem_peak_mb  finetune/stage  \n",
       "0                      5725.700195        finetune  \n",
       "1                      7517.092773        finetune  \n",
       "2                     11104.350586        finetune  \n",
       "3                     18783.645020        finetune  \n",
       "4                     12174.626465        finetune  \n",
       "5                      9389.609375        finetune  \n",
       "6                     21973.819824        finetune  \n",
       "7                     16404.604980        finetune  \n",
       "8                      3524.024414        finetune  \n",
       "9                      5727.854492        finetune  \n",
       "10                     7519.026367        finetune  \n",
       "11                    11114.338867        finetune  \n",
       "12                    18797.067871        finetune  \n",
       "13                    12168.060547        finetune  \n",
       "14                     9382.263184        finetune  \n",
       "15                    21965.084473        finetune  \n",
       "16                    16397.189453        finetune  \n",
       "17                     3518.334961        finetune  \n",
       "18                     5633.739258        finetune  \n",
       "19                     7456.000977        finetune  \n",
       "20                    11106.000977        finetune  \n",
       "21                    18846.411133        finetune  \n",
       "22                    12042.296387        finetune  \n",
       "23                     9258.346680        finetune  \n",
       "24                    21841.253418        finetune  \n",
       "25                    16270.889648        finetune  \n",
       "26                     3399.745117        finetune  \n",
       "\n",
       "[27 rows x 23 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9b03581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = pd.DataFrame({\n",
    "    'dataset': ['hyperpartisan', 'imdb', 'arxiv'],\n",
    "    'arch': ['baseline', 'baseline', 'baseline'],\n",
    "    'finetune/test/f1_macro': [0.4223, 0.895, 0.8362] \n",
    "})\n",
    "\n",
    "for col in merged.columns:\n",
    "    if col not in baseline_data.columns:\n",
    "        baseline_data[col] = 0\n",
    "\n",
    "merged = pd.concat([merged, baseline_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2bee2f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tapt/run_id</th>\n",
       "      <th>tapt/run_name</th>\n",
       "      <th>tapt/duration_s</th>\n",
       "      <th>tapt/avg_epoch_time_s</th>\n",
       "      <th>tapt/train/avg_epoch_loss</th>\n",
       "      <th>tapt/train/gpu_mem_peak_mb</th>\n",
       "      <th>tapt/stage</th>\n",
       "      <th>dataset</th>\n",
       "      <th>arch</th>\n",
       "      <th>pretrain/run_id</th>\n",
       "      <th>...</th>\n",
       "      <th>pretrain/train/avg_epoch_loss</th>\n",
       "      <th>pretrain/train/gpu_mem_peak_mb</th>\n",
       "      <th>pretrain/stage</th>\n",
       "      <th>finetune/run_id</th>\n",
       "      <th>finetune/run_name</th>\n",
       "      <th>finetune/duration_s</th>\n",
       "      <th>finetune/avg_epoch_time_s</th>\n",
       "      <th>finetune/test/f1_macro</th>\n",
       "      <th>finetune/train/gpu_mem_peak_mb</th>\n",
       "      <th>finetune/stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t67wmtc1</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.125</td>\n",
       "      <td>4140.682855</td>\n",
       "      <td>2070.341427</td>\n",
       "      <td>5.032768</td>\n",
       "      <td>16991.537598</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>3.069484</td>\n",
       "      <td>17017.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>fs5plm2w</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.125</td>\n",
       "      <td>5314.207820</td>\n",
       "      <td>1328.551955</td>\n",
       "      <td>0.860208</td>\n",
       "      <td>5725.700195</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aplguq3f</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.25</td>\n",
       "      <td>4649.542211</td>\n",
       "      <td>2324.771106</td>\n",
       "      <td>4.796945</td>\n",
       "      <td>18783.696777</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694388</td>\n",
       "      <td>18842.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>tug4ff1q</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.25</td>\n",
       "      <td>6291.965838</td>\n",
       "      <td>1572.991459</td>\n",
       "      <td>0.865219</td>\n",
       "      <td>7517.092773</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k9km9nuj</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.5</td>\n",
       "      <td>5686.842252</td>\n",
       "      <td>2843.421126</td>\n",
       "      <td>4.631720</td>\n",
       "      <td>22371.815918</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.666425</td>\n",
       "      <td>22493.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>3ksmzwr0</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb0.5</td>\n",
       "      <td>8319.524132</td>\n",
       "      <td>2079.881033</td>\n",
       "      <td>0.865297</td>\n",
       "      <td>11104.350586</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p0sm9w3v</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb1</td>\n",
       "      <td>7790.235805</td>\n",
       "      <td>3895.117902</td>\n",
       "      <td>4.316468</td>\n",
       "      <td>29547.308105</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>2.578758</td>\n",
       "      <td>29795.487793</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>0zr9387l</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_favor_nb1</td>\n",
       "      <td>12386.017915</td>\n",
       "      <td>3096.504479</td>\n",
       "      <td>0.855688</td>\n",
       "      <td>18783.645020</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4h910kzj</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h2_c128</td>\n",
       "      <td>5230.441202</td>\n",
       "      <td>2615.220601</td>\n",
       "      <td>2.223732</td>\n",
       "      <td>21855.970215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>2.277830</td>\n",
       "      <td>21848.345215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>xf2puymg</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h2_c128</td>\n",
       "      <td>7525.100944</td>\n",
       "      <td>1881.275236</td>\n",
       "      <td>0.872998</td>\n",
       "      <td>12174.626465</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kb5ewjpp</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h2_c64</td>\n",
       "      <td>4578.586053</td>\n",
       "      <td>2289.293026</td>\n",
       "      <td>2.299913</td>\n",
       "      <td>19551.526855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344709</td>\n",
       "      <td>19543.765137</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ebmfez6l</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h2_c64</td>\n",
       "      <td>6224.091129</td>\n",
       "      <td>1556.022782</td>\n",
       "      <td>0.868965</td>\n",
       "      <td>9389.609375</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7olzfob3</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h4_c128</td>\n",
       "      <td>7900.804413</td>\n",
       "      <td>3950.402206</td>\n",
       "      <td>2.023962</td>\n",
       "      <td>29580.395020</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>2.248216</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>wqah0rrq</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h4_c128</td>\n",
       "      <td>12944.441540</td>\n",
       "      <td>3236.110385</td>\n",
       "      <td>0.874043</td>\n",
       "      <td>21973.819824</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9hb1xk49</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h4_c64</td>\n",
       "      <td>6600.884504</td>\n",
       "      <td>3300.442252</td>\n",
       "      <td>2.072415</td>\n",
       "      <td>24974.400879</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>2.285827</td>\n",
       "      <td>24964.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>v0prpm6r</td>\n",
       "      <td>E2_finetuning_arxiv_bertsmall_lsh_h4_c64</td>\n",
       "      <td>10335.925940</td>\n",
       "      <td>2583.981485</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>16404.604980</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jibomvsj</td>\n",
       "      <td>E1_pretraining_arxiv_bertsmall_mha</td>\n",
       "      <td>8411.599251</td>\n",
       "      <td>4205.799625</td>\n",
       "      <td>1.672165</td>\n",
       "      <td>14793.368652</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156817</td>\n",
       "      <td>14785.845215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ied1ulsn</td>\n",
       "      <td>E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean</td>\n",
       "      <td>13235.081686</td>\n",
       "      <td>3308.770421</td>\n",
       "      <td>0.883798</td>\n",
       "      <td>3524.024414</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tzihl99m</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>5584.716981</td>\n",
       "      <td>930.786164</td>\n",
       "      <td>4.505261</td>\n",
       "      <td>16990.050293</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>3.069484</td>\n",
       "      <td>17017.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>xhbujbys</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb...</td>\n",
       "      <td>3936.268652</td>\n",
       "      <td>562.324093</td>\n",
       "      <td>0.534071</td>\n",
       "      <td>5727.854492</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>71eagzdx</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>6335.581790</td>\n",
       "      <td>1055.930298</td>\n",
       "      <td>3.913812</td>\n",
       "      <td>18786.679199</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694388</td>\n",
       "      <td>18842.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>3klrqv94</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb...</td>\n",
       "      <td>4820.846868</td>\n",
       "      <td>688.692410</td>\n",
       "      <td>0.569878</td>\n",
       "      <td>7519.026367</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pu4bht10</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>8143.261960</td>\n",
       "      <td>1357.210327</td>\n",
       "      <td>3.665452</td>\n",
       "      <td>22377.155762</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.666425</td>\n",
       "      <td>22493.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>3jtkh8iw</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5</td>\n",
       "      <td>6889.461540</td>\n",
       "      <td>984.208791</td>\n",
       "      <td>0.593013</td>\n",
       "      <td>11114.338867</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vyeowm82</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_nb1</td>\n",
       "      <td>11534.012031</td>\n",
       "      <td>1922.335338</td>\n",
       "      <td>3.267827</td>\n",
       "      <td>29563.599121</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>2.578758</td>\n",
       "      <td>29795.487793</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>k2j8xkd0</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_favor_nb1</td>\n",
       "      <td>10759.819213</td>\n",
       "      <td>1537.117030</td>\n",
       "      <td>0.550229</td>\n",
       "      <td>18797.067871</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i1bylyot</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h2_...</td>\n",
       "      <td>7766.640232</td>\n",
       "      <td>1294.440039</td>\n",
       "      <td>2.439533</td>\n",
       "      <td>21849.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>2.277830</td>\n",
       "      <td>21848.345215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>yjqias1k</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128</td>\n",
       "      <td>6567.803232</td>\n",
       "      <td>938.257605</td>\n",
       "      <td>0.612541</td>\n",
       "      <td>12168.060547</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2g7hhgqh</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64</td>\n",
       "      <td>6697.284041</td>\n",
       "      <td>1116.214007</td>\n",
       "      <td>2.557153</td>\n",
       "      <td>19545.526855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344709</td>\n",
       "      <td>19543.765137</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ys8qnftm</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64</td>\n",
       "      <td>5297.502927</td>\n",
       "      <td>756.786132</td>\n",
       "      <td>0.653656</td>\n",
       "      <td>9382.263184</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o9riawb7</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h4_...</td>\n",
       "      <td>11807.380705</td>\n",
       "      <td>1967.896784</td>\n",
       "      <td>2.263041</td>\n",
       "      <td>29574.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>2.248216</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>gyaw279z</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128</td>\n",
       "      <td>11314.827481</td>\n",
       "      <td>1616.403926</td>\n",
       "      <td>0.556799</td>\n",
       "      <td>21965.084473</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>m6psyu6o</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64</td>\n",
       "      <td>9655.740008</td>\n",
       "      <td>1609.290001</td>\n",
       "      <td>2.351454</td>\n",
       "      <td>24966.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>2.285827</td>\n",
       "      <td>24964.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>16tul7l4</td>\n",
       "      <td>E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64</td>\n",
       "      <td>8770.437606</td>\n",
       "      <td>1252.919658</td>\n",
       "      <td>0.624688</td>\n",
       "      <td>16397.189453</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vi5hkih4</td>\n",
       "      <td>E1_pretraining_hyperpartisan_bertsmall_mha</td>\n",
       "      <td>5994.331384</td>\n",
       "      <td>999.055231</td>\n",
       "      <td>1.887050</td>\n",
       "      <td>14786.095215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156817</td>\n",
       "      <td>14785.845215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ulsu61q4</td>\n",
       "      <td>E1_finetuning_hyperpartisan_bertsmall_mha_f1_d...</td>\n",
       "      <td>4338.320566</td>\n",
       "      <td>619.760081</td>\n",
       "      <td>0.645501</td>\n",
       "      <td>3518.334961</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mzlsnnzy</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.125</td>\n",
       "      <td>1365.717010</td>\n",
       "      <td>91.047801</td>\n",
       "      <td>3.199574</td>\n",
       "      <td>17015.669434</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>3.069484</td>\n",
       "      <td>17017.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>d659uj2j</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.125</td>\n",
       "      <td>452.629664</td>\n",
       "      <td>56.578708</td>\n",
       "      <td>0.911595</td>\n",
       "      <td>5633.739258</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1qxi1tf7</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.25</td>\n",
       "      <td>1540.193664</td>\n",
       "      <td>102.679578</td>\n",
       "      <td>2.865847</td>\n",
       "      <td>18840.782715</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694388</td>\n",
       "      <td>18842.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>if2ozrk2</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.25</td>\n",
       "      <td>535.890604</td>\n",
       "      <td>66.986325</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>7456.000977</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1crsv1ae</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.5</td>\n",
       "      <td>1944.468278</td>\n",
       "      <td>129.631219</td>\n",
       "      <td>2.834124</td>\n",
       "      <td>22489.657715</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.666425</td>\n",
       "      <td>22493.976074</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>hhoyoxfp</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb0.5</td>\n",
       "      <td>728.522018</td>\n",
       "      <td>91.065252</td>\n",
       "      <td>0.916397</td>\n",
       "      <td>11106.000977</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>jucxpr34</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb1</td>\n",
       "      <td>3032.204451</td>\n",
       "      <td>202.146963</td>\n",
       "      <td>2.761649</td>\n",
       "      <td>29793.169434</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>2.578758</td>\n",
       "      <td>29795.487793</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>u44pgkly</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_favor_nb1</td>\n",
       "      <td>1232.003859</td>\n",
       "      <td>154.000482</td>\n",
       "      <td>0.917398</td>\n",
       "      <td>18846.411133</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>o031jp0p</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h2_c128</td>\n",
       "      <td>2053.627131</td>\n",
       "      <td>136.908475</td>\n",
       "      <td>2.502445</td>\n",
       "      <td>21848.151855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>2.277830</td>\n",
       "      <td>21848.345215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>tq5q132s</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h2_c128</td>\n",
       "      <td>790.832176</td>\n",
       "      <td>98.854022</td>\n",
       "      <td>0.926597</td>\n",
       "      <td>12042.296387</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a16qafd4</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h2_c64</td>\n",
       "      <td>1774.054965</td>\n",
       "      <td>118.270331</td>\n",
       "      <td>2.602880</td>\n",
       "      <td>19543.776855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344709</td>\n",
       "      <td>19543.765137</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>j2x1q39c</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h2_c64</td>\n",
       "      <td>646.792933</td>\n",
       "      <td>80.849117</td>\n",
       "      <td>0.927800</td>\n",
       "      <td>9258.346680</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9fnn58aq</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h4_c128</td>\n",
       "      <td>3070.909744</td>\n",
       "      <td>204.727316</td>\n",
       "      <td>2.478065</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>2.248216</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>s1c2rnaf</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h4_c128</td>\n",
       "      <td>1298.198816</td>\n",
       "      <td>162.274852</td>\n",
       "      <td>0.929800</td>\n",
       "      <td>21841.253418</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2fpocsb8</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h4_c64</td>\n",
       "      <td>2522.166988</td>\n",
       "      <td>168.144466</td>\n",
       "      <td>2.534777</td>\n",
       "      <td>24966.026855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>2.285827</td>\n",
       "      <td>24964.151855</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>u262vax9</td>\n",
       "      <td>E2_finetuning_imdb_bertsmall_lsh_h4_c64</td>\n",
       "      <td>1020.584711</td>\n",
       "      <td>127.573089</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>16270.889648</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ghydwtq8</td>\n",
       "      <td>E1_pretraining_imdb_bertsmall_mha</td>\n",
       "      <td>1113.179355</td>\n",
       "      <td>74.211957</td>\n",
       "      <td>2.387619</td>\n",
       "      <td>14784.345215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156817</td>\n",
       "      <td>14785.845215</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>l2jq2x7g</td>\n",
       "      <td>E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean</td>\n",
       "      <td>309.168199</td>\n",
       "      <td>38.646025</td>\n",
       "      <td>0.928599</td>\n",
       "      <td>3399.745117</td>\n",
       "      <td>finetune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tapt/run_id                                      tapt/run_name  \\\n",
       "0     t67wmtc1       E2_pretraining_arxiv_bertsmall_favor_nb0.125   \n",
       "1     aplguq3f        E2_pretraining_arxiv_bertsmall_favor_nb0.25   \n",
       "2     k9km9nuj         E2_pretraining_arxiv_bertsmall_favor_nb0.5   \n",
       "3     p0sm9w3v           E2_pretraining_arxiv_bertsmall_favor_nb1   \n",
       "4     4h910kzj         E2_pretraining_arxiv_bertsmall_lsh_h2_c128   \n",
       "5     kb5ewjpp          E2_pretraining_arxiv_bertsmall_lsh_h2_c64   \n",
       "6     7olzfob3         E2_pretraining_arxiv_bertsmall_lsh_h4_c128   \n",
       "7     9hb1xk49          E2_pretraining_arxiv_bertsmall_lsh_h4_c64   \n",
       "8     jibomvsj                 E1_pretraining_arxiv_bertsmall_mha   \n",
       "9     tzihl99m  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "10    71eagzdx  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "11    pu4bht10  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "12    vyeowm82   E2_pretraining_hyperpartisan_bertsmall_favor_nb1   \n",
       "13    i1bylyot  E2_pretraining_hyperpartisan_bertsmall_lsh_h2_...   \n",
       "14    2g7hhgqh  E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64   \n",
       "15    o9riawb7  E2_pretraining_hyperpartisan_bertsmall_lsh_h4_...   \n",
       "16    m6psyu6o  E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64   \n",
       "17    vi5hkih4         E1_pretraining_hyperpartisan_bertsmall_mha   \n",
       "18    mzlsnnzy        E2_pretraining_imdb_bertsmall_favor_nb0.125   \n",
       "19    1qxi1tf7         E2_pretraining_imdb_bertsmall_favor_nb0.25   \n",
       "20    1crsv1ae          E2_pretraining_imdb_bertsmall_favor_nb0.5   \n",
       "21    jucxpr34            E2_pretraining_imdb_bertsmall_favor_nb1   \n",
       "22    o031jp0p          E2_pretraining_imdb_bertsmall_lsh_h2_c128   \n",
       "23    a16qafd4           E2_pretraining_imdb_bertsmall_lsh_h2_c64   \n",
       "24    9fnn58aq          E2_pretraining_imdb_bertsmall_lsh_h4_c128   \n",
       "25    2fpocsb8           E2_pretraining_imdb_bertsmall_lsh_h4_c64   \n",
       "26    ghydwtq8                  E1_pretraining_imdb_bertsmall_mha   \n",
       "27           0                                                  0   \n",
       "28           0                                                  0   \n",
       "29           0                                                  0   \n",
       "\n",
       "    tapt/duration_s  tapt/avg_epoch_time_s  tapt/train/avg_epoch_loss  \\\n",
       "0       4140.682855            2070.341427                   5.032768   \n",
       "1       4649.542211            2324.771106                   4.796945   \n",
       "2       5686.842252            2843.421126                   4.631720   \n",
       "3       7790.235805            3895.117902                   4.316468   \n",
       "4       5230.441202            2615.220601                   2.223732   \n",
       "5       4578.586053            2289.293026                   2.299913   \n",
       "6       7900.804413            3950.402206                   2.023962   \n",
       "7       6600.884504            3300.442252                   2.072415   \n",
       "8       8411.599251            4205.799625                   1.672165   \n",
       "9       5584.716981             930.786164                   4.505261   \n",
       "10      6335.581790            1055.930298                   3.913812   \n",
       "11      8143.261960            1357.210327                   3.665452   \n",
       "12     11534.012031            1922.335338                   3.267827   \n",
       "13      7766.640232            1294.440039                   2.439533   \n",
       "14      6697.284041            1116.214007                   2.557153   \n",
       "15     11807.380705            1967.896784                   2.263041   \n",
       "16      9655.740008            1609.290001                   2.351454   \n",
       "17      5994.331384             999.055231                   1.887050   \n",
       "18      1365.717010              91.047801                   3.199574   \n",
       "19      1540.193664             102.679578                   2.865847   \n",
       "20      1944.468278             129.631219                   2.834124   \n",
       "21      3032.204451             202.146963                   2.761649   \n",
       "22      2053.627131             136.908475                   2.502445   \n",
       "23      1774.054965             118.270331                   2.602880   \n",
       "24      3070.909744             204.727316                   2.478065   \n",
       "25      2522.166988             168.144466                   2.534777   \n",
       "26      1113.179355              74.211957                   2.387619   \n",
       "27         0.000000               0.000000                   0.000000   \n",
       "28         0.000000               0.000000                   0.000000   \n",
       "29         0.000000               0.000000                   0.000000   \n",
       "\n",
       "    tapt/train/gpu_mem_peak_mb tapt/stage        dataset           arch  \\\n",
       "0                 16991.537598       tapt          arxiv  favor_nb0.125   \n",
       "1                 18783.696777       tapt          arxiv   favor_nb0.25   \n",
       "2                 22371.815918       tapt          arxiv    favor_nb0.5   \n",
       "3                 29547.308105       tapt          arxiv      favor_nb1   \n",
       "4                 21855.970215       tapt          arxiv    lsh_h2_c128   \n",
       "5                 19551.526855       tapt          arxiv     lsh_h2_c64   \n",
       "6                 29580.395020       tapt          arxiv    lsh_h4_c128   \n",
       "7                 24974.400879       tapt          arxiv     lsh_h4_c64   \n",
       "8                 14793.368652       tapt          arxiv            mha   \n",
       "9                 16990.050293       tapt  hyperpartisan  favor_nb0.125   \n",
       "10                18786.679199       tapt  hyperpartisan   favor_nb0.25   \n",
       "11                22377.155762       tapt  hyperpartisan    favor_nb0.5   \n",
       "12                29563.599121       tapt  hyperpartisan      favor_nb1   \n",
       "13                21849.901855       tapt  hyperpartisan    lsh_h2_c128   \n",
       "14                19545.526855       tapt  hyperpartisan     lsh_h2_c64   \n",
       "15                29574.901855       tapt  hyperpartisan    lsh_h4_c128   \n",
       "16                24966.901855       tapt  hyperpartisan     lsh_h4_c64   \n",
       "17                14786.095215       tapt  hyperpartisan            mha   \n",
       "18                17015.669434       tapt           imdb  favor_nb0.125   \n",
       "19                18840.782715       tapt           imdb   favor_nb0.25   \n",
       "20                22489.657715       tapt           imdb    favor_nb0.5   \n",
       "21                29793.169434       tapt           imdb      favor_nb1   \n",
       "22                21848.151855       tapt           imdb    lsh_h2_c128   \n",
       "23                19543.776855       tapt           imdb     lsh_h2_c64   \n",
       "24                29574.151855       tapt           imdb    lsh_h4_c128   \n",
       "25                24966.026855       tapt           imdb     lsh_h4_c64   \n",
       "26                14784.345215       tapt           imdb            mha   \n",
       "27                    0.000000          0  hyperpartisan       baseline   \n",
       "28                    0.000000          0           imdb       baseline   \n",
       "29                    0.000000          0          arxiv       baseline   \n",
       "\n",
       "   pretrain/run_id  ... pretrain/train/avg_epoch_loss  \\\n",
       "0         z28fmtgo  ...                      3.069484   \n",
       "1         0itox8yh  ...                      2.694388   \n",
       "2         yjcapkj3  ...                      2.666425   \n",
       "3         noug0x9s  ...                      2.578758   \n",
       "4         hu06m597  ...                      2.277830   \n",
       "5         eudij3yf  ...                      2.344709   \n",
       "6         0e2zxgor  ...                      2.248216   \n",
       "7         wi223o9o  ...                      2.285827   \n",
       "8         hd228t3k  ...                      2.156817   \n",
       "9         z28fmtgo  ...                      3.069484   \n",
       "10        0itox8yh  ...                      2.694388   \n",
       "11        yjcapkj3  ...                      2.666425   \n",
       "12        noug0x9s  ...                      2.578758   \n",
       "13        hu06m597  ...                      2.277830   \n",
       "14        eudij3yf  ...                      2.344709   \n",
       "15        0e2zxgor  ...                      2.248216   \n",
       "16        wi223o9o  ...                      2.285827   \n",
       "17        hd228t3k  ...                      2.156817   \n",
       "18        z28fmtgo  ...                      3.069484   \n",
       "19        0itox8yh  ...                      2.694388   \n",
       "20        yjcapkj3  ...                      2.666425   \n",
       "21        noug0x9s  ...                      2.578758   \n",
       "22        hu06m597  ...                      2.277830   \n",
       "23        eudij3yf  ...                      2.344709   \n",
       "24        0e2zxgor  ...                      2.248216   \n",
       "25        wi223o9o  ...                      2.285827   \n",
       "26        hd228t3k  ...                      2.156817   \n",
       "27               0  ...                      0.000000   \n",
       "28               0  ...                      0.000000   \n",
       "29               0  ...                      0.000000   \n",
       "\n",
       "    pretrain/train/gpu_mem_peak_mb  pretrain/stage  finetune/run_id  \\\n",
       "0                     17017.976074        pretrain         fs5plm2w   \n",
       "1                     18842.976074        pretrain         tug4ff1q   \n",
       "2                     22493.976074        pretrain         3ksmzwr0   \n",
       "3                     29795.487793        pretrain         0zr9387l   \n",
       "4                     21848.345215        pretrain         xf2puymg   \n",
       "5                     19543.765137        pretrain         ebmfez6l   \n",
       "6                     29574.151855        pretrain         wqah0rrq   \n",
       "7                     24964.151855        pretrain         v0prpm6r   \n",
       "8                     14785.845215        pretrain         ied1ulsn   \n",
       "9                     17017.976074        pretrain         xhbujbys   \n",
       "10                    18842.976074        pretrain         3klrqv94   \n",
       "11                    22493.976074        pretrain         3jtkh8iw   \n",
       "12                    29795.487793        pretrain         k2j8xkd0   \n",
       "13                    21848.345215        pretrain         yjqias1k   \n",
       "14                    19543.765137        pretrain         ys8qnftm   \n",
       "15                    29574.151855        pretrain         gyaw279z   \n",
       "16                    24964.151855        pretrain         16tul7l4   \n",
       "17                    14785.845215        pretrain         ulsu61q4   \n",
       "18                    17017.976074        pretrain         d659uj2j   \n",
       "19                    18842.976074        pretrain         if2ozrk2   \n",
       "20                    22493.976074        pretrain         hhoyoxfp   \n",
       "21                    29795.487793        pretrain         u44pgkly   \n",
       "22                    21848.345215        pretrain         tq5q132s   \n",
       "23                    19543.765137        pretrain         j2x1q39c   \n",
       "24                    29574.151855        pretrain         s1c2rnaf   \n",
       "25                    24964.151855        pretrain         u262vax9   \n",
       "26                    14785.845215        pretrain         l2jq2x7g   \n",
       "27                        0.000000               0                0   \n",
       "28                        0.000000               0                0   \n",
       "29                        0.000000               0                0   \n",
       "\n",
       "                                    finetune/run_name finetune/duration_s  \\\n",
       "0         E2_finetuning_arxiv_bertsmall_favor_nb0.125         5314.207820   \n",
       "1          E2_finetuning_arxiv_bertsmall_favor_nb0.25         6291.965838   \n",
       "2           E2_finetuning_arxiv_bertsmall_favor_nb0.5         8319.524132   \n",
       "3             E2_finetuning_arxiv_bertsmall_favor_nb1        12386.017915   \n",
       "4           E2_finetuning_arxiv_bertsmall_lsh_h2_c128         7525.100944   \n",
       "5            E2_finetuning_arxiv_bertsmall_lsh_h2_c64         6224.091129   \n",
       "6           E2_finetuning_arxiv_bertsmall_lsh_h4_c128        12944.441540   \n",
       "7            E2_finetuning_arxiv_bertsmall_lsh_h4_c64        10335.925940   \n",
       "8      E1_finetuning_arxiv_bertsmall_mha_f1_d0.2_mean        13235.081686   \n",
       "9   E2_finetuning_hyperpartisan_bertsmall_favor_nb...         3936.268652   \n",
       "10  E2_finetuning_hyperpartisan_bertsmall_favor_nb...         4820.846868   \n",
       "11  E2_finetuning_hyperpartisan_bertsmall_favor_nb0.5         6889.461540   \n",
       "12    E2_finetuning_hyperpartisan_bertsmall_favor_nb1        10759.819213   \n",
       "13  E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c128         6567.803232   \n",
       "14   E2_finetuning_hyperpartisan_bertsmall_lsh_h2_c64         5297.502927   \n",
       "15  E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c128        11314.827481   \n",
       "16   E2_finetuning_hyperpartisan_bertsmall_lsh_h4_c64         8770.437606   \n",
       "17  E1_finetuning_hyperpartisan_bertsmall_mha_f1_d...         4338.320566   \n",
       "18         E2_finetuning_imdb_bertsmall_favor_nb0.125          452.629664   \n",
       "19          E2_finetuning_imdb_bertsmall_favor_nb0.25          535.890604   \n",
       "20           E2_finetuning_imdb_bertsmall_favor_nb0.5          728.522018   \n",
       "21             E2_finetuning_imdb_bertsmall_favor_nb1         1232.003859   \n",
       "22           E2_finetuning_imdb_bertsmall_lsh_h2_c128          790.832176   \n",
       "23            E2_finetuning_imdb_bertsmall_lsh_h2_c64          646.792933   \n",
       "24           E2_finetuning_imdb_bertsmall_lsh_h4_c128         1298.198816   \n",
       "25            E2_finetuning_imdb_bertsmall_lsh_h4_c64         1020.584711   \n",
       "26      E1_finetuning_imdb_bertsmall_mha_f2_d0.2_mean          309.168199   \n",
       "27                                                  0            0.000000   \n",
       "28                                                  0            0.000000   \n",
       "29                                                  0            0.000000   \n",
       "\n",
       "   finetune/avg_epoch_time_s finetune/test/f1_macro  \\\n",
       "0                1328.551955               0.860208   \n",
       "1                1572.991459               0.865219   \n",
       "2                2079.881033               0.865297   \n",
       "3                3096.504479               0.855688   \n",
       "4                1881.275236               0.872998   \n",
       "5                1556.022782               0.868965   \n",
       "6                3236.110385               0.874043   \n",
       "7                2583.981485               0.866779   \n",
       "8                3308.770421               0.883798   \n",
       "9                 562.324093               0.534071   \n",
       "10                688.692410               0.569878   \n",
       "11                984.208791               0.593013   \n",
       "12               1537.117030               0.550229   \n",
       "13                938.257605               0.612541   \n",
       "14                756.786132               0.653656   \n",
       "15               1616.403926               0.556799   \n",
       "16               1252.919658               0.624688   \n",
       "17                619.760081               0.645501   \n",
       "18                 56.578708               0.911595   \n",
       "19                 66.986325               0.909774   \n",
       "20                 91.065252               0.916397   \n",
       "21                154.000482               0.917398   \n",
       "22                 98.854022               0.926597   \n",
       "23                 80.849117               0.927800   \n",
       "24                162.274852               0.929800   \n",
       "25                127.573089               0.925000   \n",
       "26                 38.646025               0.928599   \n",
       "27                  0.000000               0.422300   \n",
       "28                  0.000000               0.895000   \n",
       "29                  0.000000               0.836200   \n",
       "\n",
       "    finetune/train/gpu_mem_peak_mb  finetune/stage  \n",
       "0                      5725.700195        finetune  \n",
       "1                      7517.092773        finetune  \n",
       "2                     11104.350586        finetune  \n",
       "3                     18783.645020        finetune  \n",
       "4                     12174.626465        finetune  \n",
       "5                      9389.609375        finetune  \n",
       "6                     21973.819824        finetune  \n",
       "7                     16404.604980        finetune  \n",
       "8                      3524.024414        finetune  \n",
       "9                      5727.854492        finetune  \n",
       "10                     7519.026367        finetune  \n",
       "11                    11114.338867        finetune  \n",
       "12                    18797.067871        finetune  \n",
       "13                    12168.060547        finetune  \n",
       "14                     9382.263184        finetune  \n",
       "15                    21965.084473        finetune  \n",
       "16                    16397.189453        finetune  \n",
       "17                     3518.334961        finetune  \n",
       "18                     5633.739258        finetune  \n",
       "19                     7456.000977        finetune  \n",
       "20                    11106.000977        finetune  \n",
       "21                    18846.411133        finetune  \n",
       "22                    12042.296387        finetune  \n",
       "23                     9258.346680        finetune  \n",
       "24                    21841.253418        finetune  \n",
       "25                    16270.889648        finetune  \n",
       "26                     3399.745117        finetune  \n",
       "27                        0.000000               0  \n",
       "28                        0.000000               0  \n",
       "29                        0.000000               0  \n",
       "\n",
       "[30 rows x 23 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f040692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['finetune/train/gpu_mem_peak_gb'] = merged['finetune/train/gpu_mem_peak_mb']/1024\n",
    "merged['tapt/train/gpu_mem_peak_gb'] = merged['tapt/train/gpu_mem_peak_mb']/1024\n",
    "merged['pretrain/train/gpu_mem_peak_gb'] = merged['pretrain/train/gpu_mem_peak_mb']/1024\n",
    "\n",
    "merged['finetune/duration_min'] = merged['finetune/duration_s']/60\n",
    "merged['tapt/duration_min'] = merged['tapt/duration_s']/60\n",
    "merged['pretrain/duration_min'] = merged['pretrain/duration_s']/60\n",
    "\n",
    "merged['pretrain/avg_epoch_time_min'] = merged['pretrain/avg_epoch_time_s']/60\n",
    "merged['tapt/avg_epoch_time_min'] = merged['tapt/avg_epoch_time_s']/60\n",
    "merged['finetune/avg_epoch_time_min'] = merged['finetune/avg_epoch_time_s']/60\n",
    "\n",
    "merged['finetune/test/f1_macro'] = merged['finetune/test/f1_macro'] * 100\n",
    "\n",
    "merged['finetune_tapt/duration_min'] = merged['finetune/duration_min'] + merged['tapt/duration_min']\n",
    "merged['finetune_tapt/train/gpu_mem_peak_gb'] = merged[['finetune/train/gpu_mem_peak_gb', 'tapt/train/gpu_mem_peak_gb']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b3e3733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tapt/run_id', 'tapt/run_name', 'tapt/duration_s',\n",
       "       'tapt/avg_epoch_time_s', 'tapt/train/avg_epoch_loss',\n",
       "       'tapt/train/gpu_mem_peak_mb', 'tapt/stage', 'dataset', 'arch',\n",
       "       'pretrain/run_id', 'pretrain/run_name', 'pretrain/duration_s',\n",
       "       'pretrain/avg_epoch_time_s', 'pretrain/train/avg_epoch_loss',\n",
       "       'pretrain/train/gpu_mem_peak_mb', 'pretrain/stage', 'finetune/run_id',\n",
       "       'finetune/run_name', 'finetune/duration_s', 'finetune/avg_epoch_time_s',\n",
       "       'finetune/test/f1_macro', 'finetune/train/gpu_mem_peak_mb',\n",
       "       'finetune/stage', 'finetune/train/gpu_mem_peak_gb',\n",
       "       'tapt/train/gpu_mem_peak_gb', 'pretrain/train/gpu_mem_peak_gb',\n",
       "       'finetune/duration_min', 'tapt/duration_min', 'pretrain/duration_min',\n",
       "       'pretrain/avg_epoch_time_min', 'tapt/avg_epoch_time_min',\n",
       "       'finetune/avg_epoch_time_min', 'finetune_tapt/duration_min',\n",
       "       'finetune_tapt/train/gpu_mem_peak_gb'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5da3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = calculate_relative_metrics(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in final['dataset'].unique():\n",
    "    mask = final['dataset'] == dataset\n",
    "    mha_mask = mask & (final['arch'] == 'mha')\n",
    "    \n",
    "    if mha_mask.sum() > 0:\n",
    "        mha_vram = final.loc[mha_mask, 'pretrain/train/gpu_mem_peak_gb'].values[0]\n",
    "        if mha_vram != 0:\n",
    "            final.loc[mask, 'pretrain/train/gpu_mem_peak_gb_pct_vs_mha'] = \\\n",
    "                ((final.loc[mask, 'pretrain/train/gpu_mem_peak_gb'] - mha_vram) / mha_vram) * 100\n",
    "        \n",
    "        mha_dur = final.loc[mha_mask, 'pretrain/avg_epoch_time_min'].values[0]\n",
    "        if mha_dur != 0:\n",
    "            final.loc[mask, 'pretrain/avg_epoch_time_min_pct_vs_mha'] = \\\n",
    "                ((final.loc[mask, 'pretrain/avg_epoch_time_min'] - mha_dur) / mha_dur) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f5a539c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tapt/run_id</th>\n",
       "      <th>tapt/run_name</th>\n",
       "      <th>tapt/duration_s</th>\n",
       "      <th>tapt/avg_epoch_time_s</th>\n",
       "      <th>tapt/train/avg_epoch_loss</th>\n",
       "      <th>tapt/train/gpu_mem_peak_mb</th>\n",
       "      <th>tapt/stage</th>\n",
       "      <th>dataset</th>\n",
       "      <th>arch</th>\n",
       "      <th>pretrain/run_id</th>\n",
       "      <th>...</th>\n",
       "      <th>tapt/avg_epoch_time_min</th>\n",
       "      <th>finetune/avg_epoch_time_min</th>\n",
       "      <th>finetune_tapt/duration_min</th>\n",
       "      <th>finetune_tapt/train/gpu_mem_peak_gb</th>\n",
       "      <th>finetune_tapt/train/gpu_mem_peak_gb_pct_vs_mha</th>\n",
       "      <th>finetune_tapt/duration_min_pct_vs_mha</th>\n",
       "      <th>finetune/test/f1_macro_pp_vs_mha</th>\n",
       "      <th>finetune/test/f1_macro_pp_vs_baseline</th>\n",
       "      <th>pretrain/train/gpu_mem_peak_gb_pct_vs_mha</th>\n",
       "      <th>pretrain/avg_epoch_time_min_pct_vs_mha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t67wmtc1</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.125</td>\n",
       "      <td>4140.682855</td>\n",
       "      <td>2070.341427</td>\n",
       "      <td>5.032768</td>\n",
       "      <td>16991.537598</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>34.505690</td>\n",
       "      <td>22.142533</td>\n",
       "      <td>157.581511</td>\n",
       "      <td>16.593298</td>\n",
       "      <td>14.859151</td>\n",
       "      <td>-56.321753</td>\n",
       "      <td>-2.359026</td>\n",
       "      <td>2.400755</td>\n",
       "      <td>15.096404</td>\n",
       "      <td>39.438455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aplguq3f</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.25</td>\n",
       "      <td>4649.542211</td>\n",
       "      <td>2324.771106</td>\n",
       "      <td>4.796945</td>\n",
       "      <td>18783.696777</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>38.746185</td>\n",
       "      <td>26.216524</td>\n",
       "      <td>182.358467</td>\n",
       "      <td>18.343454</td>\n",
       "      <td>26.973762</td>\n",
       "      <td>-49.454108</td>\n",
       "      <td>-1.857847</td>\n",
       "      <td>2.901934</td>\n",
       "      <td>27.439289</td>\n",
       "      <td>53.780885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k9km9nuj</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb0.5</td>\n",
       "      <td>5686.842252</td>\n",
       "      <td>2843.421126</td>\n",
       "      <td>4.631720</td>\n",
       "      <td>22371.815918</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>47.390352</td>\n",
       "      <td>34.664684</td>\n",
       "      <td>233.439440</td>\n",
       "      <td>21.847476</td>\n",
       "      <td>51.228678</td>\n",
       "      <td>-35.295548</td>\n",
       "      <td>-1.850099</td>\n",
       "      <td>2.909682</td>\n",
       "      <td>52.131824</td>\n",
       "      <td>93.996445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p0sm9w3v</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_favor_nb1</td>\n",
       "      <td>7790.235805</td>\n",
       "      <td>3895.117902</td>\n",
       "      <td>4.316468</td>\n",
       "      <td>29547.308105</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>64.918632</td>\n",
       "      <td>51.608408</td>\n",
       "      <td>336.270895</td>\n",
       "      <td>28.854793</td>\n",
       "      <td>99.733467</td>\n",
       "      <td>-6.792853</td>\n",
       "      <td>-2.810935</td>\n",
       "      <td>1.948846</td>\n",
       "      <td>101.513592</td>\n",
       "      <td>175.727792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4h910kzj</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h2_c128</td>\n",
       "      <td>5230.441202</td>\n",
       "      <td>2615.220601</td>\n",
       "      <td>2.223732</td>\n",
       "      <td>21855.970215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>43.587010</td>\n",
       "      <td>31.354587</td>\n",
       "      <td>212.592369</td>\n",
       "      <td>21.343721</td>\n",
       "      <td>47.741672</td>\n",
       "      <td>-41.073913</td>\n",
       "      <td>-1.079961</td>\n",
       "      <td>3.679820</td>\n",
       "      <td>47.765278</td>\n",
       "      <td>130.278624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kb5ewjpp</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h2_c64</td>\n",
       "      <td>4578.586053</td>\n",
       "      <td>2289.293026</td>\n",
       "      <td>2.299913</td>\n",
       "      <td>19551.526855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>38.154884</td>\n",
       "      <td>25.933713</td>\n",
       "      <td>180.044620</td>\n",
       "      <td>19.093288</td>\n",
       "      <td>32.164129</td>\n",
       "      <td>-50.095457</td>\n",
       "      <td>-1.483286</td>\n",
       "      <td>3.276495</td>\n",
       "      <td>32.178884</td>\n",
       "      <td>61.739014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7olzfob3</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h4_c128</td>\n",
       "      <td>7900.804413</td>\n",
       "      <td>3950.402206</td>\n",
       "      <td>2.023962</td>\n",
       "      <td>29580.395020</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>65.840037</td>\n",
       "      <td>53.935173</td>\n",
       "      <td>347.420766</td>\n",
       "      <td>28.887105</td>\n",
       "      <td>99.957128</td>\n",
       "      <td>-3.702346</td>\n",
       "      <td>-0.975513</td>\n",
       "      <td>3.784268</td>\n",
       "      <td>100.016647</td>\n",
       "      <td>260.290285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9hb1xk49</td>\n",
       "      <td>E2_pretraining_arxiv_bertsmall_lsh_h4_c64</td>\n",
       "      <td>6600.884504</td>\n",
       "      <td>3300.442252</td>\n",
       "      <td>2.072415</td>\n",
       "      <td>24974.400879</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>55.007371</td>\n",
       "      <td>43.066358</td>\n",
       "      <td>282.280174</td>\n",
       "      <td>24.389063</td>\n",
       "      <td>68.821595</td>\n",
       "      <td>-21.757934</td>\n",
       "      <td>-1.701870</td>\n",
       "      <td>3.057911</td>\n",
       "      <td>68.838179</td>\n",
       "      <td>127.673625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jibomvsj</td>\n",
       "      <td>E1_pretraining_arxiv_bertsmall_mha</td>\n",
       "      <td>8411.599251</td>\n",
       "      <td>4205.799625</td>\n",
       "      <td>1.672165</td>\n",
       "      <td>14793.368652</td>\n",
       "      <td>tapt</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>70.096660</td>\n",
       "      <td>55.146174</td>\n",
       "      <td>360.778016</td>\n",
       "      <td>14.446649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.759781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tzihl99m</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>5584.716981</td>\n",
       "      <td>930.786164</td>\n",
       "      <td>4.505261</td>\n",
       "      <td>16990.050293</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>15.513103</td>\n",
       "      <td>9.372068</td>\n",
       "      <td>158.683094</td>\n",
       "      <td>16.591846</td>\n",
       "      <td>14.905592</td>\n",
       "      <td>-7.855353</td>\n",
       "      <td>-11.142997</td>\n",
       "      <td>11.177103</td>\n",
       "      <td>15.096404</td>\n",
       "      <td>39.438455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>71eagzdx</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>6335.581790</td>\n",
       "      <td>1055.930298</td>\n",
       "      <td>3.913812</td>\n",
       "      <td>18786.679199</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>17.598838</td>\n",
       "      <td>11.478207</td>\n",
       "      <td>185.940478</td>\n",
       "      <td>18.346366</td>\n",
       "      <td>27.056393</td>\n",
       "      <td>7.972558</td>\n",
       "      <td>-7.562275</td>\n",
       "      <td>14.757824</td>\n",
       "      <td>27.439289</td>\n",
       "      <td>53.780885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pu4bht10</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_n...</td>\n",
       "      <td>8143.261960</td>\n",
       "      <td>1357.210327</td>\n",
       "      <td>3.665452</td>\n",
       "      <td>22377.155762</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>22.620172</td>\n",
       "      <td>16.403480</td>\n",
       "      <td>250.545392</td>\n",
       "      <td>21.852691</td>\n",
       "      <td>51.339183</td>\n",
       "      <td>45.487563</td>\n",
       "      <td>-5.248785</td>\n",
       "      <td>17.071315</td>\n",
       "      <td>52.131824</td>\n",
       "      <td>93.996445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vyeowm82</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_favor_nb1</td>\n",
       "      <td>11534.012031</td>\n",
       "      <td>1922.335338</td>\n",
       "      <td>3.267827</td>\n",
       "      <td>29563.599121</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>32.038922</td>\n",
       "      <td>25.618617</td>\n",
       "      <td>371.563854</td>\n",
       "      <td>28.870702</td>\n",
       "      <td>99.941896</td>\n",
       "      <td>115.760981</td>\n",
       "      <td>-9.527178</td>\n",
       "      <td>12.792921</td>\n",
       "      <td>101.513592</td>\n",
       "      <td>175.727792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i1bylyot</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h2_...</td>\n",
       "      <td>7766.640232</td>\n",
       "      <td>1294.440039</td>\n",
       "      <td>2.439533</td>\n",
       "      <td>21849.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>21.574001</td>\n",
       "      <td>15.637627</td>\n",
       "      <td>238.907391</td>\n",
       "      <td>21.337795</td>\n",
       "      <td>47.773307</td>\n",
       "      <td>38.729569</td>\n",
       "      <td>-3.296025</td>\n",
       "      <td>19.024075</td>\n",
       "      <td>47.765278</td>\n",
       "      <td>130.278624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2g7hhgqh</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64</td>\n",
       "      <td>6697.284041</td>\n",
       "      <td>1116.214007</td>\n",
       "      <td>2.557153</td>\n",
       "      <td>19545.526855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>18.603567</td>\n",
       "      <td>12.613102</td>\n",
       "      <td>199.913116</td>\n",
       "      <td>19.087429</td>\n",
       "      <td>32.188563</td>\n",
       "      <td>16.086238</td>\n",
       "      <td>0.815451</td>\n",
       "      <td>23.135551</td>\n",
       "      <td>32.178884</td>\n",
       "      <td>61.739014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o9riawb7</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h4_...</td>\n",
       "      <td>11807.380705</td>\n",
       "      <td>1967.896784</td>\n",
       "      <td>2.263041</td>\n",
       "      <td>29574.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>32.798280</td>\n",
       "      <td>26.940065</td>\n",
       "      <td>385.370136</td>\n",
       "      <td>28.881740</td>\n",
       "      <td>100.018338</td>\n",
       "      <td>123.778061</td>\n",
       "      <td>-8.870178</td>\n",
       "      <td>13.449922</td>\n",
       "      <td>100.016647</td>\n",
       "      <td>260.290285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>m6psyu6o</td>\n",
       "      <td>E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64</td>\n",
       "      <td>9655.740008</td>\n",
       "      <td>1609.290001</td>\n",
       "      <td>2.351454</td>\n",
       "      <td>24966.901855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>26.821500</td>\n",
       "      <td>20.881994</td>\n",
       "      <td>307.102960</td>\n",
       "      <td>24.381740</td>\n",
       "      <td>68.853923</td>\n",
       "      <td>78.329607</td>\n",
       "      <td>-2.081280</td>\n",
       "      <td>20.238820</td>\n",
       "      <td>68.838179</td>\n",
       "      <td>127.673625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vi5hkih4</td>\n",
       "      <td>E1_pretraining_hyperpartisan_bertsmall_mha</td>\n",
       "      <td>5994.331384</td>\n",
       "      <td>999.055231</td>\n",
       "      <td>1.887050</td>\n",
       "      <td>14786.095215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>16.650921</td>\n",
       "      <td>10.329335</td>\n",
       "      <td>172.210866</td>\n",
       "      <td>14.439546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.320100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mzlsnnzy</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.125</td>\n",
       "      <td>1365.717010</td>\n",
       "      <td>91.047801</td>\n",
       "      <td>3.199574</td>\n",
       "      <td>17015.669434</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.125</td>\n",
       "      <td>z28fmtgo</td>\n",
       "      <td>...</td>\n",
       "      <td>1.517463</td>\n",
       "      <td>0.942978</td>\n",
       "      <td>30.305778</td>\n",
       "      <td>16.616865</td>\n",
       "      <td>15.092479</td>\n",
       "      <td>27.841235</td>\n",
       "      <td>-1.700428</td>\n",
       "      <td>1.659489</td>\n",
       "      <td>15.096404</td>\n",
       "      <td>39.438455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1qxi1tf7</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.25</td>\n",
       "      <td>1540.193664</td>\n",
       "      <td>102.679578</td>\n",
       "      <td>2.865847</td>\n",
       "      <td>18840.782715</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.25</td>\n",
       "      <td>0itox8yh</td>\n",
       "      <td>...</td>\n",
       "      <td>1.711326</td>\n",
       "      <td>1.116439</td>\n",
       "      <td>34.601404</td>\n",
       "      <td>18.399202</td>\n",
       "      <td>27.437384</td>\n",
       "      <td>45.961812</td>\n",
       "      <td>-1.882525</td>\n",
       "      <td>1.477392</td>\n",
       "      <td>27.439289</td>\n",
       "      <td>53.780885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1crsv1ae</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb0.5</td>\n",
       "      <td>1944.468278</td>\n",
       "      <td>129.631219</td>\n",
       "      <td>2.834124</td>\n",
       "      <td>22489.657715</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb0.5</td>\n",
       "      <td>yjcapkj3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160520</td>\n",
       "      <td>1.517754</td>\n",
       "      <td>44.549838</td>\n",
       "      <td>21.962556</td>\n",
       "      <td>52.118050</td>\n",
       "      <td>87.928069</td>\n",
       "      <td>-1.220260</td>\n",
       "      <td>2.139658</td>\n",
       "      <td>52.131824</td>\n",
       "      <td>93.996445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>jucxpr34</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_favor_nb1</td>\n",
       "      <td>3032.204451</td>\n",
       "      <td>202.146963</td>\n",
       "      <td>2.761649</td>\n",
       "      <td>29793.169434</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>favor_nb1</td>\n",
       "      <td>noug0x9s</td>\n",
       "      <td>...</td>\n",
       "      <td>3.369116</td>\n",
       "      <td>2.566675</td>\n",
       "      <td>71.070139</td>\n",
       "      <td>29.094892</td>\n",
       "      <td>101.518356</td>\n",
       "      <td>199.800727</td>\n",
       "      <td>-1.120092</td>\n",
       "      <td>2.239825</td>\n",
       "      <td>101.513592</td>\n",
       "      <td>175.727792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>o031jp0p</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h2_c128</td>\n",
       "      <td>2053.627131</td>\n",
       "      <td>136.908475</td>\n",
       "      <td>2.502445</td>\n",
       "      <td>21848.151855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c128</td>\n",
       "      <td>hu06m597</td>\n",
       "      <td>...</td>\n",
       "      <td>2.281808</td>\n",
       "      <td>1.647567</td>\n",
       "      <td>47.407655</td>\n",
       "      <td>21.336086</td>\n",
       "      <td>47.778962</td>\n",
       "      <td>99.983422</td>\n",
       "      <td>-0.200200</td>\n",
       "      <td>3.159718</td>\n",
       "      <td>47.765278</td>\n",
       "      <td>130.278624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a16qafd4</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h2_c64</td>\n",
       "      <td>1774.054965</td>\n",
       "      <td>118.270331</td>\n",
       "      <td>2.602880</td>\n",
       "      <td>19543.776855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h2_c64</td>\n",
       "      <td>eudij3yf</td>\n",
       "      <td>...</td>\n",
       "      <td>1.971172</td>\n",
       "      <td>1.347485</td>\n",
       "      <td>40.347465</td>\n",
       "      <td>19.085720</td>\n",
       "      <td>32.192374</td>\n",
       "      <td>70.200869</td>\n",
       "      <td>-0.079952</td>\n",
       "      <td>3.279965</td>\n",
       "      <td>32.178884</td>\n",
       "      <td>61.739014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9fnn58aq</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h4_c128</td>\n",
       "      <td>3070.909744</td>\n",
       "      <td>204.727316</td>\n",
       "      <td>2.478065</td>\n",
       "      <td>29574.151855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c128</td>\n",
       "      <td>0e2zxgor</td>\n",
       "      <td>...</td>\n",
       "      <td>3.412122</td>\n",
       "      <td>2.704581</td>\n",
       "      <td>72.818476</td>\n",
       "      <td>28.881008</td>\n",
       "      <td>100.036941</td>\n",
       "      <td>207.175876</td>\n",
       "      <td>0.120082</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>100.016647</td>\n",
       "      <td>260.290285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2fpocsb8</td>\n",
       "      <td>E2_pretraining_imdb_bertsmall_lsh_h4_c64</td>\n",
       "      <td>2522.166988</td>\n",
       "      <td>168.144466</td>\n",
       "      <td>2.534777</td>\n",
       "      <td>24966.026855</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>lsh_h4_c64</td>\n",
       "      <td>wi223o9o</td>\n",
       "      <td>...</td>\n",
       "      <td>2.802408</td>\n",
       "      <td>2.126218</td>\n",
       "      <td>59.045862</td>\n",
       "      <td>24.380886</td>\n",
       "      <td>68.867992</td>\n",
       "      <td>149.077779</td>\n",
       "      <td>-0.359918</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>68.838179</td>\n",
       "      <td>127.673625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ghydwtq8</td>\n",
       "      <td>E1_pretraining_imdb_bertsmall_mha</td>\n",
       "      <td>1113.179355</td>\n",
       "      <td>74.211957</td>\n",
       "      <td>2.387619</td>\n",
       "      <td>14784.345215</td>\n",
       "      <td>tapt</td>\n",
       "      <td>imdb</td>\n",
       "      <td>mha</td>\n",
       "      <td>hd228t3k</td>\n",
       "      <td>...</td>\n",
       "      <td>1.236866</td>\n",
       "      <td>0.644100</td>\n",
       "      <td>23.705793</td>\n",
       "      <td>14.437837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.359917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>hyperpartisan</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-22.320100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>imdb</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-3.359917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-4.759781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tapt/run_id                                      tapt/run_name  \\\n",
       "0     t67wmtc1       E2_pretraining_arxiv_bertsmall_favor_nb0.125   \n",
       "1     aplguq3f        E2_pretraining_arxiv_bertsmall_favor_nb0.25   \n",
       "2     k9km9nuj         E2_pretraining_arxiv_bertsmall_favor_nb0.5   \n",
       "3     p0sm9w3v           E2_pretraining_arxiv_bertsmall_favor_nb1   \n",
       "4     4h910kzj         E2_pretraining_arxiv_bertsmall_lsh_h2_c128   \n",
       "5     kb5ewjpp          E2_pretraining_arxiv_bertsmall_lsh_h2_c64   \n",
       "6     7olzfob3         E2_pretraining_arxiv_bertsmall_lsh_h4_c128   \n",
       "7     9hb1xk49          E2_pretraining_arxiv_bertsmall_lsh_h4_c64   \n",
       "8     jibomvsj                 E1_pretraining_arxiv_bertsmall_mha   \n",
       "9     tzihl99m  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "10    71eagzdx  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "11    pu4bht10  E2_pretraining_hyperpartisan_bertsmall_favor_n...   \n",
       "12    vyeowm82   E2_pretraining_hyperpartisan_bertsmall_favor_nb1   \n",
       "13    i1bylyot  E2_pretraining_hyperpartisan_bertsmall_lsh_h2_...   \n",
       "14    2g7hhgqh  E2_pretraining_hyperpartisan_bertsmall_lsh_h2_c64   \n",
       "15    o9riawb7  E2_pretraining_hyperpartisan_bertsmall_lsh_h4_...   \n",
       "16    m6psyu6o  E2_pretraining_hyperpartisan_bertsmall_lsh_h4_c64   \n",
       "17    vi5hkih4         E1_pretraining_hyperpartisan_bertsmall_mha   \n",
       "18    mzlsnnzy        E2_pretraining_imdb_bertsmall_favor_nb0.125   \n",
       "19    1qxi1tf7         E2_pretraining_imdb_bertsmall_favor_nb0.25   \n",
       "20    1crsv1ae          E2_pretraining_imdb_bertsmall_favor_nb0.5   \n",
       "21    jucxpr34            E2_pretraining_imdb_bertsmall_favor_nb1   \n",
       "22    o031jp0p          E2_pretraining_imdb_bertsmall_lsh_h2_c128   \n",
       "23    a16qafd4           E2_pretraining_imdb_bertsmall_lsh_h2_c64   \n",
       "24    9fnn58aq          E2_pretraining_imdb_bertsmall_lsh_h4_c128   \n",
       "25    2fpocsb8           E2_pretraining_imdb_bertsmall_lsh_h4_c64   \n",
       "26    ghydwtq8                  E1_pretraining_imdb_bertsmall_mha   \n",
       "27           0                                                  0   \n",
       "28           0                                                  0   \n",
       "29           0                                                  0   \n",
       "\n",
       "    tapt/duration_s  tapt/avg_epoch_time_s  tapt/train/avg_epoch_loss  \\\n",
       "0       4140.682855            2070.341427                   5.032768   \n",
       "1       4649.542211            2324.771106                   4.796945   \n",
       "2       5686.842252            2843.421126                   4.631720   \n",
       "3       7790.235805            3895.117902                   4.316468   \n",
       "4       5230.441202            2615.220601                   2.223732   \n",
       "5       4578.586053            2289.293026                   2.299913   \n",
       "6       7900.804413            3950.402206                   2.023962   \n",
       "7       6600.884504            3300.442252                   2.072415   \n",
       "8       8411.599251            4205.799625                   1.672165   \n",
       "9       5584.716981             930.786164                   4.505261   \n",
       "10      6335.581790            1055.930298                   3.913812   \n",
       "11      8143.261960            1357.210327                   3.665452   \n",
       "12     11534.012031            1922.335338                   3.267827   \n",
       "13      7766.640232            1294.440039                   2.439533   \n",
       "14      6697.284041            1116.214007                   2.557153   \n",
       "15     11807.380705            1967.896784                   2.263041   \n",
       "16      9655.740008            1609.290001                   2.351454   \n",
       "17      5994.331384             999.055231                   1.887050   \n",
       "18      1365.717010              91.047801                   3.199574   \n",
       "19      1540.193664             102.679578                   2.865847   \n",
       "20      1944.468278             129.631219                   2.834124   \n",
       "21      3032.204451             202.146963                   2.761649   \n",
       "22      2053.627131             136.908475                   2.502445   \n",
       "23      1774.054965             118.270331                   2.602880   \n",
       "24      3070.909744             204.727316                   2.478065   \n",
       "25      2522.166988             168.144466                   2.534777   \n",
       "26      1113.179355              74.211957                   2.387619   \n",
       "27         0.000000               0.000000                   0.000000   \n",
       "28         0.000000               0.000000                   0.000000   \n",
       "29         0.000000               0.000000                   0.000000   \n",
       "\n",
       "    tapt/train/gpu_mem_peak_mb tapt/stage        dataset           arch  \\\n",
       "0                 16991.537598       tapt          arxiv  favor_nb0.125   \n",
       "1                 18783.696777       tapt          arxiv   favor_nb0.25   \n",
       "2                 22371.815918       tapt          arxiv    favor_nb0.5   \n",
       "3                 29547.308105       tapt          arxiv      favor_nb1   \n",
       "4                 21855.970215       tapt          arxiv    lsh_h2_c128   \n",
       "5                 19551.526855       tapt          arxiv     lsh_h2_c64   \n",
       "6                 29580.395020       tapt          arxiv    lsh_h4_c128   \n",
       "7                 24974.400879       tapt          arxiv     lsh_h4_c64   \n",
       "8                 14793.368652       tapt          arxiv            mha   \n",
       "9                 16990.050293       tapt  hyperpartisan  favor_nb0.125   \n",
       "10                18786.679199       tapt  hyperpartisan   favor_nb0.25   \n",
       "11                22377.155762       tapt  hyperpartisan    favor_nb0.5   \n",
       "12                29563.599121       tapt  hyperpartisan      favor_nb1   \n",
       "13                21849.901855       tapt  hyperpartisan    lsh_h2_c128   \n",
       "14                19545.526855       tapt  hyperpartisan     lsh_h2_c64   \n",
       "15                29574.901855       tapt  hyperpartisan    lsh_h4_c128   \n",
       "16                24966.901855       tapt  hyperpartisan     lsh_h4_c64   \n",
       "17                14786.095215       tapt  hyperpartisan            mha   \n",
       "18                17015.669434       tapt           imdb  favor_nb0.125   \n",
       "19                18840.782715       tapt           imdb   favor_nb0.25   \n",
       "20                22489.657715       tapt           imdb    favor_nb0.5   \n",
       "21                29793.169434       tapt           imdb      favor_nb1   \n",
       "22                21848.151855       tapt           imdb    lsh_h2_c128   \n",
       "23                19543.776855       tapt           imdb     lsh_h2_c64   \n",
       "24                29574.151855       tapt           imdb    lsh_h4_c128   \n",
       "25                24966.026855       tapt           imdb     lsh_h4_c64   \n",
       "26                14784.345215       tapt           imdb            mha   \n",
       "27                    0.000000          0  hyperpartisan       baseline   \n",
       "28                    0.000000          0           imdb       baseline   \n",
       "29                    0.000000          0          arxiv       baseline   \n",
       "\n",
       "   pretrain/run_id  ... tapt/avg_epoch_time_min  finetune/avg_epoch_time_min  \\\n",
       "0         z28fmtgo  ...               34.505690                    22.142533   \n",
       "1         0itox8yh  ...               38.746185                    26.216524   \n",
       "2         yjcapkj3  ...               47.390352                    34.664684   \n",
       "3         noug0x9s  ...               64.918632                    51.608408   \n",
       "4         hu06m597  ...               43.587010                    31.354587   \n",
       "5         eudij3yf  ...               38.154884                    25.933713   \n",
       "6         0e2zxgor  ...               65.840037                    53.935173   \n",
       "7         wi223o9o  ...               55.007371                    43.066358   \n",
       "8         hd228t3k  ...               70.096660                    55.146174   \n",
       "9         z28fmtgo  ...               15.513103                     9.372068   \n",
       "10        0itox8yh  ...               17.598838                    11.478207   \n",
       "11        yjcapkj3  ...               22.620172                    16.403480   \n",
       "12        noug0x9s  ...               32.038922                    25.618617   \n",
       "13        hu06m597  ...               21.574001                    15.637627   \n",
       "14        eudij3yf  ...               18.603567                    12.613102   \n",
       "15        0e2zxgor  ...               32.798280                    26.940065   \n",
       "16        wi223o9o  ...               26.821500                    20.881994   \n",
       "17        hd228t3k  ...               16.650921                    10.329335   \n",
       "18        z28fmtgo  ...                1.517463                     0.942978   \n",
       "19        0itox8yh  ...                1.711326                     1.116439   \n",
       "20        yjcapkj3  ...                2.160520                     1.517754   \n",
       "21        noug0x9s  ...                3.369116                     2.566675   \n",
       "22        hu06m597  ...                2.281808                     1.647567   \n",
       "23        eudij3yf  ...                1.971172                     1.347485   \n",
       "24        0e2zxgor  ...                3.412122                     2.704581   \n",
       "25        wi223o9o  ...                2.802408                     2.126218   \n",
       "26        hd228t3k  ...                1.236866                     0.644100   \n",
       "27               0  ...                0.000000                     0.000000   \n",
       "28               0  ...                0.000000                     0.000000   \n",
       "29               0  ...                0.000000                     0.000000   \n",
       "\n",
       "    finetune_tapt/duration_min  finetune_tapt/train/gpu_mem_peak_gb  \\\n",
       "0                   157.581511                            16.593298   \n",
       "1                   182.358467                            18.343454   \n",
       "2                   233.439440                            21.847476   \n",
       "3                   336.270895                            28.854793   \n",
       "4                   212.592369                            21.343721   \n",
       "5                   180.044620                            19.093288   \n",
       "6                   347.420766                            28.887105   \n",
       "7                   282.280174                            24.389063   \n",
       "8                   360.778016                            14.446649   \n",
       "9                   158.683094                            16.591846   \n",
       "10                  185.940478                            18.346366   \n",
       "11                  250.545392                            21.852691   \n",
       "12                  371.563854                            28.870702   \n",
       "13                  238.907391                            21.337795   \n",
       "14                  199.913116                            19.087429   \n",
       "15                  385.370136                            28.881740   \n",
       "16                  307.102960                            24.381740   \n",
       "17                  172.210866                            14.439546   \n",
       "18                   30.305778                            16.616865   \n",
       "19                   34.601404                            18.399202   \n",
       "20                   44.549838                            21.962556   \n",
       "21                   71.070139                            29.094892   \n",
       "22                   47.407655                            21.336086   \n",
       "23                   40.347465                            19.085720   \n",
       "24                   72.818476                            28.881008   \n",
       "25                   59.045862                            24.380886   \n",
       "26                   23.705793                            14.437837   \n",
       "27                    0.000000                             0.000000   \n",
       "28                    0.000000                             0.000000   \n",
       "29                    0.000000                             0.000000   \n",
       "\n",
       "    finetune_tapt/train/gpu_mem_peak_gb_pct_vs_mha  \\\n",
       "0                                        14.859151   \n",
       "1                                        26.973762   \n",
       "2                                        51.228678   \n",
       "3                                        99.733467   \n",
       "4                                        47.741672   \n",
       "5                                        32.164129   \n",
       "6                                        99.957128   \n",
       "7                                        68.821595   \n",
       "8                                         0.000000   \n",
       "9                                        14.905592   \n",
       "10                                       27.056393   \n",
       "11                                       51.339183   \n",
       "12                                       99.941896   \n",
       "13                                       47.773307   \n",
       "14                                       32.188563   \n",
       "15                                      100.018338   \n",
       "16                                       68.853923   \n",
       "17                                        0.000000   \n",
       "18                                       15.092479   \n",
       "19                                       27.437384   \n",
       "20                                       52.118050   \n",
       "21                                      101.518356   \n",
       "22                                       47.778962   \n",
       "23                                       32.192374   \n",
       "24                                      100.036941   \n",
       "25                                       68.867992   \n",
       "26                                        0.000000   \n",
       "27                                     -100.000000   \n",
       "28                                     -100.000000   \n",
       "29                                     -100.000000   \n",
       "\n",
       "   finetune_tapt/duration_min_pct_vs_mha finetune/test/f1_macro_pp_vs_mha  \\\n",
       "0                             -56.321753                        -2.359026   \n",
       "1                             -49.454108                        -1.857847   \n",
       "2                             -35.295548                        -1.850099   \n",
       "3                              -6.792853                        -2.810935   \n",
       "4                             -41.073913                        -1.079961   \n",
       "5                             -50.095457                        -1.483286   \n",
       "6                              -3.702346                        -0.975513   \n",
       "7                             -21.757934                        -1.701870   \n",
       "8                               0.000000                         0.000000   \n",
       "9                              -7.855353                       -11.142997   \n",
       "10                              7.972558                        -7.562275   \n",
       "11                             45.487563                        -5.248785   \n",
       "12                            115.760981                        -9.527178   \n",
       "13                             38.729569                        -3.296025   \n",
       "14                             16.086238                         0.815451   \n",
       "15                            123.778061                        -8.870178   \n",
       "16                             78.329607                        -2.081280   \n",
       "17                              0.000000                         0.000000   \n",
       "18                             27.841235                        -1.700428   \n",
       "19                             45.961812                        -1.882525   \n",
       "20                             87.928069                        -1.220260   \n",
       "21                            199.800727                        -1.120092   \n",
       "22                             99.983422                        -0.200200   \n",
       "23                             70.200869                        -0.079952   \n",
       "24                            207.175876                         0.120082   \n",
       "25                            149.077779                        -0.359918   \n",
       "26                              0.000000                         0.000000   \n",
       "27                           -100.000000                       -22.320100   \n",
       "28                           -100.000000                        -3.359917   \n",
       "29                           -100.000000                        -4.759781   \n",
       "\n",
       "   finetune/test/f1_macro_pp_vs_baseline  \\\n",
       "0                               2.400755   \n",
       "1                               2.901934   \n",
       "2                               2.909682   \n",
       "3                               1.948846   \n",
       "4                               3.679820   \n",
       "5                               3.276495   \n",
       "6                               3.784268   \n",
       "7                               3.057911   \n",
       "8                               4.759781   \n",
       "9                              11.177103   \n",
       "10                             14.757824   \n",
       "11                             17.071315   \n",
       "12                             12.792921   \n",
       "13                             19.024075   \n",
       "14                             23.135551   \n",
       "15                             13.449922   \n",
       "16                             20.238820   \n",
       "17                             22.320100   \n",
       "18                              1.659489   \n",
       "19                              1.477392   \n",
       "20                              2.139658   \n",
       "21                              2.239825   \n",
       "22                              3.159718   \n",
       "23                              3.279965   \n",
       "24                              3.480000   \n",
       "25                              3.000000   \n",
       "26                              3.359917   \n",
       "27                              0.000000   \n",
       "28                              0.000000   \n",
       "29                              0.000000   \n",
       "\n",
       "    pretrain/train/gpu_mem_peak_gb_pct_vs_mha  \\\n",
       "0                                   15.096404   \n",
       "1                                   27.439289   \n",
       "2                                   52.131824   \n",
       "3                                  101.513592   \n",
       "4                                   47.765278   \n",
       "5                                   32.178884   \n",
       "6                                  100.016647   \n",
       "7                                   68.838179   \n",
       "8                                    0.000000   \n",
       "9                                   15.096404   \n",
       "10                                  27.439289   \n",
       "11                                  52.131824   \n",
       "12                                 101.513592   \n",
       "13                                  47.765278   \n",
       "14                                  32.178884   \n",
       "15                                 100.016647   \n",
       "16                                  68.838179   \n",
       "17                                   0.000000   \n",
       "18                                  15.096404   \n",
       "19                                  27.439289   \n",
       "20                                  52.131824   \n",
       "21                                 101.513592   \n",
       "22                                  47.765278   \n",
       "23                                  32.178884   \n",
       "24                                 100.016647   \n",
       "25                                  68.838179   \n",
       "26                                   0.000000   \n",
       "27                                -100.000000   \n",
       "28                                -100.000000   \n",
       "29                                -100.000000   \n",
       "\n",
       "    pretrain/avg_epoch_time_min_pct_vs_mha  \n",
       "0                                39.438455  \n",
       "1                                53.780885  \n",
       "2                                93.996445  \n",
       "3                               175.727792  \n",
       "4                               130.278624  \n",
       "5                                61.739014  \n",
       "6                               260.290285  \n",
       "7                               127.673625  \n",
       "8                                 0.000000  \n",
       "9                                39.438455  \n",
       "10                               53.780885  \n",
       "11                               93.996445  \n",
       "12                              175.727792  \n",
       "13                              130.278624  \n",
       "14                               61.739014  \n",
       "15                              260.290285  \n",
       "16                              127.673625  \n",
       "17                                0.000000  \n",
       "18                               39.438455  \n",
       "19                               53.780885  \n",
       "20                               93.996445  \n",
       "21                              175.727792  \n",
       "22                              130.278624  \n",
       "23                               61.739014  \n",
       "24                              260.290285  \n",
       "25                              127.673625  \n",
       "26                                0.000000  \n",
       "27                             -100.000000  \n",
       "28                             -100.000000  \n",
       "29                             -100.000000  \n",
       "\n",
       "[30 rows x 40 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "111b99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('final_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba307c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% === Tab 4.8: Pretraining Wikipedia ===\n",
      "\\begin{tabular}{@{}lccc@{}}\n",
      "\\toprule\n",
      "\\textbf{Konfiguracja} & \\textbf{Max VRAM [GB]} & \\textbf{Czas/epoka [min]} & \\textbf{Min. loss} \\\\\n",
      "\\midrule\n",
      "\\textit{SDPA} & 14.44 & 7.88 & 2.157 \\\\\n",
      "\\midrule\n",
      "\\multicolumn{4}{l}{\\textit{LSH}} \\\\\n",
      "$N_h{=}2$, $C{=}64$ & 19.09 & 12.75 & 2.345 \\\\\n",
      "$N_h{=}2$, $C{=}128$ & 21.34 & 18.15 & 2.278 \\\\\n",
      "$N_h{=}4$, $C{=}64$ & 24.38 & 17.94 & 2.286 \\\\\n",
      "$N_h{=}4$, $C{=}128$ & 28.88 & 28.39 & 2.248 \\\\\n",
      "\\midrule\n",
      "\\multicolumn{4}{l}{\\textit{FAVOR+}} \\\\\n",
      "$N_f{=}0.125$ & 16.62 & 10.99 & 3.069 \\\\\n",
      "$N_f{=}0.25$ & 18.40 & 12.12 & 2.694 \\\\\n",
      "$N_f{=}0.5$ & 21.97 & 15.29 & 2.666 \\\\\n",
      "$N_f{=}1.0$ & 29.10 & 21.73 & 2.579 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "% === Tab 4.9: TAPT wszystkie datasety ===\n",
      "\\begin{tabular}{@{}lccccccccc@{}}\n",
      "\\toprule\n",
      "& \\multicolumn{3}{c}{\\textbf{\\textbf{VRAM [GB]}}} & \\multicolumn{3}{c}{\\textbf{\\textbf{Czas [min]}}} & \\multicolumn{3}{c}{\\textbf{\\textbf{Min loss}}} \\\\\n",
      "\\cmidrule(lr){2-4}\n",
      "\\cmidrule(lr){5-7}\n",
      "\\cmidrule(lr){8-10}\n",
      "\\textbf{Model} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} \\\\\n",
      "\\midrule\n",
      "\\textit{SDPA} & 14.44 & 14.44 & 14.45 & 1.24 & 16.65 & 70.10 & 2.388 & 1.887 & 1.672 \\\\\n",
      "\\midrule\n",
      "\\multicolumn{10}{l}{\\textit{LSH}} \\\\\n",
      "$N_h{=}2$, $C{=}64$ & 19.09 & 19.09 & 19.09 & 1.97 & 18.60 & 38.15 & 2.603 & 2.557 & 2.300 \\\\\n",
      "$N_h{=}2$, $C{=}128$ & 21.34 & 21.34 & 21.34 & 2.28 & 21.57 & 43.59 & 2.502 & 2.440 & 2.224 \\\\\n",
      "$N_h{=}4$, $C{=}64$ & 24.38 & 24.38 & 24.39 & 2.80 & 26.82 & 55.01 & 2.535 & 2.351 & 2.072 \\\\\n",
      "$N_h{=}4$, $C{=}128$ & 28.88 & 28.88 & 28.89 & 3.41 & 32.80 & 65.84 & 2.478 & 2.263 & 2.024 \\\\\n",
      "\\midrule\n",
      "\\multicolumn{10}{l}{\\textit{FAVOR+}} \\\\\n",
      "$N_f{=}0.125$ & 16.62 & 16.59 & 16.59 & 1.52 & 15.51 & 34.51 & 3.200 & 4.505 & 5.033 \\\\\n",
      "$N_f{=}0.25$ & 18.40 & 18.35 & 18.34 & 1.71 & 17.60 & 38.75 & 2.866 & 3.914 & 4.797 \\\\\n",
      "$N_f{=}0.5$ & 21.96 & 21.85 & 21.85 & 2.16 & 22.62 & 47.39 & 2.834 & 3.665 & 4.632 \\\\\n",
      "$N_f{=}1.0$ & 29.09 & 28.87 & 28.85 & 3.37 & 32.04 & 64.92 & 2.762 & 3.268 & 4.316 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "% === Tab 4.10: Finetune wszystkie datasety ===\n",
      "\\begin{tabular}{@{}lccccccccc@{}}\n",
      "\\toprule\n",
      "& \\multicolumn{3}{c}{\\textbf{\\textbf{VRAM [GB]}}} & \\multicolumn{3}{c}{\\textbf{\\textbf{Czas [min]}}} & \\multicolumn{3}{c}{\\textbf{\\textbf{F1 [\\%]}}} \\\\\n",
      "\\cmidrule(lr){2-4}\n",
      "\\cmidrule(lr){5-7}\n",
      "\\cmidrule(lr){8-10}\n",
      "\\textbf{Model} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} \\\\\n",
      "\\midrule\n",
      "\\textit{TF-IDF + LR} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 89.50 & 42.23 & 83.62 \\\\\n",
      "\\midrule\n",
      "\\textit{SDPA} & 3.32 & 3.44 & 3.44 & 0.64 & 10.33 & 55.15 & 92.86 & 64.55 & 88.38 \\\\\n",
      "\\midrule\n",
      "\\multicolumn{10}{l}{\\textit{LSH}} \\\\\n",
      "$N_h{=}2$, $C{=}64$ & 9.04 & 9.16 & 9.17 & 1.35 & 12.61 & 25.93 & 92.78 & 65.37 & 86.90 \\\\\n",
      "$N_h{=}2$, $C{=}128$ & 11.76 & 11.88 & 11.89 & 1.65 & 15.64 & 31.35 & 92.66 & 61.25 & 87.30 \\\\\n",
      "$N_h{=}4$, $C{=}64$ & 15.89 & 16.01 & 16.02 & 2.13 & 20.88 & 43.07 & 92.50 & 62.47 & 86.68 \\\\\n",
      "$N_h{=}4$, $C{=}128$ & 21.33 & 21.45 & 21.46 & 2.70 & 26.94 & 53.94 & 92.98 & 55.68 & 87.40 \\\\\n",
      "\\midrule\n",
      "\\multicolumn{10}{l}{\\textit{FAVOR+}} \\\\\n",
      "$N_f{=}0.125$ & 5.50 & 5.59 & 5.59 & 0.94 & 9.37 & 22.14 & 91.16 & 53.41 & 86.02 \\\\\n",
      "$N_f{=}0.25$ & 7.28 & 7.34 & 7.34 & 1.12 & 11.48 & 26.22 & 90.98 & 56.99 & 86.52 \\\\\n",
      "$N_f{=}0.5$ & 10.85 & 10.85 & 10.84 & 1.52 & 16.40 & 34.66 & 91.64 & 59.30 & 86.53 \\\\\n",
      "$N_f{=}1.0$ & 18.40 & 18.36 & 18.34 & 2.57 & 25.62 & 51.61 & 91.74 & 55.02 & 85.57 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "% === Tab 4.15: VRAM % vs SDPA ===\n",
      "\\begin{tabular}{@{}lcccc@{}}\n",
      "\\toprule\n",
      "& \\textbf{Pretraining} & \\multicolumn{3}{c}{\\textbf{}} \\\\\n",
      "\\cmidrule(lr){2-2}\n",
      "\\cmidrule(lr){3-5}\n",
      "\\textbf{Model} & \\textbf{Wikipedia} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} \\\\\n",
      "\\midrule\n",
      "\\textit{SDPA} & +0.0\\% & +0.0\\% & +0.0\\% & +0.0\\% \\\\\n",
      "\\midrule\n",
      "\\multicolumn{5}{l}{\\textit{LSH}} \\\\\n",
      "$N_h{=}2$, $C{=}64$ & +32.2\\% & +32.2\\% & +32.2\\% & +32.2\\% \\\\\n",
      "$N_h{=}2$, $C{=}128$ & +47.8\\% & +47.8\\% & +47.8\\% & +47.7\\% \\\\\n",
      "$N_h{=}4$, $C{=}64$ & +68.9\\% & +68.9\\% & +68.9\\% & +68.8\\% \\\\\n",
      "$N_h{=}4$, $C{=}128$ & +100.0\\% & +100.0\\% & +100.0\\% & +100.0\\% \\\\\n",
      "\\midrule\n",
      "\\multicolumn{5}{l}{\\textit{FAVOR+}} \\\\\n",
      "$N_f{=}0.125$ & +15.1\\% & +15.1\\% & +14.9\\% & +14.9\\% \\\\\n",
      "$N_f{=}0.25$ & +27.4\\% & +27.4\\% & +27.1\\% & +27.0\\% \\\\\n",
      "$N_f{=}0.5$ & +52.1\\% & +52.1\\% & +51.3\\% & +51.2\\% \\\\\n",
      "$N_f{=}1.0$ & +101.5\\% & +101.5\\% & +99.9\\% & +99.7\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "% === Tab 4.16: Czas % vs SDPA ===\n",
      "\\begin{tabular}{@{}lcccc@{}}\n",
      "\\toprule\n",
      "& \\textbf{Pretraining} & \\multicolumn{3}{c}{\\textbf{}} \\\\\n",
      "\\cmidrule(lr){2-2}\n",
      "\\cmidrule(lr){3-5}\n",
      "\\textbf{Model} & \\textbf{Wikipedia} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} \\\\\n",
      "\\midrule\n",
      "\\textit{SDPA} & +0.0\\% & +0.0\\% & +0.0\\% & +0.0\\% \\\\\n",
      "\\midrule\n",
      "\\multicolumn{5}{l}{\\textit{LSH}} \\\\\n",
      "$N_h{=}2$, $C{=}64$ & +70.2\\% & +70.2\\% & +16.1\\% & -50.1\\% \\\\\n",
      "$N_h{=}2$, $C{=}128$ & +100.0\\% & +100.0\\% & +38.7\\% & -41.1\\% \\\\\n",
      "$N_h{=}4$, $C{=}64$ & +149.1\\% & +149.1\\% & +78.3\\% & -21.8\\% \\\\\n",
      "$N_h{=}4$, $C{=}128$ & +207.2\\% & +207.2\\% & +123.8\\% & -3.7\\% \\\\\n",
      "\\midrule\n",
      "\\multicolumn{5}{l}{\\textit{FAVOR+}} \\\\\n",
      "$N_f{=}0.125$ & +27.8\\% & +27.8\\% & -7.9\\% & -56.3\\% \\\\\n",
      "$N_f{=}0.25$ & +46.0\\% & +46.0\\% & +8.0\\% & -49.5\\% \\\\\n",
      "$N_f{=}0.5$ & +87.9\\% & +87.9\\% & +45.5\\% & -35.3\\% \\\\\n",
      "$N_f{=}1.0$ & +199.8\\% & +199.8\\% & +115.8\\% & -6.8\\% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "% === Tab 4.17: F1 comparison ===\n",
      "\\begin{tabular}{@{}lccc@{}}\n",
      "\\toprule\n",
      "\\textbf{Model} & \\textbf{IMDB} & \\textbf{Hyper.} & \\textbf{Arxiv} \\\\\n",
      "\\midrule\n",
      "\\textit{TF-IDF + LR} & 89.50 & 42.23 & 83.62 \\\\\n",
      "\\midrule\n",
      "\\textit{SDPA} (vs TF-IDF+LR) & 92.86 (+3.4) & 64.55 (+22.3) & 88.38 (+4.8) \\\\\n",
      "\\midrule\n",
      "\\multicolumn{4}{l}{\\textit{LSH (vs SDPA / vs TF-IDF+LR)}} \\\\\n",
      "$N_h{=}2$, $C{=}64$ & -0.1 / +3.3 & +0.8 / +23.1 & -1.5 / +3.3 \\\\\n",
      "$N_h{=}2$, $C{=}128$ & -0.2 / +3.2 & -3.3 / +19.0 & -1.1 / +3.7 \\\\\n",
      "$N_h{=}4$, $C{=}64$ & -0.4 / +3.0 & -2.1 / +20.2 & -1.7 / +3.1 \\\\\n",
      "$N_h{=}4$, $C{=}128$ & +0.1 / +3.5 & -8.9 / +13.4 & -1.0 / +3.8 \\\\\n",
      "\\midrule\n",
      "\\multicolumn{4}{l}{\\textit{FAVOR+ (vs SDPA / vs TF-IDF+LR)}} \\\\\n",
      "$N_f{=}0.125$ & -1.7 / +1.7 & -11.1 / +11.2 & -2.4 / +2.4 \\\\\n",
      "$N_f{=}0.25$ & -1.9 / +1.5 & -7.6 / +14.8 & -1.9 / +2.9 \\\\\n",
      "$N_f{=}0.5$ & -1.2 / +2.1 & -5.2 / +17.1 & -1.9 / +2.9 \\\\\n",
      "$N_f{=}1.0$ & -1.1 / +2.2 & -9.5 / +12.8 & -2.8 / +1.9 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"% === Tab 4.8: Pretraining Wikipedia ===\")\n",
    "print(generate_latex_table_simple(\n",
    "    df=final,\n",
    "    metrics=[\n",
    "        {'col': 'pretrain/train/gpu_mem_peak_gb', 'label': 'Max VRAM [GB]', 'fmt': '{:.2f}'},\n",
    "        {'col': 'pretrain/avg_epoch_time_min', 'label': 'Czas/epoka [min]', 'fmt': '{:.2f}'},\n",
    "        {'col': 'pretrain/train/avg_epoch_loss', 'label': 'Min. loss', 'fmt': '{:.3f}'},\n",
    "    ],\n",
    "    filter_col='dataset',\n",
    "    filter_val='imdb', \n",
    "))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"% === Tab 4.9: TAPT wszystkie datasety ===\")\n",
    "print(generate_latex_table(\n",
    "    df=final,\n",
    "    metrics=[\n",
    "        {'col': 'tapt/train/gpu_mem_peak_gb', 'label': r'\\textbf{VRAM [GB]}', 'fmt': '{:.2f}'},\n",
    "        {'col': 'tapt/avg_epoch_time_min', 'label': r'\\textbf{Czas [min]}', 'fmt': '{:.2f}'},\n",
    "        {'col': 'tapt/train/avg_epoch_loss', 'label': r'\\textbf{Min loss}', 'fmt': '{:.3f}'},\n",
    "    ],\n",
    "))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"% === Tab 4.10: Finetune wszystkie datasety ===\")\n",
    "print(generate_latex_table(\n",
    "    df=final,\n",
    "    metrics=[\n",
    "        {'col': 'finetune/train/gpu_mem_peak_gb', 'label': r'\\textbf{VRAM [GB]}', 'fmt': '{:.2f}'},\n",
    "        {'col': 'finetune/avg_epoch_time_min', 'label': r'\\textbf{Czas [min]}', 'fmt': '{:.2f}'},\n",
    "        {'col': 'finetune/test/f1_macro', 'label': r'\\textbf{F1 [\\%]}', 'fmt': '{:.2f}'},\n",
    "    ],\n",
    "    show_baseline=True,\n",
    "))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"% === Tab 4.15: VRAM % vs SDPA ===\")\n",
    "print(generate_latex_table(\n",
    "    df=final,\n",
    "    metrics=[\n",
    "        {'col': 'finetune_tapt/train/gpu_mem_peak_gb_pct_vs_mha', 'label': '', 'fmt': '{:+.1f}', 'suffix': r'\\%'},\n",
    "    ],\n",
    "    extra_col={\n",
    "        'label': 'Pretraining',\n",
    "        'metrics': [{'col': 'finetune_tapt/train/gpu_mem_peak_gb_pct_vs_mha', 'fmt': '{:+.1f}', 'suffix': r'\\%'}]\n",
    "    },\n",
    "))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"% === Tab 4.16: Czas % vs SDPA ===\")\n",
    "print(generate_latex_table(\n",
    "    df=final,\n",
    "    metrics=[\n",
    "        {'col': 'finetune_tapt/duration_min_pct_vs_mha', 'label': '', 'fmt': '{:+.1f}', 'suffix': r'\\%'},\n",
    "    ],\n",
    "    extra_col={\n",
    "        'label': 'Pretraining',\n",
    "        'metrics': [{'col': 'finetune_tapt/duration_min_pct_vs_mha', 'fmt': '{:+.1f}', 'suffix': r'\\%'}]\n",
    "    },\n",
    "))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"% === Tab 4.17: F1 comparison ===\")\n",
    "print(generate_latex_table(\n",
    "    df=final,\n",
    "    metrics=[\n",
    "        {'col': 'finetune/test/f1_macro', 'label': '', 'fmt': '{:.2f}'},\n",
    "    ],\n",
    "    show_baseline=True,\n",
    "    show_sdpa=True,\n",
    "    combined_cell={\n",
    "        'cols': ['finetune/test/f1_macro_pp_vs_mha', 'finetune/test/f1_macro_pp_vs_baseline'],\n",
    "        'fmt': '{:+.1f}',\n",
    "        'sep': ' / '\n",
    "    },\n",
    "))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
