{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2054f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textclf_transformer.tokenizer.wordpiece_tokenizer_wrapper import WordPieceTokenizerWrapper\n",
    "tok = WordPieceTokenizerWrapper()\n",
    "tok.load(tokenizer_dir=\"src/textclf_transformer/tokenizer/BERT_original\")\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1389d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def token_stats(dataset: str, max_len: int):\n",
    "    ds = load_dataset(dataset)\n",
    "    result = {}\n",
    "    for split in list(ds.keys()):\n",
    "        if dataset == 'ccdv/arxiv-classification' and split == 'train':\n",
    "            continue\n",
    "        if dataset == 'imdb' and split =='unsupervised':\n",
    "            continue\n",
    "        \n",
    "        data = list(ds[split]['text'])\n",
    "        tokenized = tok.encode(data, max_length=max_len)\n",
    "        real_tokens = tokenized[:][0] != 0\n",
    "        token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "        max_len_achieved = torch.sum(torch.sum(real_tokens, dim=1) == max_len)\n",
    "        print(f\"Split: {split}\\nNumber of examples that are longer that max_len:\\n{max_len_achieved}\\n{max_len_achieved/len(data)*100}% of total data\")\n",
    "\n",
    "        stats = {\n",
    "            \"avg_tokens\": float(token_lengths.mean()),\n",
    "            \"std_tokens\": float(token_lengths.std()),\n",
    "            \"median_tokens\": float(torch.median(token_lengths)),\n",
    "            \"min_tokens\": int(token_lengths.min()),\n",
    "            \"max_tokens\": int(token_lengths.max()),\n",
    "        }\n",
    "        result[split] = stats\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5091f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input is treated as a list of input texts\n",
      "Split: train\n",
      "Number of examples that are longer that max_len:\n",
      "0\n",
      "0.0% of total data\n",
      "[INFO] input is treated as a list of input texts\n",
      "Split: test\n",
      "Number of examples that are longer that max_len:\n",
      "0\n",
      "0.0% of total data\n",
      "[INFO] input is treated as a list of input texts\n",
      "Split: unsupervised\n",
      "Number of examples that are longer that max_len:\n",
      "0\n",
      "0.0% of total data\n"
     ]
    }
   ],
   "source": [
    "imdb = token_stats('imdb', 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce1a24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'avg_tokens': 313.8713073730469,\n",
       "  'std_tokens': 234.29586791992188,\n",
       "  'median_tokens': 233.0,\n",
       "  'min_tokens': 13,\n",
       "  'max_tokens': 3127},\n",
       " 'test': {'avg_tokens': 306.77099609375,\n",
       "  'std_tokens': 227.89404296875,\n",
       "  'median_tokens': 230.0,\n",
       "  'min_tokens': 10,\n",
       "  'max_tokens': 3157},\n",
       " 'unsupervised': {'avg_tokens': 314.8410339355469,\n",
       "  'std_tokens': 234.513671875,\n",
       "  'median_tokens': 234.0,\n",
       "  'min_tokens': 13,\n",
       "  'max_tokens': 3446}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202c5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] input is treated as a list of input texts\n",
      "Split: validation\n",
      "Number of examples that are longer that max_len:\n",
      "182\n",
      "7.28000020980835% of total data\n",
      "[INFO] input is treated as a list of input texts\n",
      "Split: test\n",
      "Number of examples that are longer that max_len:\n",
      "164\n",
      "6.559999942779541% of total data\n"
     ]
    }
   ],
   "source": [
    "arxiv = token_stats('ccdv/arxiv-classification', 32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fe529e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation': {'avg_tokens': 15012.7451171875,\n",
       "  'std_tokens': 8385.609375,\n",
       "  'median_tokens': 12885.0,\n",
       "  'min_tokens': 1373,\n",
       "  'max_tokens': 32768},\n",
       " 'test': {'avg_tokens': 14745.8623046875,\n",
       "  'std_tokens': 8257.8232421875,\n",
       "  'median_tokens': 12631.0,\n",
       "  'min_tokens': 1268,\n",
       "  'max_tokens': 32768}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae72a544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_tokens': 127.41764068603516, 'std_tokens': 5.4616780281066895, 'median_tokens': 128.0, 'min_tokens': 19, 'max_tokens': 128}\n",
      "{'avg_tokens': 452.4222412109375, 'std_tokens': 111.8324203491211, 'median_tokens': 512.0, 'min_tokens': 20, 'max_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/wikipedia_train.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:450000][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result = stats\n",
    "print(result)\n",
    "\n",
    "real_tokens = tokenized[450000:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result = stats\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a31d9225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_tokens': 481.087155, 'std_tokens': 162.2585449491212, 'median_tokens': 600.0, 'min_tokens': 11, 'max_tokens': 600, 'avg_chars': 3003.112855, 'std_chars': 1003.5419222012681}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "path = \"data/raw/wikipedia.pkl\"  # or .pickle\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    sample = pickle.load(f)\n",
    "\n",
    "token_lengths = np.array([len(t.split()) for t in sample])\n",
    "char_lengths = np.array([len(t) for t in sample])\n",
    "result =  {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(np.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "    \"avg_chars\": float(char_lengths.mean()),\n",
    "    \"std_chars\": float(char_lengths.std()),\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "833f3996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n",
      "{'train': {'avg_tokens': 12094.607421875, 'std_tokens': 4260.59130859375, 'median_tokens': 12836.0, 'min_tokens': 910, 'max_tokens': 16384}, 'test': {'avg_tokens': 12062.6240234375, 'std_tokens': 4342.11865234375, 'median_tokens': 12863.0, 'min_tokens': 1009, 'max_tokens': 16384}, 'val': {'avg_tokens': 12055.3427734375, 'std_tokens': 4320.77392578125, 'median_tokens': 12795.0, 'min_tokens': 650, 'max_tokens': 16384}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/arxiv_train.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result = {}\n",
    "result['train'] = stats\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/arxiv_test.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result['test'] = stats\n",
    "\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/arxiv_val.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result['val'] = stats\n",
    "print('arxiv')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "039e373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb\n",
      "{'train': {'avg_tokens': 262.88775634765625, 'std_tokens': 137.1915740966797, 'median_tokens': 220.0, 'min_tokens': 10, 'max_tokens': 512}, 'test': {'avg_tokens': 262.44219970703125, 'std_tokens': 137.07003784179688, 'median_tokens': 221.0, 'min_tokens': 13, 'max_tokens': 512}, 'val': {'avg_tokens': 263.4132080078125, 'std_tokens': 137.46031188964844, 'median_tokens': 220.0, 'min_tokens': 18, 'max_tokens': 512}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/imdb_train.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result = {}\n",
    "result['train'] = stats\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/imdb_test.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result['test'] = stats\n",
    "\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/imdb_val.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result['val'] = stats\n",
    "print('imdb')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deb783b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperpartisan\n",
      "{'train': {'avg_tokens': 2422.914306640625, 'std_tokens': 807.1834106445312, 'median_tokens': 2150.0, 'min_tokens': 100, 'max_tokens': 4096}, 'test': {'avg_tokens': 2504.314453125, 'std_tokens': 898.734375, 'median_tokens': 2181.0, 'min_tokens': 1296, 'max_tokens': 4096}, 'val': {'avg_tokens': 2503.489501953125, 'std_tokens': 896.273681640625, 'median_tokens': 2197.0, 'min_tokens': 1255, 'max_tokens': 4096}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/hyperpartisan_train.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result = {}\n",
    "result['train'] = stats\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/hyperpartisan_test.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result['test'] = stats\n",
    "\n",
    "\n",
    "tokenized = torch.load(\"data/tokenized/hyperpartisan_val.pt\", weights_only=False)\n",
    "real_tokens = tokenized[:][0] != 0\n",
    "token_lengths = real_tokens.sum(dim=1, dtype=torch.float32)\n",
    "\n",
    "\n",
    "stats = {\n",
    "    \"avg_tokens\": float(token_lengths.mean()),\n",
    "    \"std_tokens\": float(token_lengths.std()),\n",
    "    \"median_tokens\": float(torch.median(token_lengths)),\n",
    "    \"min_tokens\": int(token_lengths.min()),\n",
    "    \"max_tokens\": int(token_lengths.max()),\n",
    "}\n",
    "result['val'] = stats\n",
    "print('hyperpartisan')\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
